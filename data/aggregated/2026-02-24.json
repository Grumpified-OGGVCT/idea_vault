[
  {
    "arxiv_id": "2602.19163v1",
    "title": "JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation",
    "summary": "AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.",
    "authors": [
      "Kai Liu",
      "Yanhao Zheng",
      "Kai Wang",
      "Shengqiong Wu",
      "Rongjunchen Zhang",
      "Jiebo Luo",
      "Dimitrios Hatzinakos",
      "Ziwei Liu",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19163v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19163v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.88
  },
  {
    "arxiv_id": "2602.19024v1",
    "title": "Towards Calibrating Prompt Tuning of Vision-Language Models",
    "summary": "Prompt tuning of large-scale vision-language models such as CLIP enables efficient task adaptation without updating model weights. However, it often leads to poor confidence calibration and unreliable predictive uncertainty. We address this problem by proposing a calibration framework that enhances predictive reliability while preserving the geometry of the pretrained CLIP embedding space, which is required for robust generalization. Our approach extends the standard cross-entropy loss with two complementary regularizers: (1) a mean-variance margin penalty that stabilizes inter-class logit margins by maximizing their average while minimizing dispersion, mitigating underconfidence and overconfidence spikes; and (2) a text moment-matching loss that aligns the first and second moments of tuned text embeddings with their frozen CLIP counterparts, preserving semantic dispersion crucial for generalization. Through extensive experiments across 7 prompt-tuning methods and 11 diverse datasets, we demonstrate that our approach significantly reduces the Expected Calibration Error (ECE) compared to competitive calibration techniques on both base and novel classes",
    "authors": [
      "Ashshak Sharifdeen",
      "Fahad Shamshad",
      "Muhammad Akhtar Munir",
      "Abhishek Basu",
      "Mohamed Insaf Ismithdeen",
      "Jeyapriyan Jeyamohan",
      "Chathurika Sewwandi Silva",
      "Karthik Nandakumar",
      "Muhammad Haris Khan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19024v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19024v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.87
  },
  {
    "arxiv_id": "2602.19237v1",
    "title": "Evaluating SAP RPT-1 for Enterprise Business Process Prediction: In-Context Learning vs. Traditional Machine Learning on Structured SAP Data",
    "summary": "Tabular foundation models aim to make machine learning accessible for enterprise data without task-specific training. This paper presents the first independent evaluation of SAP's Retrieval Pretrained Transformer (RPT-1) from a practitioner perspective. RPT-1 is a compact 64.6 MB model pretrained on 1.34 TB of structured data across 3.1 million tables. We benchmark it against tuned gradient-boosted decision trees (XGBoost, LightGBM, CatBoost) on three SAP business scenarios: demand forecasting across SD/MM/PP modules, predictive data integrity in BC/MM/QM, and financial risk classification in FI/CO/AR. Across five-fold cross-validation on datasets ranging from 2,500 to 3,200 rows, RPT-1 reaches 91-96% of tuned GBDT accuracy without any training examples. The classification gap is modest at 3.6-4.1 percentage points on AUC-ROC, though regression tasks show wider gaps of 8.9-11.1 percentage points on R-squared. An interesting finding is a crossover at roughly 75-100 context rows where RPT-1 actually outperforms XGBoost under limited data. Based on these results, we propose a practical hybrid workflow: use RPT-1 for rapid screening, then train GBDT selectively where prediction accuracy justifies the effort. All experiments are reproducible through publicly available Hugging Face Spaces.",
    "authors": [
      "Amit Lal"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19237v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19237v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.85
  },
  {
    "arxiv_id": "2602.19066v1",
    "title": "IDLM: Inverse-distilled Diffusion Language Models",
    "summary": "Diffusion Language Models (DLMs) have recently achieved strong results in text generation. However, their multi-step sampling leads to slow inference, limiting practical use. To address this, we extend Inverse Distillation, a technique originally developed to accelerate continuous diffusion models, to the discrete setting. Nonetheless, this extension introduces both theoretical and practical challenges. From a theoretical perspective, the inverse distillation objective lacks uniqueness guarantees, which may lead to suboptimal solutions. From a practical standpoint, backpropagation in the discrete space is non-trivial and often unstable. To overcome these challenges, we first provide a theoretical result demonstrating that our inverse formulation admits a unique solution, thereby ensuring valid optimization. We then introduce gradient-stable relaxations to support effective training. As a result, experiments on multiple DLMs show that our method, Inverse-distilled Diffusion Language Models (IDLM), reduces the number of inference steps by 4x-64x, while preserving the teacher model's entropy and generative perplexity.",
    "authors": [
      "David Li",
      "Nikita Gushchin",
      "Dmitry Abulkhanov",
      "Eric Moulines",
      "Ivan Oseledets",
      "Maxim Panov",
      "Alexander Korotin"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19066v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19066v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.85
  },
  {
    "arxiv_id": "2602.19111v1",
    "title": "Astra: Activation-Space Tail-Eigenvector Low-Rank Adaptation of Large Language Models",
    "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, especially LoRA, are widely used for adapting pre-trained models to downstream tasks due to their computational and storage efficiency. However, in the context of LoRA and its variants, the potential of activation subspaces corresponding to tail eigenvectors remains substantially under-exploited, which may lead to suboptimal fine-tuning performance. In this work, we propose Astra (Activation-Space Tail-Eigenvector Low-Rank Adaptation), a novel PEFT method that leverages the tail eigenvectors of the model output activations-estimated from a small task-specific calibration set-to construct task-adaptive low-rank adapters. By constraining updates to the subspace spanned by these tail eigenvectors, Astra achieves faster convergence and improved downstream performance with a significantly reduced parameter budget. Extensive experiments across natural language understanding (NLU) and natural language generation (NLG) tasks demonstrate that Astra consistently outperforms existing PEFT baselines across 16 benchmarks and even surpasses full fine-tuning (FFT) in certain scenarios.",
    "authors": [
      "Kainan Liu",
      "Yong Zhang",
      "Ning Cheng",
      "Yun Zhu",
      "Yanmeng Wang",
      "Shaojun Wang",
      "Jing Xiao"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19111v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19111v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2602.19339v1",
    "title": "SplitLight: An Exploratory Toolkit for Recommender Systems Datasets and Splits",
    "summary": "Offline evaluation of recommender systems is often affected by hidden, under-documented choices in data preparation. Seemingly minor decisions in filtering, handling repeats, cold-start treatment, and splitting strategy design can substantially reorder model rankings and undermine reproducibility and cross-paper comparability.   In this paper, we introduce SplitLight, an open-source exploratory toolkit that enables researchers and practitioners designing preprocessing and splitting pipelines or reviewing external artifacts to make these decisions measurable, comparable, and reportable. Given an interaction log and derived split subsets, SplitLight analyzes core and temporal dataset statistics, characterizes repeat consumption patterns and timestamp anomalies, and diagnoses split validity, including temporal leakage, cold-user/item exposure, and distribution shifts. SplitLight further allows side-by-side comparison of alternative splitting strategies through comprehensive aggregated summaries and interactive visualizations. Delivered as both a Python toolkit and an interactive no-code interface, SplitLight produces audit summaries that justify evaluation protocols and support transparent, reliable, and comparable experimentation in recommender systems research and industry.",
    "authors": [
      "Anna Volodkevich",
      "Dmitry Anikin",
      "Danil Gusak",
      "Anton Klenitskiy",
      "Evgeny Frolov",
      "Alexey Vasilev"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19339v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19339v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2602.19322v1",
    "title": "US-JEPA: A Joint Embedding Predictive Architecture for Medical Ultrasound",
    "summary": "Ultrasound (US) imaging poses unique challenges for representation learning due to its inherently noisy acquisition process. The low signal-to-noise ratio and stochastic speckle patterns hinder standard self-supervised learning methods relying on a pixel-level reconstruction objective. Joint-Embedding Predictive Architectures (JEPAs) address this drawback by predicting masked latent representations rather than raw pixels. However, standard approaches depend on hyperparameter-brittle and computationally expensive online teachers updated via exponential moving average. We propose US-JEPA, a self-supervised framework that adopts the Static-teacher Asymmetric Latent Training (SALT) objective. By using a frozen, domain-specific teacher to provide stable latent targets, US-JEPA decouples student-teacher optimization and pushes the student to expand upon the semantic priors of the teacher. In addition, we provide the first rigorous comparison of all publicly available state-of-the-art ultrasound foundation models on UltraBench, a public dataset benchmark spanning multiple organs and pathological conditions. Under linear probing for diverse classification tasks, US-JEPA achieves performance competitive with or superior to domain-specific and universal vision foundation model baselines. Our results demonstrate that masked latent prediction provides a stable and efficient path toward robust ultrasound representations.",
    "authors": [
      "Ashwath Radhachandran",
      "Vedrana Ivezi\u0107",
      "Shreeram Athreya",
      "Ronit Anilkumar",
      "Corey W. Arnold",
      "William Speier"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19322v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19322v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2602.19128v1",
    "title": "K-Search: LLM Kernel Generation via Co-Evolving Intrinsic World Model",
    "summary": "Optimizing GPU kernels is critical for efficient modern machine learning systems yet remains challenging due to the complex interplay of design factors and rapid hardware evolution. Existing automated approaches typically treat Large Language Models (LLMs) merely as stochastic code generators within heuristic-guided evolutionary loops. These methods often struggle with complex kernels requiring coordinated, multi-step structural transformations, as they lack explicit planning capabilities and frequently discard promising strategies due to inefficient or incorrect intermediate implementations. To address this, we propose Search via Co-Evolving World Model and build K-Search based on this method. By replacing static search heuristics with a co-evolving world model, our framework leverages LLMs' prior domain knowledge to guide the search, actively exploring the optimization space. This approach explicitly decouples high-level algorithmic planning from low-level program instantiation, enabling the system to navigate non-monotonic optimization paths while remaining resilient to temporary implementation defects. We evaluate K-Search on diverse, complex kernels from FlashInfer, including GQA, MLA, and MoE kernels. Our results show that K-Search significantly outperforms state-of-the-art evolutionary search methods, achieving an average 2.10x improvement and up to a 14.3x gain on complex MoE kernels. On the GPUMode TriMul task, K-Search achieves state-of-the-art performance on H100, reaching 1030us and surpassing both prior evolution and human-designed solutions.",
    "authors": [
      "Shiyi Cao",
      "Ziming Mao",
      "Joseph E. Gonzalez",
      "Ion Stoica"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19128v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19128v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2602.19315v1",
    "title": "Online Navigation Planning for Long-term Autonomous Operation of Underwater Gliders",
    "summary": "Underwater glider robots have become an indispensable tool for ocean sampling. Although stakeholders are calling for tools to manage increasingly large fleets of gliders, successful autonomous long-term deployments have thus far been scarce, which hints at a lack of suitable methodologies and systems. In this work, we formulate glider navigation planning as a stochastic shortest-path Markov Decision Process and propose a sample-based online planner based on Monte Carlo Tree Search. Samples are generated by a physics-informed simulator that captures uncertain execution of controls and ocean current forecasts while remaining computationally tractable. The simulator parameters are fitted using historical glider data. We integrate these methods into an autonomous command-and-control system for Slocum gliders that enables closed-loop replanning at each surfacing. The resulting system was validated in two field deployments in the North Sea totalling approximately 3 months and 1000 km of autonomous operation. Results demonstrate improved efficiency compared to straight-to-goal navigation and show the practicality of sample-based planning for long-term marine autonomy.",
    "authors": [
      "Victor-Alexandru Darvariu",
      "Charlotte Z. Reed",
      "Jan Stratmann",
      "Bruno Lacerda",
      "Benjamin Allsup",
      "Stephen Woodward",
      "Elizabeth Siddle",
      "Trishna Saeharaseelan",
      "Owain Jones",
      "Dan Jones",
      "Tobias Ferreira",
      "Chloe Baker",
      "Kevin Chaplin",
      "James Kirk",
      "Ashley Morris",
      "Ryan Patmore",
      "Jeff Polton",
      "Charlotte Williams",
      "Alexandra Kokkinaki",
      "Alvaro Lorenzo Lopez",
      "Justin J. H. Buck",
      "Nick Hawes"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19315v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19315v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2602.19268v1",
    "title": "CORVET: A CORDIC-Powered, Resource-Frugal Mixed-Precision Vector Processing Engine for High-Throughput AIoT applications",
    "summary": "This brief presents a runtime-adaptive, performance-enhanced vector engine featuring a low-resource, iterative CORDIC-based MAC unit for edge AI acceleration. The proposed design enables dynamic reconfiguration between approximate and accurate modes, exploiting the latency-accuracy trade-off for a wide range of workloads. Its resource-efficient approach further enables up to 4x throughput improvement within the same hardware resources by leveraging vectorised, time-multiplexed execution and flexible precision scaling. With a time-multiplexed multi-AF block and a lightweight pooling and normalisation unit, the proposed vector engine supports flexible precision (4/8/16-bit) and high MAC density. The ASIC implementation results show that each MAC stage can save up to 33% of time and 21% of power, with a 256-PE configuration that achieves higher compute density (4.83 TOPS/mm2 ) and energy efficiency (11.67 TOPS/W) than previous state-of-the-art work. A detailed hardware-software co-design methodology for object detection and classification tasks on Pynq-Z2 is discussed to assess the proposed architecture, demonstrating a scalable, energy-efficient solution for edge AI applications.",
    "authors": [
      "Sonu Kumar",
      "Mohd Faisal Khan",
      "Mukul Lokhande",
      "Santosh Kumar Vishvakarma"
    ],
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "eess.IV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19268v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19268v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2602.19240v1",
    "title": "Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering",
    "summary": "Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.",
    "authors": [
      "Sen Zhao",
      "Lincheng Zhou",
      "Yue Chen",
      "Ding Zou"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19240v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19240v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2602.19153v1",
    "title": "Constrained Diffusion for Accelerated Structure Relaxation of Inorganic Solids with Point Defects",
    "summary": "Point defects affect material properties by altering electronic states and modifying local bonding environments. However, high-throughput first-principles simulations of point defects are costly due to large simulation cells and complex energy landscapes. To this end, we propose a generative framework for simulating point defects, overcoming the limits of costly first-principles simulators. By leveraging a primal-dual algorithm, we introduce a constraint-aware diffusion model which outperforms existing constrained diffusion approaches in this domain. Across six defect configuration settings for Bi2Te3, the proposed approach provides state-of-the-art performance generating physically grounded structures.",
    "authors": [
      "Jingyi Cui",
      "Jacob K. Christopher",
      "Ankita Biswas",
      "Prasanna V. Balachandran",
      "Ferdinando Fioretto"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19153v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19153v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2602.19116v1",
    "title": "Event-Triggered Gossip for Distributed Learning",
    "summary": "While distributed learning offers a new learning paradigm for distributed network with no central coordination, it is constrained by communication bottleneck between nodes.   We develop a new event-triggered gossip framework for distributed learning to reduce inter-node communication overhead. The framework introduces an adaptive communication control mechanism that enables each node to autonomously decide in a fully decentralized fashion when to exchange model information with its neighbors based on local model deviations. We analyze the ergodic convergence of the proposed framework under noconvex objectives and interpret the convergence guarantees under different triggering conditions. Simulation results show that the proposed framework achieves substantially lower communication overhead than the state-of-the-art distributed learning methods, reducing cumulative point-to-point transmissions by \\textbf{71.61\\%} with only a marginal performance loss, compared with the conventional full-communication baseline.",
    "authors": [
      "Zhiyuan Zhai",
      "Xiaojun Yuan",
      "Wei Ni",
      "Xin Wang",
      "Rui Zhang",
      "Geoffrey Ye Li"
    ],
    "categories": [
      "eess.SP",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19116v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19116v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2602.19313v1",
    "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics",
    "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.",
    "authors": [
      "Shirui Chen",
      "Cole Harrison",
      "Ying-Chun Lee",
      "Angela Jin Yang",
      "Zhongzheng Ren",
      "Lillian J. Ratliff",
      "Jiafei Duan",
      "Dieter Fox",
      "Ranjay Krishna"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19313v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19313v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.19248v1",
    "title": "No Need For Real Anomaly: MLLM Empowered Zero-Shot Video Anomaly Detection",
    "summary": "The collection and detection of video anomaly data has long been a challenging problem due to its rare occurrence and spatio-temporal scarcity. Existing video anomaly detection (VAD) methods under perform in open-world scenarios. Key contributing factors include limited dataset diversity, and inadequate understanding of context-dependent anomalous semantics. To address these issues, i) we propose LAVIDA, an end-to-end zero-shot video anomaly detection framework. ii) LAVIDA employs an Anomaly Exposure Sampler that transforms segmented objects into pseudo-anomalies to enhance model adaptability to unseen anomaly categories. It further integrates a Multimodal Large Language Model (MLLM) to bolster semantic comprehension capabilities. Additionally, iii) we design a token compression approach based on reverse attention to handle the spatio-temporal scarcity of anomalous patterns and decrease computational cost. The training process is conducted solely on pseudo anomalies without any VAD data. Evaluations across four benchmark VAD datasets demonstrate that LAVIDA achieves SOTA performance in both frame-level and pixel-level anomaly detection under the zero-shot setting. Our code is available in https://github.com/VitaminCreed/LAVIDA.",
    "authors": [
      "Zunkai Dai",
      "Ke Li",
      "Jiajia Liu",
      "Jie Yang",
      "Yuanyuan Qiao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19248v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19248v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.19213v1",
    "title": "SegMoTE: Token-Level Mixture of Experts for Medical Image Segmentation",
    "summary": "Medical image segmentation is vital for clinical diagnosis and quantitative analysis, yet remains challenging due to the heterogeneity of imaging modalities and the high cost of pixel-level annotations. Although general interactive segmentation models like SAM have achieved remarkable progress, their transfer to medical imaging still faces two key bottlenecks: (i) the lack of adaptive mechanisms for modality- and anatomy-specific tasks, which limits generalization in out-of-distribution medical scenarios; and (ii) current medical adaptation methods fine-tune on large, heterogeneous datasets without selection, leading to noisy supervision, higher cost, and negative transfer. To address these issues, we propose SegMoTE, an efficient and adaptive framework for medical image segmentation. SegMoTE preserves SAM's original prompt interface, efficient inference, and zero-shot generalization while introducing only a small number of learnable parameters to dynamically adapt across modalities and tasks. In addition, we design a progressive prompt tokenization mechanism that enables fully automatic segmentation, significantly reducing annotation dependence. Trained on MedSeg-HQ, a curated dataset less than 1% of existing large-scale datasets, SegMoTE achieves SOTA performance across diverse imaging modalities and anatomical tasks. It represents the first efficient, robust, and scalable adaptation of general segmentation models to the medical domain under extremely low annotation cost, advancing the practical deployment of foundation vision models in clinical applications.",
    "authors": [
      "Yujie Lu",
      "Jingwen Li",
      "Sibo Ju",
      "Yanzhou Su",
      "he yao",
      "Yisong Liu",
      "Min Zhu",
      "Junlong Cheng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19213v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19213v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.19146v1",
    "title": "VIGiA: Instructional Video Guidance via Dialogue Reasoning and Retrieval",
    "summary": "We introduce VIGiA, a novel multimodal dialogue model designed to understand and reason over complex, multi-step instructional video action plans. Unlike prior work which focuses mainly on text-only guidance, or treats vision and language in isolation, VIGiA supports grounded, plan-aware dialogue that requires reasoning over visual inputs, instructional plans, and interleaved user interactions. To this end, VIGiA incorporates two key capabilities: (1) multimodal plan reasoning, enabling the model to align uni- and multimodal queries with the current task plan and respond accurately; and (2) plan-based retrieval, allowing it to retrieve relevant plan steps in either textual or visual representations. Experiments were done on a novel dataset with rich Instructional Video Dialogues aligned with Cooking and DIY plans. Our evaluation shows that VIGiA outperforms existing state-of-the-art models on all tasks in a conversational plan guidance setting, reaching over 90\\% accuracy on plan-aware VQA.",
    "authors": [
      "Diogo Gl\u00f3ria-Silva",
      "David Semedo",
      "Jo\u00e3o Maglh\u00e3es"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19146v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19146v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.19096v1",
    "title": "The Power of Decaying Steps: Enhancing Attack Stability and Transferability for Sign-based Optimizers",
    "summary": "Crafting adversarial examples can be formulated as an optimization problem. While sign-based optimizers such as I-FGSM and MI-FGSM have become the de facto standard for the induced optimization problems, there still exist several unsolved problems in theoretical grounding and practical reliability especially in non-convergence and instability, which inevitably influences their transferability. Contrary to the expectation, we observe that the attack success rate may degrade sharply when more number of iterations are conducted. In this paper, we address these issues from an optimization perspective. By reformulating the sign-based optimizer as a specific coordinate-wise gradient descent, we argue that one cause for non-convergence and instability is their non-decaying step-size scheduling. Based upon this viewpoint, we propose a series of new attack algorithms that enforce Monotonically Decreasing Coordinate-wise Step-sizes (MDCS) within sign-based optimizers. Typically, we further provide theoretical guarantees proving that MDCS-MI attains an optimal convergence rate of $O(1/\\sqrt{T})$, where $T$ is the number of iterations. Extensive experiments on image classification and cross-modal retrieval tasks demonstrate that our approach not only significantly improves transferability but also enhances attack stability compared to state-of-the-art sign-based methods.",
    "authors": [
      "Wei Tao",
      "Yang Dai",
      "Jincai Huang",
      "Qing Tao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19096v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19096v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.19041v1",
    "title": "Back to Blackwell: Closing the Loop on Intransitivity in Multi-Objective Preference Fine-Tuning",
    "summary": "A recurring challenge in preference fine-tuning (PFT) is handling $\\textit{intransitive}$ (i.e., cyclic) preferences. Intransitive preferences often stem from either $\\textit{(i)}$ inconsistent rankings along a single objective or $\\textit{(ii)}$ scalarizing multiple objectives into a single metric. Regardless of their source, the downstream implication of intransitive preferences is the same: there is no well-defined optimal policy, breaking a core assumption of the standard PFT pipeline. In response, we propose a novel, game-theoretic solution concept -- the $\\textit{Maximum Entropy Blackwell Winner}$ ($\\textit{MaxEntBW}$) -- that is well-defined under multi-objective intransitive preferences. To enable computing MaxEntBWs at scale, we derive $\\texttt{PROSPER}$: a provably efficient PFT algorithm. Unlike prior self-play techniques, $\\texttt{PROSPER}$ directly handles multiple objectives without requiring scalarization. We then apply $\\texttt{PROSPER}$ to the problem of fine-tuning large language models (LLMs) from multi-objective LLM-as-a-Judge feedback (e.g., rubric-based judges), a setting where both sources of intransitivity arise. We find that $\\texttt{PROSPER}$ outperforms all baselines considered across both instruction following and general chat benchmarks, releasing trained model checkpoints at the 7B and 3B parameter scales.",
    "authors": [
      "Jiahao Zhang",
      "Lujing Zhang",
      "Keltin Grimes",
      "Zhuohao Yu",
      "Gokul Swamy",
      "Zhiwei Steven Wu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19041v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19041v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.19223v1",
    "title": "Characterizing MARL for Energy Control: A Multi-KPI Benchmark on the CityLearn Environment",
    "summary": "The optimization of urban energy systems is crucial for the advancement of sustainable and resilient smart cities, which are becoming increasingly complex with multiple decision-making units. To address scalability and coordination concerns, Multi-Agent Reinforcement Learning (MARL) is a promising solution. This paper addresses the imperative need for comprehensive and reliable benchmarking of MARL algorithms on energy management tasks. CityLearn is used as a case study environment because it realistically simulates urban energy systems, incorporates multiple storage systems, and utilizes renewable energy sources. By doing so, our work sets a new standard for evaluation, conducting a comparative study across multiple key performance indicators (KPIs). This approach illuminates the key strengths and weaknesses of various algorithms, moving beyond traditional KPI averaging which often masks critical insights. Our experiments utilize widely accepted baselines such as Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC), and encompass diverse training schemes including Decentralized Training with Decentralized Execution (DTDE) and Centralized Training with Decentralized Execution (CTDE) approaches and different neural network architectures. Our work also proposes novel KPIs that tackle real world implementation challenges such as individual building contribution and battery storage lifetime. Our findings show that DTDE consistently outperforms CTDE in both average and worst-case performance. Additionally, temporal dependency learning improved control on memory dependent KPIs such as ramping and battery usage, contributing to more sustainable battery operation. Results also reveal robustness to agent or resource removal, highlighting both the resilience and decentralizability of the learned policies.",
    "authors": [
      "Aymen Khouja",
      "Imen Jendoubi",
      "Oumayma Mahjoub",
      "Oussama Mahfoudhi",
      "Claude Formanek",
      "Siddarth Singh",
      "Ruan De Kock"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19223v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19223v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.19058v1",
    "title": "Do LLMs and VLMs Share Neurons for Inference? Evidence and Mechanisms of Cross-Modal Transfer",
    "summary": "Large vision-language models (LVLMs) have rapidly advanced across various domains, yet they still lag behind strong text-only large language models (LLMs) on tasks that require multi-step inference and compositional decision-making. Motivated by their shared transformer architectures, we investigate whether the two model families rely on common internal computation for such inference. At the neuron level, we uncover a surprisingly large overlap: more than half of the top-activated units during multi-step inference are shared between representative LLMs and LVLMs, revealing a modality-invariant inference subspace.   Through causal probing via activation amplification, we further show that these shared neurons encode consistent and interpretable concept-level effects, demonstrating their functional contribution to inference. Building on this insight, we propose Shared Neuron Low-Rank Fusion (SNRF), a parameter-efficient framework that transfers mature inference circuitry from LLMs to LVLMs. SNRF profiles cross-model activations to identify shared neurons, computes a low-rank approximation of inter-model weight differences, and injects these updates selectively within the shared-neuron subspace. This mechanism strengthens multimodal inference performance with minimal parameter changes and requires no large-scale multimodal fine-tuning.   Across diverse mathematics and perception benchmarks, SNRF consistently enhances LVLM inference performance while preserving perceptual capabilities. Our results demonstrate that shared neurons form an interpretable bridge between LLMs and LVLMs, enabling low-cost transfer of inference ability into multimodal models. Our code is available at [https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons](https://github.com/chenhangcuisg-code/Do-LLMs-VLMs-Share-Neurons).",
    "authors": [
      "Chenhang Cui",
      "An Zhang",
      "Yuxin Chen",
      "Gelei Deng",
      "Jingnan Zheng",
      "Zhenkai Liang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19058v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19058v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.19212v1",
    "title": "Retrieval Augmented Enhanced Dual Co-Attention Framework for Target Aware Multimodal Bengali Hateful Meme Detection",
    "summary": "Hateful content on social media increasingly appears as multimodal memes that combine images and text to convey harmful narratives. In low-resource languages such as Bengali, automated detection remains challenging due to limited annotated data, class imbalance, and pervasive code-mixing. To address these issues, we augment the Bengali Hateful Memes (BHM) dataset with semantically aligned samples from the Multimodal Aggression Dataset in Bengali (MIMOSA), improving both class balance and semantic diversity. We propose the Enhanced Dual Co-attention Framework (xDORA), integrating vision encoders (CLIP, DINOv2) and multilingual text encoders (XGLM, XLM-R) via weighted attention pooling to learn robust cross-modal representations. Building on these embeddings, we develop a FAISS-based k-nearest neighbor classifier for non-parametric inference and introduce RAG-Fused DORA, which incorporates retrieval-driven contextual reasoning. We further evaluate LLaVA under zero-shot, few-shot, and retrieval-augmented prompting settings. Experiments on the extended dataset show that xDORA (CLIP + XLM-R) achieves macro-average F1-scores of 0.78 for hateful meme identification and 0.71 for target entity detection, while RAG-Fused DORA improves performance to 0.79 and 0.74, yielding gains over the DORA baseline. The FAISS-based classifier performs competitively and demonstrates robustness for rare classes through semantic similarity modeling. In contrast, LLaVA exhibits limited effectiveness in few-shot settings, with only modest improvements under retrieval augmentation, highlighting constraints of pretrained vision-language models for code-mixed Bengali content without fine-tuning. These findings demonstrate the effectiveness of supervised, retrieval-augmented, and non-parametric multimodal frameworks for addressing linguistic and cultural complexities in low-resource hate speech detection.",
    "authors": [
      "Raihan Tanvir",
      "Md. Golam Rabiul Alam"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19212v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19212v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2602.19138v1",
    "title": "CRCC: Contrast-Based Robust Cross-Subject and Cross-Site Representation Learning for EEG",
    "summary": "EEG-based neural decoding models often fail to generalize across acquisition sites due to structured, site-dependent biases implicitly exploited during training. We reformulate cross-site clinical EEG learning as a bias-factorized generalization problem, in which domain shifts arise from multiple interacting sources. We identify three fundamental bias factors and propose a general training framework that mitigates their influence through data standardization and representation-level constraints. We construct a standardized multi-site EEG benchmark for Major Depressive Disorder and introduce CRCC, a two-stage training paradigm combining encoder-decoder pretraining with joint fine-tuning via cross-subject/site contrastive learning and site-adversarial optimization. CRCC consistently outperforms state-of-the-art baselines and achieves a 10.7 percentage-point improvement in balanced accuracy under strict zero-shot site transfer, demonstrating robust generalization to unseen environments.",
    "authors": [
      "Xiaobin Wong",
      "Zhonghua Zhao",
      "Haoran Guo",
      "Zhengyi Liu",
      "Yu Wu",
      "Feng Yan",
      "Zhiren Wang",
      "Sen Song"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19138v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19138v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2602.19049v1",
    "title": "IAPO: Information-Aware Policy Optimization for Token-Efficient Reasoning",
    "summary": "Large language models increasingly rely on long chains of thought to improve accuracy, yet such gains come with substantial inference-time costs. We revisit token-efficient post-training and argue that existing sequence-level reward-shaping methods offer limited control over how reasoning effort is allocated across tokens. To bridge the gap, we propose IAPO, an information-theoretic post-training framework that assigns token-wise advantages based on each token's conditional mutual information (MI) with the final answer. This yields an explicit, principled mechanism for identifying informative reasoning steps and suppressing low-utility exploration. We provide a theoretical analysis showing that our IAPO can induce monotonic reductions in reasoning verbosity without harming correctness. Empirically, IAPO consistently improves reasoning accuracy while reducing reasoning length by up to 36%, outperforming existing token-efficient RL methods across various reasoning datasets. Extensive empirical evaluations demonstrate that information-aware advantage shaping is a powerful and general direction for token-efficient post-training. The code is available at https://github.com/YinhanHe123/IAPO.",
    "authors": [
      "Yinhan He",
      "Yaochen Zhu",
      "Mingjia Shi",
      "Wendy Zheng",
      "Lin Su",
      "Xiaoqing Wang",
      "Qi Guo",
      "Jundong Li"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19049v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19049v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2602.19157v1",
    "title": "Facet-Level Persona Control by Trait-Activated Routing with Contrastive SAE for Role-Playing LLMs",
    "summary": "Personality control in Role-Playing Agents (RPAs) is commonly achieved via training-free methods that inject persona descriptions and memory through prompts or retrieval-augmented generation, or via supervised fine-tuning (SFT) on persona-specific corpora. While SFT can be effective, it requires persona-labeled data and retraining for new roles, limiting flexibility. In contrast, prompt- and RAG-based signals are easy to apply but can be diluted in long dialogues, leading to drifting and sometimes inconsistent persona behavior. To address this, we propose a contrastive Sparse AutoEncoder (SAE) framework that learns facet-level personality control vectors aligned with the Big Five 30-facet model. A new 15,000-sample leakage-controlled corpus is constructed to provide balanced supervision for each facet. The learned vectors are integrated into the model's residual space and dynamically selected by a trait-activated routing module, enabling precise and interpretable personality steering. Experiments on Large Language Models (LLMs) show that the proposed method maintains stable character fidelity and output quality across contextualized settings, outperforming Contrastive Activation Addition (CAA) and prompt-only baselines. The combined SAE+Prompt configuration achieves the best overall performance, confirming that contrastively trained latent vectors can enhance persona control while preserving dialogue coherence.",
    "authors": [
      "Wenqiu Tang",
      "Zhen Wan",
      "Takahiro Komamizu",
      "Ichiro Ide"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19157v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19157v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.19089v1",
    "title": "Ani3DHuman: Photorealistic 3D Human Animation with Self-guided Stochastic Sampling",
    "summary": "Current 3D human animation methods struggle to achieve photorealism: kinematics-based approaches lack non-rigid dynamics (e.g., clothing dynamics), while methods that leverage video diffusion priors can synthesize non-rigid motion but suffer from quality artifacts and identity loss. To overcome these limitations, we present Ani3DHuman, a framework that marries kinematics-based animation with video diffusion priors. We first introduce a layered motion representation that disentangles rigid motion from residual non-rigid motion. Rigid motion is generated by a kinematic method, which then produces a coarse rendering to guide the video diffusion model in generating video sequences that restore the residual non-rigid motion. However, this restoration task, based on diffusion sampling, is highly challenging, as the initial renderings are out-of-distribution, causing standard deterministic ODE samplers to fail. Therefore, we propose a novel self-guided stochastic sampling method, which effectively addresses the out-of-distribution problem by combining stochastic sampling (for photorealistic quality) with self-guidance (for identity fidelity). These restored videos provide high-quality supervision, enabling the optimization of the residual non-rigid motion field. Extensive experiments demonstrate that \\MethodName can generate photorealistic 3D human animation, outperforming existing methods. Code is available in https://github.com/qiisun/ani3dhuman.",
    "authors": [
      "Qi Sun",
      "Can Wang",
      "Jiaxiang Shang",
      "Yingchun Liu",
      "Jing Liao"
    ],
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19089v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19089v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.19326v1",
    "title": "City Editing: Hierarchical Agentic Execution for Dependency-Aware Urban Geospatial Modification",
    "summary": "As cities evolve over time, challenges such as traffic congestion and functional imbalance increasingly necessitate urban renewal through efficient modification of existing plans, rather than complete re-planning. In practice, even minor urban changes require substantial manual effort to redraw geospatial layouts, slowing the iterative planning and decision-making procedure. Motivated by recent advances in agentic systems and multimodal reasoning, we formulate urban renewal as a machine-executable task that iteratively modifies existing urban plans represented in structured geospatial formats. More specifically, we represent urban layouts using GeoJSON and decompose natural-language editing instructions into hierarchical geometric intents spanning polygon-, line-, and point-level operations. To coordinate interdependent edits across spatial elements and abstraction levels, we propose a hierarchical agentic framework that jointly performs multi-level planning and execution with explicit propagation of intermediate spatial constraints. We further introduce an iterative execution-validation mechanism that mitigates error accumulation and enforces global spatial consistency during multi-step editing. Extensive experiments across diverse urban editing scenarios demonstrate significant improvements in efficiency, robustness, correctness, and spatial validity over existing baselines.",
    "authors": [
      "Rui Liu",
      "Steven Jige Quan",
      "Zhong-Ren Peng",
      "Zijun Yao",
      "Han Wang",
      "Zhengzhang Chen",
      "Kunpeng Liu",
      "Yanjie Fu",
      "Dongjie Wang"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19326v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19326v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.19317v1",
    "title": "Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering",
    "summary": "Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users' background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user's profile. Existing methods use the user's query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.",
    "authors": [
      "Maryam Amirizaniani",
      "Alireza Salemi",
      "Hamed Zamani"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19317v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19317v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.19312v1",
    "title": "Metasurfaces-Integrated Wireless Neural Networks for Lightweight Over-The-Air Edge Inference",
    "summary": "The upcoming sixth Generation (6G) of wireless networks envisions ultra-low latency and energy efficient Edge Inference (EI) for diverse Internet of Things (IoT) applications. However, traditional digital hardware for machine learning is power intensive, motivating the need for alternative computation paradigms. Over-The-Air (OTA) computation is regarded as an emerging transformative approach assigning the wireless channel to actively perform computational tasks. This article introduces the concept of Metasurfaces-Integrated Neural Networks (MINNs), a physical-layer-enabled deep learning framework that leverages programmable multi-layer metasurface structures and Multiple-Input Multiple-Output (MIMO) channels to realize computational layers in the wave propagation domain. The MINN system is conceptualized as three modules: Encoder, Channel (uncontrollable propagation features and metasurfaces), and Decoder. The first and last modules, realized respectively at the multi-antenna transmitter and receiver, consist of conventional digital or purposely designed analog Deep Neural Network (DNN) layers, and the metasurfaces responses of the Channel module are optimized alongside all modules as trainable weights. This architecture enables computation offloading into the end-to-end physical layer, flexibly among its constituent modules, achieving performance comparable to fully digital DNNs while significantly reducing power consumption. The training of the MINN framework, two representative variations, and performance results for indicative applications are presented, highlighting the potential of MINNs as a lightweight and sustainable solution for future EI-enabled wireless systems. The article is concluded with a list of open challenges and promising research directions.",
    "authors": [
      "Kyriakos Stylianopoulos",
      "Mario Edoardo Pandolfo",
      "Paolo Di Lorenzo",
      "George C. Alexandropoulos"
    ],
    "categories": [
      "cs.ET",
      "cs.LG",
      "eess.SP"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19312v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19312v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.19263v1",
    "title": "Prognostics of Multisensor Systems with Unknown and Unlabeled Failure Modes via Bayesian Nonparametric Process Mixtures",
    "summary": "Modern manufacturing systems often experience multiple and unpredictable failure behaviors, yet most existing prognostic models assume a fixed, known set of failure modes with labeled historical data. This assumption limits the use of digital twins for predictive maintenance, especially in high-mix or adaptive production environments, where new failure modes may emerge, and the failure mode labels may be unavailable.   To address these challenges, we propose a novel Bayesian nonparametric framework that unifies a Dirichlet process mixture module for unsupervised failure mode discovery with a neural network-based prognostic module. The key innovation lies in an iterative feedback mechanism to jointly learn two modules. These modules iteratively update one another to dynamically infer, expand, or merge failure modes as new data arrive while providing high prognostic accuracy.   Experiments on both simulation and aircraft engine datasets show that the proposed approach performs competitively with or significantly better than existing approaches. It also exhibits robust online adaptation capabilities, making it well-suited for digital-twin-based system health management in complex manufacturing environments.",
    "authors": [
      "Kani Fu",
      "Sanduni S Disanayaka Mudiyanselage",
      "Chunli Dai",
      "Minhee Kim"
    ],
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19263v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19263v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.19193v1",
    "title": "Visual Prompt Guided Unified Pushing Policy",
    "summary": "As one of the simplest non-prehensile manipulation skills, pushing has been widely studied as an effective means to rearrange objects. Existing approaches, however, typically rely on multi-step push plans composed of pre-defined pushing primitives with limited application scopes, which restrict their efficiency and versatility across different scenarios. In this work, we propose a unified pushing policy that incorporates a lightweight prompting mechanism into a flow matching policy to guide the generation of reactive, multimodal pushing actions. The visual prompt can be specified by a high-level planner, enabling the reuse of the pushing policy across a wide range of planning problems. Experimental results demonstrate that the proposed unified pushing policy not only outperforms existing baselines but also effectively serves as a low-level primitive within a VLM-guided planning framework to solve table-cleaning tasks efficiently.",
    "authors": [
      "Hieu Bui",
      "Ziyan Gao",
      "Yuya Hosoda",
      "Joo-Ho Lee"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19193v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19193v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.19331v1",
    "title": "Partial Soft-Matching Distance for Neural Representational Comparison with Partial Unit Correspondence",
    "summary": "Representational similarity metrics typically force all units to be matched, making them susceptible to noise and outliers common in neural representations. We extend the soft-matching distance to a partial optimal transport setting that allows some neurons to remain unmatched, yielding rotation-sensitive but robust correspondences. This partial soft-matching distance provides theoretical advantages -- relaxing strict mass conservation while maintaining interpretable transport costs -- and practical benefits through efficient neuron ranking in terms of cross-network alignment without costly iterative recomputation. In simulations, it preserves correct matches under outliers and reliably selects the correct model in noise-corrupted identification tasks. On fMRI data, it automatically excludes low-reliability voxels and produces voxel rankings by alignment quality that closely match computationally expensive brute-force approaches. It achieves higher alignment precision across homologous brain areas than standard soft-matching, which is forced to match all units regardless of quality. In deep networks, highly matched units exhibit similar maximally exciting images, while unmatched units show divergent patterns. This ability to partition by match quality enables focused analyses, e.g., testing whether networks have privileged axes even within their most aligned subpopulations. Overall, partial soft-matching provides a principled and practical method for representational comparison under partial correspondence.",
    "authors": [
      "Chaitanya Kapoor",
      "Alex H. Williams",
      "Meenakshi Khosla"
    ],
    "categories": [
      "cs.LG",
      "cs.NE",
      "stat.ML"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19331v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19331v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19327v1",
    "title": "Soft Sequence Policy Optimization: Bridging GMPO and SAPO",
    "summary": "A significant portion of recent research on Large Language Model (LLM) alignment focuses on developing new policy optimization methods based on Group Relative Policy Optimization (GRPO). Two prominent directions have emerged: (i) a shift toward sequence-level importance sampling weights that better align with the sequence-level rewards used in many tasks, and (ii) alternatives to PPO-style clipping that aim to avoid the associated loss of training signal and entropy collapse. Recent work, such as Soft Adaptive Policy Optimization (SAPO), reformulates the Scopic objective within the GRPO framework and achieves both sequence coherence and token adaptivity. Geometric-Mean Policy Optimization (GMPO) leverages token-wise ratio clipping within sequence importance sampling weights. Building on these ideas, this work proposes a new objective that promotes effective policy exploration while maintaining training stability. Specifically, we introduce Soft Sequence Policy Optimization, an off-policy reinforcement learning objective that incorporates soft gating functions over token-level probability ratios within sequence-level importance weights.",
    "authors": [
      "Svetlana Glazyrina",
      "Maksim Kryzhanovskiy",
      "Roman Ischenko"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19327v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19327v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19314v1",
    "title": "IPv2: An Improved Image Purification Strategy for Real-World Ultra-Low-Dose Lung CT Denoising",
    "summary": "The image purification strategy constructs an intermediate distribution with aligned anatomical structures, which effectively corrects the spatial misalignment between real-world ultra-low-dose CT and normal-dose CT images and significantly enhances the structural preservation ability of denoising models. However, this strategy exhibits two inherent limitations. First, it suppresses noise only in the chest wall and bone regions while leaving the image background untreated. Second, it lacks a dedicated mechanism for denoising the lung parenchyma. To address these issues, we systematically redesign the original image purification strategy and propose an improved version termed IPv2. The proposed strategy introduces three core modules, namely Remove Background, Add noise, and Remove noise. These modules endow the model with denoising capability in both background and lung tissue regions during training data construction and provide a more reasonable evaluation protocol through refined label construction at the testing stage. Extensive experiments on our previously established real-world patient lung CT dataset acquired at 2% radiation dose demonstrate that IPv2 consistently improves background suppression and lung parenchyma restoration across multiple mainstream denoising models. The code is publicly available at https://github.com/MonkeyDadLufy/Image-Purification-Strategy-v2.",
    "authors": [
      "Guoliang Gong",
      "Man Yu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19314v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19314v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19244v1",
    "title": "Robust Exploration in Directed Controller Synthesis via Reinforcement Learning with Soft Mixture-of-Experts",
    "summary": "On-the-fly Directed Controller Synthesis (OTF-DCS) mitigates state-space explosion by incrementally exploring the system and relies critically on an exploration policy to guide search efficiently. Recent reinforcement learning (RL) approaches learn such policies and achieve promising zero-shot generalization from small training instances to larger unseen ones. However, a fundamental limitation is anisotropic generalization, where an RL policy exhibits strong performance only in a specific region of the domain-parameter space while remaining fragile elsewhere due to training stochasticity and trajectory-dependent bias. To address this, we propose a Soft Mixture-of-Experts framework that combines multiple RL experts via a prior-confidence gating mechanism and treats these anisotropic behaviors as complementary specializations. The evaluation on the Air Traffic benchmark shows that Soft-MoE substantially expands the solvable parameter space and improves robustness compared to any single expert.",
    "authors": [
      "Toshihide Ubukata",
      "Zhiyao Wang",
      "Enhong Mu",
      "Jialong Li",
      "Kenji Tei"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19244v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19244v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19225v1",
    "title": "Proximity-Based Multi-Turn Optimization: Practical Credit Assignment for LLM Agent Training",
    "summary": "Multi-turn LLM agents are becoming pivotal to production systems, spanning customer service automation, e-commerce assistance, and interactive task management, where accurately distinguishing high-value informative signals from stochastic noise is critical for sample-efficient training. In real-world scenarios, a failure in a trivial task may reflect random instability, whereas success in a high-difficulty task signifies a genuine capability breakthrough. Yet, existing group-based policy optimization methods rigidly rely on statistical deviation within discrete batches, frequently misallocating credit when task difficulty fluctuates. To address this issue, we propose Proximity-based Multi-turn Optimization (ProxMO), a practical and robust framework engineered specifically for the constraints of real-world deployment. ProxMO integrates global context via two lightweight mechanisms: success-rate-aware modulation dynamically adapts gradient intensity based on episode-level difficulty, while proximity-based soft aggregation derives baselines through continuous semantic weighting at the step level. Extensive evaluations on ALFWorld and WebShop benchmarks demonstrate that ProxMO yields substantial performance gains over existing baselines with negligible computational cost. Ablation studies further validate the independent and synergistic efficacy of both mechanisms. Crucially, ProxMO offers plug-and-play compatibility with standard GRPO frameworks, facilitating immediate, low-friction adoption in existing industrial training pipelines. Our implementation is available at: \\href{https://anonymous.4open.science/r/proxmo-B7E7/README.md}{https://anonymous.4open.science/r/proxmo}.",
    "authors": [
      "Yangyi Fang",
      "Jiaye Lin",
      "Xiaoliang Fu",
      "Cong Qin",
      "Haolin Shi",
      "Chang Liu",
      "Peilin Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19225v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19225v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19207v1",
    "title": "HybridFL: A Federated Learning Approach for Financial Crime Detection",
    "summary": "Federated learning (FL) is a privacy-preserving machine learning paradigm that enables multiple parties to collaboratively train models on privately owned data without sharing raw information. While standard FL typically addresses either horizontal or vertical data partitions, many real-world scenarios exhibit a complex hybrid distribution. This paper proposes Hybrid Federated Learning (HybridFL) to address data split both horizontally across disjoint users and vertically across complementary feature sets. We evaluate HybridFL in a financial crime detection context, where a transaction party holds transaction-level attributes and multiple banks maintain private account-level features. By integrating horizontal aggregation and vertical feature fusion, the proposed architecture enables joint learning while strictly preserving data locality. Experiments on AMLSim and SWIFT datasets demonstrate that HybridFL significantly outperforms the transaction-only local model and achieves performance comparable to a centralized benchmark.",
    "authors": [
      "Afsana Khan",
      "Marijn ten Thij",
      "Guangzhi Tang",
      "Anna Wilbik"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19207v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19207v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19079v1",
    "title": "TriTopic: Tri-Modal Graph-Based Topic Modeling with Iterative Refinement and Archetypes",
    "summary": "Topic modeling extracts latent themes from large text collections, but leading approaches like BERTopic face critical limitations: stochastic instability, loss of lexical precision (\"Embedding Blur\"), and reliance on a single data perspective.   We present TriTopic, a framework that addresses these weaknesses through a tri-modal graph fusing semantic embeddings, TF-IDF, and metadata. Three core innovations drive its performance: hybrid graph construction via Mutual kNN and Shared Nearest Neighbors to eliminate noise and combat the curse of dimensionality; Consensus Leiden Clustering for reproducible, stable partitions; and Iterative Refinement that sharpens embeddings through dynamic centroid-pulling. TriTopic also replaces the \"average document\" concept with archetype-based topic representations defined by boundary cases rather than centers alone.   In benchmarks across 20 Newsgroups, BBC News, AG News, and Arxiv, TriTopic achieves the highest NMI on every dataset (mean NMI 0.575 vs. 0.513 for BERTopic, 0.416 for NMF, 0.299 for LDA), guarantees 100% corpus coverage with 0% outliers, and is available as an open-source PyPI library.",
    "authors": [
      "Roman Egger"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19079v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19079v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.18986v1",
    "title": "Quantifying Automation Risk in High-Automation AI Systems: A Bayesian Framework for Failure Propagation and Optimal Oversight",
    "summary": "Organizations across finance, healthcare, transportation, content moderation, and critical infrastructure are rapidly deploying highly automated AI systems, yet they lack principled methods to quantify how increasing automation amplifies harm when failures occur. We propose a parsimonious Bayesian risk decomposition expressing expected loss as the product of three terms: the probability of system failure, the conditional probability that a failure propagates into harm given the automation level, and the expected severity of harm. This framework isolates a critical quantity -- the conditional probability that failures propagate into harm -- which captures execution and oversight risk rather than model accuracy alone. We develop complete theoretical foundations: formal proofs of the decomposition, a harm propagation equivalence theorem linking the harm propagation probability to observable execution controls, risk elasticity measures, efficient frontier analysis for automation policy, and optimal resource allocation principles with second-order conditions. We motivate the framework with an illustrative case study of the 2012 Knight Capital incident ($440M loss) as one instantiation of a broadly applicable failure pattern, and characterize the research design required to empirically validate the framework at scale across deployment domains. This work provides the theoretical foundations for a new class of deployment-focused risk governance tools for agentic and automated AI systems.",
    "authors": [
      "Vishal Srivastava",
      "Tanmay Sah"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18986v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18986v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.19333v1",
    "title": "PerSoMed: A Large-Scale Balanced Dataset for Persian Social Media Text Classification",
    "summary": "This research introduces the first large-scale, well-balanced Persian social media text classification dataset, specifically designed to address the lack of comprehensive resources in this domain. The dataset comprises 36,000 posts across nine categories (Economic, Artistic, Sports, Political, Social, Health, Psychological, Historical, and Science & Technology), each containing 4,000 samples to ensure balanced class distribution. Data collection involved 60,000 raw posts from various Persian social media platforms, followed by rigorous preprocessing and hybrid annotation combining ChatGPT-based few-shot prompting with human verification. To mitigate class imbalance, we employed undersampling with semantic redundancy removal and advanced data augmentation strategies integrating lexical replacement and generative prompting. We benchmarked several models, including BiLSTM, XLM-RoBERTa (with LoRA and AdaLoRA adaptations), FaBERT, SBERT-based architectures, and the Persian-specific TookaBERT (Base and Large). Experimental results show that transformer-based models consistently outperform traditional neural networks, with TookaBERT-Large achieving the best performance (Precision: 0.9622, Recall: 0.9621, F1- score: 0.9621). Class-wise evaluation further confirms robust performance across all categories, though social and political texts exhibited slightly lower scores due to inherent ambiguity. This research presents a new high-quality dataset and provides comprehensive evaluations of cutting-edge models, establishing a solid foundation for further developments in Persian NLP, including trend analysis, social behavior modeling, and user classification. The dataset is publicly available to support future research endeavors.",
    "authors": [
      "Isun Chehreh",
      "Ebrahim Ansari"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.SI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19333v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19333v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.19316v1",
    "title": "Pay Attention to CTC: Fast and Robust Pseudo-Labelling for Unified Speech Recognition",
    "summary": "Unified Speech Recognition (USR) has emerged as a semi-supervised framework for training a single model for audio, visual, and audiovisual speech recognition, achieving state-of-the-art results on in-distribution benchmarks. However, its reliance on autoregressive pseudo-labelling makes training expensive, while its decoupled supervision of CTC and attention branches increases susceptibility to self-reinforcing errors, particularly under distribution shifts involving longer sequences, noise, or unseen domains. We propose CTC-driven teacher forcing, where greedily decoded CTC pseudo-labels are fed into the decoder to generate attention targets in a single forward pass. Although these can be globally incoherent, in the pseudo-labelling setting they enable efficient and effective knowledge transfer. Because CTC and CTC-driven attention pseudo-labels have the same length, the decoder can predict both simultaneously, benefiting from the robustness of CTC and the expressiveness of attention without costly beam search. We further propose mixed sampling to mitigate the exposure bias of the decoder relying solely on CTC inputs. The resulting method, USR 2.0, halves training time, improves robustness to out-of-distribution inputs, and achieves state-of-the-art results on LRS3, LRS2, and WildVSR, surpassing USR and modality-specific self-supervised baselines.",
    "authors": [
      "Alexandros Haliassos",
      "Rodrigo Mira",
      "Stavros Petridis"
    ],
    "categories": [
      "cs.CV",
      "cs.SD"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19316v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19316v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.19219v1",
    "title": "Controlled Face Manipulation and Synthesis for Data Augmentation",
    "summary": "Deep learning vision models excel with abundant supervision, but many applications face label scarcity and class imbalance. Controllable image editing can augment scarce labeled data, yet edits often introduce artifacts and entangle non-target attributes. We study this in facial expression analysis, targeting Action Unit (AU) manipulation where annotation is costly and AU co-activation drives entanglement. We present a facial manipulation method that operates in the semantic latent space of a pre-trained face generator (Diffusion Autoencoder). Using lightweight linear models, we reduce entanglement of semantic features via (i) dependency-aware conditioning that accounts for AU co-activation, and (ii) orthogonal projection that removes nuisance attribute directions (e.g., glasses), together with an expression neutralization step to enable absolute AU edit. We use these edits to balance AU occurrence by editing labeled faces and to diversify identities/demographics via controlled synthesis. Augmenting AU detector training with the generated data improves accuracy and yields more disentangled predictions with fewer co-activation shortcuts, outperforming alternative data-efficient training strategies and suggesting improvements similar to what would require substantially more labeled data in our learning-curve analysis. Compared to prior methods, our edits are stronger, produce fewer artifacts, and preserve identity better.",
    "authors": [
      "Joris Kirchner",
      "Amogh Gudi",
      "Marian Bittner",
      "Chirag Raman"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19219v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19219v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.19140v1",
    "title": "CaReFlow: Cyclic Adaptive Rectified Flow for Multimodal Fusion",
    "summary": "Modality gap significantly restricts the effectiveness of multimodal fusion. Previous methods often use techniques such as diffusion models and adversarial learning to reduce the modality gap, but they typically focus on one-to-one alignment without exposing the data points of the source modality to the global distribution information of the target modality. To this end, leveraging the characteristic of rectified flow that can map one distribution to another via a straight trajectory, we extend rectified flow for modality distribution mapping. Specifically, we leverage the `one-to-many mapping' strategy in rectified flow that allows each data point of the source modality to observe the overall target distribution. This also alleviates the issue of insufficient paired data within each sample, enabling a more robust distribution transformation. Moreover, to achieve more accurate distribution mapping and address the ambiguous flow directions in one-to-many mapping, we design `adaptive relaxed alignment', enforcing stricter alignment for modality pairs belonging to the same sample, while applying relaxed mapping for pairs not belonging to the same sample or category. Additionally, to prevent information loss during distribution mapping, we introduce `cyclic rectified flow' to ensure the transferred features can be translated back to the original features, allowing multimodal representations to learn sufficient modality-specific information. After distribution alignment, our approach achieves very competitive results on multiple tasks of multimodal affective computing even with a simple fusion method, and visualizations verify that it can effectively reduce the modality gap.",
    "authors": [
      "Sijie Mai",
      "Shiqin Han"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19140v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19140v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.19087v1",
    "title": "Detecting Cybersecurity Threats by Integrating Explainable AI with SHAP Interpretability and Strategic Data Sampling",
    "summary": "The critical need for transparent and trustworthy machine learning in cybersecurity operations drives the development of this integrated Explainable AI (XAI) framework. Our methodology addresses three fundamental challenges in deploying AI for threat detection: handling massive datasets through Strategic Sampling Methodology that preserves class distributions while enabling efficient model development; ensuring experimental rigor via Automated Data Leakage Prevention that systematically identifies and removes contaminated features; and providing operational transparency through Integrated XAI Implementation using SHAP analysis for model-agnostic interpretability across algorithms. Applied to the CIC-IDS2017 dataset, our approach maintains detection efficacy while reducing computational overhead and delivering actionable explanations for security analysts. The framework demonstrates that explainability, computational efficiency, and experimental integrity can be simultaneously achieved, providing a robust foundation for deploying trustworthy AI systems in security operations centers where decision transparency is paramount.",
    "authors": [
      "Norrakith Srisumrith",
      "Sunantha Sodsee"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19087v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19087v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.19063v1",
    "title": "Direction-aware 3D Large Multimodal Models",
    "summary": "3D large multimodal models (3D LMMs) rely heavily on ego poses for enabling directional question-answering and spatial reasoning. However, most existing point cloud benchmarks contain rich directional queries but lack the corresponding ego poses, making them inherently ill-posed in 3D large multimodal modelling. In this work, we redefine a new and rigorous paradigm that enables direction-aware 3D LMMs by identifying and supplementing ego poses into point cloud benchmarks and transforming the corresponding point cloud data according to the identified ego poses. We enable direction-aware 3D LMMs with two novel designs. The first is PoseRecover, a fully automatic pose recovery pipeline that matches questions with ego poses from RGB-D video extrinsics via object-frustum intersection and visibility check with Z-buffers. The second is PoseAlign that transforms the point cloud data to be aligned with the identified ego poses instead of either injecting ego poses into textual prompts or introducing pose-encoded features in the projection layers. Extensive experiments show that our designs yield consistent improvements across multiple 3D LMM backbones such as LL3DA, LL3DA-SONATA, Chat-Scene, and 3D-LLAVA, improving ScanRefer mIoU by 30.0% and Scan2Cap LLM-as-judge accuracy by 11.7%. In addition, our approach is simple, generic, and training-efficient, requiring only instruction tuning while establishing a strong baseline for direction-aware 3D-LMMs.",
    "authors": [
      "Quan Liu",
      "Weihao Xuan",
      "Junjue Wang",
      "Naoto Yokoya",
      "Ling Shao",
      "Shijian Lu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19063v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19063v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.19320v1",
    "title": "Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations",
    "summary": "Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.",
    "authors": [
      "Dongming Jiang",
      "Yi Li",
      "Songtao Wei",
      "Jinxin Yang",
      "Ayushi Kishore",
      "Alysa Zhao",
      "Dingyi Kang",
      "Xu Hu",
      "Feng Chen",
      "Qiannan Li",
      "Bingzhe Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19320v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19320v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.19297v1",
    "title": "Automated Generation of Microfluidic Netlists using Large Language Models",
    "summary": "Microfluidic devices have emerged as powerful tools in various laboratory applications, but the complexity of their design limits accessibility for many practitioners. While progress has been made in microfluidic design automation (MFDA), a practical and intuitive solution is still needed to connect microfluidic practitioners with MFDA techniques. This work introduces the first practical application of large language models (LLMs) in this context, providing a preliminary demonstration. Building on prior research in hardware description language (HDL) code generation with LLMs, we propose an initial methodology to convert natural language microfluidic device specifications into system-level structural Verilog netlists. We demonstrate the feasibility of our approach by generating structural netlists for practical benchmarks representative of typical microfluidic designs with correct functional flow and an average syntactical accuracy of 88%.",
    "authors": [
      "Jasper Davidson",
      "Skylar Stockham",
      "Allen Boston",
      "Ashton Snelgrove. Valerio Tenace",
      "Pierre-Emmanuel Gaillardon"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19297v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19297v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.19281v1",
    "title": "Limited Reasoning Space: The cage of long-horizon reasoning in LLMs",
    "summary": "The test-time compute strategy, such as Chain-of-Thought (CoT), has significantly enhanced the ability of large language models to solve complex tasks like logical reasoning. However, empirical studies indicate that simply increasing the compute budget can sometimes lead to a collapse in test-time performance when employing typical task decomposition strategies such as CoT. This work hypothesizes that reasoning failures with larger compute budgets stem from static planning methods, which hardly perceive the intrinsic boundaries of LLM reasoning. We term it as the Limited Reasoning Space hypothesis and perform theoretical analysis through the lens of a non-autonomous stochastic dynamical system. This insight suggests that there is an optimal range for compute budgets; over-planning can lead to redundant feedback and may even impair reasoning capabilities. To exploit the compute-scaling benefits and suppress over-planning, this work proposes Halo, a model predictive control framework for LLM planning. Halo is designed for long-horizon tasks with reason-based planning and crafts an entropy-driven dual controller, which adopts a Measure-then-Plan strategy to achieve controllable reasoning. Experimental results demonstrate that Halo outperforms static baselines on complex long-horizon tasks by dynamically regulating planning at the reasoning boundary.",
    "authors": [
      "Zhenyu Li",
      "Guanlin Wu",
      "Cheems Wang",
      "Yongqiang Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19281v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19281v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.19115v1",
    "title": "How Do LLMs Encode Scientific Quality? An Empirical Study Using Monosemantic Features from Sparse Autoencoders",
    "summary": "In recent years, there has been a growing use of generative AI, and large language models (LLMs) in particular, to support both the assessment and generation of scientific work. Although some studies have shown that LLMs can, to a certain extent, evaluate research according to perceived quality, our understanding of the internal mechanisms that enable this capability remains limited. This paper presents the first study that investigates how LLMs encode the concept of scientific quality through relevant monosemantic features extracted using sparse autoencoders. We derive such features under different experimental settings and assess their ability to serve as predictors across three tasks related to research quality: predicting citation count, journal SJR, and journal h-index. The results indicate that LLMs encode features associated with multiple dimensions of scientific quality. In particular, we identify four recurring types of features that capture key aspects of how research quality is represented: 1) features reflecting research methodologies; 2) features related to publication type, with literature reviews typically exhibiting higher impact; 3) features associated with high-impact research fields and technologies; and 4) features corresponding to specific scientific jargons. These findings represent an important step toward understanding how LLMs encapsulate concepts related to research quality.",
    "authors": [
      "Michael McCoubrey",
      "Angelo Salatino",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19115v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19115v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.19027v1",
    "title": "Pushing the Limits of Inverse Lithography with Generative Reinforcement Learning",
    "summary": "Inverse lithography (ILT) is critical for modern semiconductor manufacturing but suffers from highly non-convex objectives that often trap optimization in poor local minima. Generative AI has been explored to warm-start ILT, yet most approaches train deterministic image-to-image translators to mimic sub-optimal datasets, providing limited guidance for escaping non-convex traps during refinement. We reformulate mask synthesis as conditional sampling: a generator learns a distribution over masks conditioned on the design and proposes multiple candidates. The generator is first pretrained with WGAN plus a reconstruction loss, then fine-tuned using Group Relative Policy Optimization (GRPO) with an ILT-guided imitation loss. At inference, we sample a small batch of masks, run fast batched ILT refinement, evaluate lithography metrics (e.g., EPE, process window), and select the best candidate. On \\texttt{LithoBench} dataset, the proposed hybrid framework reduces EPE violations under a 3\\,nm tolerance and roughly doubles throughput versus a strong numerical ILT baseline, while improving final mask quality. We also present over 20\\% EPE improvement on \\texttt{ICCAD13} contest cases with 3$\\times$ speedup over the SOTA numerical ILT solver. By learning to propose ILT-friendly initializations, our approach mitigates non-convexity and advances beyond what traditional solvers or GenAI can achieve.",
    "authors": [
      "Haoyu Yang",
      "Haoxing Ren"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19027v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19027v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.19020v1",
    "title": "Learning to Detect Language Model Training Data via Active Reconstruction",
    "summary": "Detecting LLM training data is generally framed as a membership inference attack (MIA) problem. However, conventional MIAs operate passively on fixed model weights, using log-likelihoods or text generations. In this work, we introduce \\textbf{Active Data Reconstruction Attack} (ADRA), a family of MIA that actively induces a model to reconstruct a given text through training. We hypothesize that training data are \\textit{more reconstructible} than non-members, and the difference in their reconstructibility can be exploited for membership inference. Motivated by findings that reinforcement learning (RL) sharpens behaviors already encoded in weights, we leverage on-policy RL to actively elicit data reconstruction by finetuning a policy initialized from the target model. To effectively use RL for MIA, we design reconstruction metrics and contrastive rewards. The resulting algorithms, \\textsc{ADRA} and its adaptive variant \\textsc{ADRA+}, improve both reconstruction and detection given a pool of candidate data. Experiments show that our methods consistently outperform existing MIAs in detecting pre-training, post-training, and distillation data, with an average improvement of 10.7\\% over the previous runner-up. In particular, \\MethodPlus~improves over Min-K\\%++ by 18.8\\% on BookMIA for pre-training detection and by 7.6\\% on AIME for post-training detection.",
    "authors": [
      "Junjie Oscar Yin",
      "John X. Morris",
      "Vitaly Shmatikov",
      "Sewon Min",
      "Hannaneh Hajishirzi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19020v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19020v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.19208v1",
    "title": "How to Allocate, How to Learn? Dynamic Rollout Allocation and Advantage Modulation for Policy Optimization",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective for Large Language Model (LLM) reasoning, yet current methods face key challenges in resource allocation and policy optimization dynamics: (i) uniform rollout allocation ignores gradient variance heterogeneity across problems, and (ii) the softmax policy structure causes gradient attenuation for high-confidence correct actions, while excessive gradient updates may destabilize training. Therefore, we propose DynaMO, a theoretically-grounded dual-pronged optimization framework. At the sequence level, we prove that uniform allocation is suboptimal and derive variance-minimizing allocation from the first principle, establishing Bernoulli variance as a computable proxy for gradient informativeness. At the token level, we develop gradient-aware advantage modulation grounded in theoretical analysis of gradient magnitude bounds. Our framework compensates for gradient attenuation of high-confidence correct actions while utilizing entropy changes as computable indicators to stabilize excessive update magnitudes. Extensive experiments conducted on a diverse range of mathematical reasoning benchmarks demonstrate consistent improvements over strong RLVR baselines. Our implementation is available at: \\href{https://anonymous.4open.science/r/dynamo-680E/README.md}{https://anonymous.4open.science/r/dynamo}.",
    "authors": [
      "Yangyi Fang",
      "Jiaye Lin",
      "Xiaoliang Fu",
      "Cong Qin",
      "Haolin Shi",
      "Chaowen Hu",
      "Lu Pan",
      "Ke Zeng",
      "Xunliang Cai"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19208v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19208v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.19190v1",
    "title": "FUSAR-GPT : A Spatiotemporal Feature-Embedded and Two-Stage Decoupled Visual Language Model for SAR Imagery",
    "summary": "Research on the intelligent interpretation of all-weather, all-time Synthetic Aperture Radar (SAR) is crucial for advancing remote sensing applications. In recent years, although Visual Language Models (VLMs) have demonstrated strong open-world understanding capabilities on RGB images, their performance is severely limited when directly applied to the SAR field due to the complexity of the imaging mechanism, sensitivity to scattering features, and the scarcity of high-quality text corpora. To systematically address this issue, we constructed the inaugural SAR Image-Text-AlphaEarth feature triplet dataset and developed FUSAR-GPT, a VLM specifically for SAR. FUSAR-GPT innovatively introduces a geospatial baseline model as a 'world knowledge' prior and embeds multi-source remote-sensing temporal features into the model's visual backbone via 'spatiotemporal anchors', enabling dynamic compensation for the sparse representation of targets in SAR images. Furthermore, we designed a two-stage SFT strategy to decouple the knowledge injection and task execution of large models. The spatiotemporal feature embedding and the two-stage decoupling paradigm enable FUSAR-GPT to achieve state-of-the-art performance across several typical remote sensing visual-language benchmark tests, significantly outperforming mainstream baseline models by over 12%.",
    "authors": [
      "Xiaokun Zhang",
      "Yi Yang",
      "Ziqi Ye",
      " Baiyun",
      "Xiaorong Guo",
      "Qingchen Fang",
      "Ruyi Zhang",
      "Xinpeng Zhou",
      "Haipeng Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19190v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19190v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.19177v1",
    "title": "Next Reply Prediction X Dataset: Linguistic Discrepancies in Naively Generated Content",
    "summary": "The increasing use of Large Language Models (LLMs) as proxies for human participants in social science research presents a promising, yet methodologically risky, paradigm shift. While LLMs offer scalability and cost-efficiency, their \"naive\" application, where they are prompted to generate content without explicit behavioral constraints, introduces significant linguistic discrepancies that challenge the validity of research findings. This paper addresses these limitations by introducing a novel, history-conditioned reply prediction task on authentic X (formerly Twitter) data, to create a dataset designed to evaluate the linguistic output of LLMs against human-generated content. We analyze these discrepancies using stylistic and content-based metrics, providing a quantitative framework for researchers to assess the quality and authenticity of synthetic data. Our findings highlight the need for more sophisticated prompting techniques and specialized datasets to ensure that LLM-generated content accurately reflects the complex linguistic patterns of human communication, thereby improving the validity of computational social science studies.",
    "authors": [
      "Simon M\u00fcnker",
      "Nils Schwager",
      "Kai Kugler",
      "Michael Heseltine",
      "Achim Rettinger"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19177v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19177v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.19166v1",
    "title": "CosyAccent: Duration-Controllable Accent Normalization Using Source-Synthesis Training Data",
    "summary": "Accent normalization (AN) systems often struggle with unnatural outputs and undesired content distortion, stemming from both suboptimal training data and rigid duration modeling. In this paper, we propose a \"source-synthesis\" methodology for training data construction. By generating source L2 speech and using authentic native speech as the training target, our approach avoids learning from TTS artifacts and, crucially, requires no real L2 data in training. Alongside this data strategy, we introduce CosyAccent, a non-autoregressive model that resolves the trade-off between prosodic naturalness and duration control. CosyAccent implicitly models rhythm for flexibility yet offers explicit control over total output duration. Experiments show that, despite being trained without any real L2 speech, CosyAccent achieves significantly improved content preservation and superior naturalness compared to strong baselines trained on real-world data.",
    "authors": [
      "Qibing Bai",
      "Shuhao Shi",
      "Shuai Wang",
      "Yukai Ju",
      "Yannan Wang",
      "Haizhou Li"
    ],
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19166v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19166v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.19113v1",
    "title": "Learning from Complexity: Exploring Dynamic Sample Pruning of Spatio-Temporal Training",
    "summary": "Spatio-temporal forecasting is fundamental to intelligent systems in transportation, climate science, and urban planning. However, training deep learning models on the massive, often redundant, datasets from these domains presents a significant computational bottleneck. Existing solutions typically focus on optimizing model architectures or optimizers, while overlooking the inherent inefficiency of the training data itself. This conventional approach of iterating over the entire static dataset each epoch wastes considerable resources on easy-to-learn or repetitive samples. In this paper, we explore a novel training-efficiency techniques, namely learning from complexity with dynamic sample pruning, ST-Prune, for spatio-temporal forecasting. Through dynamic sample pruning, we aim to intelligently identify the most informative samples based on the model's real-time learning state, thereby accelerating convergence and improving training efficiency. Extensive experiments conducted on real-world spatio-temporal datasets show that ST-Prune significantly accelerates the training speed while maintaining or even improving the model performance, and it also has scalability and universality.",
    "authors": [
      "Wei Chen",
      "Junle Chen",
      "Yuqian Wu",
      "Yuxuan Liang",
      "Xiaofang Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19113v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19113v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.19043v1",
    "title": "Uncovering Context Reliance in Unstructured Knowledge Editing",
    "summary": "Editing Large language models (LLMs) with real-world, unstructured knowledge is essential for correcting and updating their internal parametric knowledge. In this work, we revisit the fundamental next-token prediction (NTP) as a candidate paradigm for unstructured editing. We identify Context Reliance as a critical failure mode of NTP-based approaches, where knowledge acquired from edited text becomes highly dependent on its preceding context, leading to recall failures when that context is absent during inference. This hypothesis is supported by our empirical validation that prepending context during inference recovers knowledge recall. We further theoretically demonstrate that Context Reliance is an inherent consequence of gradient-based optimization, which tends to bind acquired knowledge to a specific aggregated contextual representation. To address this, we propose a simple yet effective COntext-INdependent editing framework (COIN), encouraging model to focus on knowledge within local scope rather than memorizing contextual patterns. Evaluations show that COIN reduces Context Reliance by 45.2% and outperforms strong baselines by 23.6% in editing success rate, highlighting the vital role of mitigating Context Reliance for robust editing.",
    "authors": [
      "Zisheng Zhou",
      "Mengqi Zhang",
      "Shiguang Wu",
      "Xiaotian Ye",
      "Chi Zhang",
      "Zhumin Chen",
      "Pengjie Ren"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19043v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19043v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.19019v1",
    "title": "TokenTrace: Multi-Concept Attribution through Watermarked Token Recovery",
    "summary": "Generative AI models pose a significant challenge to intellectual property (IP), as they can replicate unique artistic styles and concepts without attribution. While watermarking offers a potential solution, existing methods often fail in complex scenarios where multiple concepts (e.g., an object and an artistic style) are composed within a single image. These methods struggle to disentangle and attribute each concept individually. In this work, we introduce TokenTrace, a novel proactive watermarking framework for robust, multi-concept attribution. Our method embeds secret signatures into the semantic domain by simultaneously perturbing the text prompt embedding and the initial latent noise that guide the diffusion model's generation process. For retrieval, we propose a query-based TokenTrace module that takes the generated image and a textual query specifying which concepts need to be retrieved (e.g., a specific object or style) as inputs. This query-based mechanism allows the module to disentangle and independently verify the presence of multiple concepts from a single generated image. Extensive experiments show that our method achieves state-of-the-art performance on both single-concept (object and style) and multi-concept attribution tasks, significantly outperforming existing baselines while maintaining high visual quality and robustness to common transformations.",
    "authors": [
      "Li Zhang",
      "Shruti Agarwal",
      "John Collomosse",
      "Pengtao Xie",
      "Vishal Asnani"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19019v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19019v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.19005v1",
    "title": "GUIDE-US: Grade-Informed Unpaired Distillation of Encoder Knowledge from Histopathology to Micro-UltraSound",
    "summary": "Purpose: Non-invasive grading of prostate cancer (PCa) from micro-ultrasound (micro-US) could expedite triage and guide biopsies toward the most aggressive regions, yet current models struggle to infer tissue micro-structure at coarse imaging resolutions.   Methods: We introduce an unpaired histopathology knowledge-distillation strategy that trains a micro-US encoder to emulate the embedding distribution of a pretrained histopathology foundation model, conditioned on International Society of Urological Pathology (ISUP) grades. Training requires no patient-level pairing or image registration, and histopathology inputs are not used at inference.   Results: Compared to the current state of the art, our approach increases sensitivity to clinically significant PCa (csPCa) at 60% specificity by 3.5% and improves overall sensitivity at 60% specificity by 1.2%.   Conclusion: By enabling earlier and more dependable cancer risk stratification solely from imaging, our method advances clinical feasibility. Source code will be publicly released upon publication.",
    "authors": [
      "Emma Willis",
      "Tarek Elghareb",
      "Paul F. R. Wilson",
      "Minh Nguyen Nhat To",
      "Mohammad Mahdi Abootorabi",
      "Amoon Jamzad",
      "Brian Wodlinger",
      "Parvin Mousavi",
      "Purang Abolmaesumi"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19005v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19005v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.18985v1",
    "title": "InfEngine: A Self-Verifying and Self-Optimizing Intelligent Engine for Infrared Radiation Computing",
    "summary": "Infrared radiation computing underpins advances in climate science, remote sensing and spectroscopy but remains constrained by manual workflows. We introduce InfEngine, an autonomous intelligent computational engine designed to drive a paradigm shift from human-led orchestration to collaborative automation. It integrates four specialized agents through two core innovations: self-verification, enabled by joint solver-evaluator debugging, improves functional correctness and scientific plausibility; self-optimization, realized via evolutionary algorithms with self-discovered fitness functions, facilitates autonomous performance optimization. Evaluated on InfBench with 200 infrared-specific tasks and powered by InfTools with 270 curated tools, InfEngine achieves a 92.7% pass rate and delivers workflows 21x faster than manual expert effort. More fundamentally, it illustrates how researchers can transition from manual coding to collaborating with self-verifying, self-optimizing computational partners. By generating reusable, verified and optimized code, InfEngine transforms computational workflows into persistent scientific assets, accelerating the cycle of scientific discovery. Code: https://github.com/kding1225/infengine",
    "authors": [
      "Kun Ding",
      "Jian Xu",
      "Ying Wang",
      "Peipei Yang",
      "Shiming Xiang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18985v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18985v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.19324v1",
    "title": "RetinaVision: XAI-Driven Augmented Regulation for Precise Retinal Disease Classification using deep learning framework",
    "summary": "Early and accurate classification of retinal diseases is critical to counter vision loss and for guiding clinical management of retinal diseases. In this study, we proposed a deep learning method for retinal disease classification utilizing optical coherence tomography (OCT) images from the Retinal OCT Image Classification - C8 dataset (comprising 24,000 labeled images spanning eight conditions). Images were resized to 224x224 px and tested on convolutional neural network (CNN) architectures: Xception and InceptionV3. Data augmentation techniques (CutMix, MixUp) were employed to enhance model generalization. Additionally, we applied GradCAM and LIME for interpretability evaluation. We implemented this in a real-world scenario via our web application named RetinaVision. This study found that Xception was the most accurate network (95.25%), followed closely by InceptionV3 (94.82%). These results suggest that deep learning methods allow effective OCT retinal disease classification and highlight the importance of implementing accuracy and interpretability for clinical applications.",
    "authors": [
      "Mohammad Tahmid Noor",
      "Shayan Abrar",
      "Jannatul Adan Mahi",
      "Md Parvez Mia",
      "Asaduzzaman Hridoy",
      "Samanta Ghosh"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19324v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19324v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19271v1",
    "title": "Taming Preconditioner Drift: Unlocking the Potential of Second-Order Optimizers for Federated Learning on Non-IID Data",
    "summary": "Second-order optimizers can significantly accelerate large-scale training, yet their naive federated variants are often unstable or even diverge on non-IID data.   We show that a key culprit is \\emph{preconditioner drift}: client-side second-order training induces heterogeneous \\emph{curvature-defined geometries} (i.e., preconditioner coordinate systems), and server-side model averaging updates computed under incompatible metrics, corrupting the global descent direction.   To address this geometric mismatch, we propose \\texttt{FedPAC}, a \\emph{preconditioner alignment and correction} framework for reliable federated second-order optimization.   \\texttt{FedPAC} explicitly decouples parameter aggregation from geometry synchronization by:   (i) \\textbf{Alignment} (i.e.,aggregating local preconditioners into a global reference and warm-starting clients via global preconditioner); and   (ii) \\textbf{Correction} (i.e., steering local preconditioned updates using a global preconditioned direction to suppress long-term drift).   We provide drift-coupled non-convex convergence guarantees with linear speedup under partial participation.   Empirically, \\texttt{FedPAC} consistently improves stability and accuracy across vision and language tasks, achieving up to $5.8\\%$ absolute accuracy gain on CIFAR-100 with ViTs.   Code is available at https://anonymous.4open.science/r/FedPAC-8B24.",
    "authors": [
      "Junkang Liu",
      "Fanhua Shang",
      "Hongying Liu",
      "Jin Liu",
      "Weixin An",
      "Yuanyuan Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19271v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19271v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19254v1",
    "title": "RegionRoute: Regional Style Transfer with Diffusion Model",
    "summary": "Precise spatial control in diffusion-based style transfer remains challenging. This challenge arises because diffusion models treat style as a global feature and lack explicit spatial grounding of style representations, making it difficult to restrict style application to specific objects or regions. To our knowledge, existing diffusion models are unable to perform true localized style transfer, typically relying on handcrafted masks or multi-stage post-processing that introduce boundary artifacts and limit generalization. To address this, we propose an attention-supervised diffusion framework that explicitly teaches the model where to apply a given style by aligning the attention scores of style tokens with object masks during training. Two complementary objectives, a Focus loss based on KL divergence and a Cover loss using binary cross-entropy, jointly encourage accurate localization and dense coverage. A modular LoRA-MoE design further enables efficient and scalable multi-style adaptation. To evaluate localized stylization, we introduce the Regional Style Editing Score, which measures Regional Style Matching through CLIP-based similarity within the target region and Identity Preservation via masked LPIPS and pixel-level consistency on unedited areas. Experiments show that our method achieves mask-free, single-object style transfer at inference, producing regionally accurate and visually coherent results that outperform existing diffusion-based editing approaches.",
    "authors": [
      "Bowen Chen",
      "Jake Zuena",
      "Alan C. Bovik",
      "Divya Kothandaraman"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19254v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19254v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19187v1",
    "title": "Adaptive Problem Generation via Symbolic Representations",
    "summary": "We present a method for generating training data for reinforcement learning with verifiable rewards to improve small open-weights language models on mathematical tasks. Existing data generation approaches rely on open-loop pipelines and fixed modifications that do not adapt to the model's capabilities. Furthermore, they typically operate directly on word problems, limiting control over problem structure. To address this, we perform modifications in a symbolic problem space, representing each problem as a set of symbolic variables and constraints (e.g., via algebraic frameworks such as SymPy or SMT formulations). This representation enables precise control over problem structure, automatic generation of ground-truth solutions, and decouples mathematical reasoning from linguistic realization. We also show that this results in more diverse generations. To adapt the problem difficulty to the model, we introduce a closed-loop framework that learns modification strategies through prompt optimization in symbolic space. Experimental results demonstrate that both adaptive problem generation and symbolic representation modifications contribute to improving the model's math solving ability.",
    "authors": [
      "Teresa Yeo",
      "Myeongho Jeon",
      "Dulaj Weerakoon",
      "Rui Qiao",
      "Alok Prakash",
      "Armando Solar-Lezama",
      "Archan Misra"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19187v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19187v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19178v1",
    "title": "EMAD: Evidence-Centric Grounded Multimodal Diagnosis for Alzheimer's Disease",
    "summary": "Deep learning models for medical image analysis often act as black boxes, seldom aligning with clinical guidelines or explicitly linking decisions to supporting evidence. This is especially critical in Alzheimer's disease (AD), where predictions should be grounded in both anatomical and clinical findings. We present EMAD, a vision-language framework that generates structured AD diagnostic reports in which each claim is explicitly grounded in multimodal evidence. EMAD uses a hierarchical Sentence-Evidence-Anatomy (SEA) grounding mechanism: (i) sentence-to-evidence grounding links generated sentences to clinical evidence phrases, and (ii) evidence-to-anatomy grounding localizes corresponding structures on 3D brain MRI. To reduce dense annotation requirements, we propose GTX-Distill, which transfers grounding behavior from a teacher trained with limited supervision to a student operating on model-generated reports. We further introduce Executable-Rule GRPO, a reinforcement fine-tuning scheme with verifiable rewards that enforces clinical consistency, protocol adherence, and reasoning-diagnosis coherence. On the AD-MultiSense dataset, EMAD achieves state-of-the-art diagnostic accuracy and produces more transparent, anatomically faithful reports than existing methods. We will release code and grounding annotations to support future research in trustworthy medical vision-language models.",
    "authors": [
      "Qiuhui Chen",
      "Xuancheng Yao",
      "Zhenglei Zhou",
      "Xinyue Hu",
      "Yi Hong"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19178v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19178v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19160v1",
    "title": "Reasoning Capabilities of Large Language Models. Lessons Learned from General Game Playing",
    "summary": "This paper examines the reasoning capabilities of Large Language Models (LLMs) from a novel perspective, focusing on their ability to operate within formally specified, rule-governed environments. We evaluate four LLMs (Gemini 2.5 Pro and Flash variants, Llama 3.3 70B and GPT-OSS 120B) on a suite of forward-simulation tasks-including next / multistep state formulation, and legal action generation-across a diverse set of reasoning problems illustrated through General Game Playing (GGP) game instances. Beyond reporting instance-level performance, we characterize games based on 40 structural features and analyze correlations between these features and LLM performance. Furthermore, we investigate the effects of various game obfuscations to assess the role of linguistic semantics in game definitions and the impact of potential prior exposure of LLMs to specific games during training. The main results indicate that three of the evaluated models generally perform well across most experimental settings, with performance degradation observed as the evaluation horizon increases (i.e., with a higher number of game steps). Detailed case-based analysis of the LLM performance provides novel insights into common reasoning errors in the considered logic-based problem formulation, including hallucinated rules, redundant state facts, or syntactic errors. Overall, the paper reports clear progress in formal reasoning capabilities of contemporary models.",
    "authors": [
      "Maciej \u015awiechowski",
      "Adam \u017bychowski",
      "Jacek Ma\u0144dziuk"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LO"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19160v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19160v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19127v1",
    "title": "AgenticRAGTracer: A Hop-Aware Benchmark for Diagnosing Multi-Step Retrieval Reasoning in Agentic RAG",
    "summary": "With the rapid advancement of agent-based methods in recent years, Agentic RAG has undoubtedly become an important research direction. Multi-hop reasoning, which requires models to engage in deliberate thinking and multi-step interaction, serves as a critical testbed for assessing such capabilities. However, existing benchmarks typically provide only final questions and answers, while lacking the intermediate hop-level questions that gradually connect atomic questions to the final multi-hop query. This limitation prevents researchers from analyzing at which step an agent fails and restricts more fine-grained evaluation of model capabilities. Moreover, most current benchmarks are manually constructed, which is both time-consuming and labor-intensive, while also limiting scalability and generalization. To address these challenges, we introduce AgenticRAGTracer, the first Agentic RAG benchmark that is primarily constructed automatically by large language models and designed to support step-by-step validation. Our benchmark spans multiple domains, contains 1,305 data points, and has no overlap with existing mainstream benchmarks. Extensive experiments demonstrate that even the best large language models perform poorly on our dataset. For instance, GPT-5 attains merely 22.6\\% EM accuracy on the hardest portion of our dataset. Hop-aware diagnosis reveals that failures are primarily driven by distorted reasoning chains -- either collapsing prematurely or wandering into over-extension. This highlights a critical inability to allocate steps consistent with the task's logical structure, providing a diagnostic dimension missing in traditional evaluations. We believe our work will facilitate research in Agentic RAG and inspire further meaningful progress in this area. Our code and data are available at https://github.com/YqjMartin/AgenticRAGTracer.",
    "authors": [
      "Qijie You",
      "Wenkai Yu",
      "Wentao Zhang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19127v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19127v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19114v1",
    "title": "Kaiwu-PyTorch-Plugin: Bridging Deep Learning and Photonic Quantum Computing for Energy-Based Models and Active Sample Selection",
    "summary": "This paper introduces the Kaiwu-PyTorch-Plugin (KPP) to bridge Deep Learning and Photonic Quantum Computing across multiple dimensions. KPP integrates the Coherent Ising Machine into the PyTorch ecosystem, addressing classical inefficiencies in Energy-Based Models. The framework facilitates quantum integration in three key aspects: accelerating Boltzmann sampling, optimizing training data via Active Sampling, and constructing hybrid architectures like QBM-VAE and Q-Diffusion. Empirical results on single-cell and OpenWebText datasets demonstrate KPPs ability to achieve SOTA performance, validating a comprehensive quantum-classical paradigm.",
    "authors": [
      "Hongdong Zhu",
      "Qi Gao",
      "Yin Ma",
      "Shaobo Chen",
      "Haixu Liu",
      "Fengao Wang",
      "Tinglan Wang",
      "Chang Wu",
      "Kai Wen"
    ],
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19114v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19114v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19094v1",
    "title": "RKHS Representation of Algebraic Convolutional Filters with Integral Operators",
    "summary": "Integral operators play a central role in signal processing, underpinning classical convolution, and filtering on continuous network models such as graphons. While these operators are traditionally analyzed through spectral decompositions, their connection to reproducing kernel Hilbert spaces (RKHS) has not been systematically explored within the algebraic signal processing framework. In this paper, we develop a comprehensive theory showing that the range of integral operators naturally induces RKHS convolutional signal models whose reproducing kernels are determined by a box product of the operator symbols. We characterize the algebraic and spectral properties of these induced RKHS and show that polynomial filtering with integral operators corresponds to iterated box products, giving rise to a unital kernel algebra. This perspective yields pointwise RKHS representations of filters via the reproducing property, providing an alternative to operator-based implementations. Our results establish precise connections between eigendecompositions and RKHS representations in graphon signal processing, extend naturally to directed graphons, and enable novel spatial--spectral localization results. Furthermore, we show that when the spectral domain is a subset of the original domain of the signals, optimal filters for regularized learning problems admit finite-dimensional RKHS representations, providing a principled foundation for learnable filters in integral-operator-based neural architectures.",
    "authors": [
      "Alejandro Parada-Mayorga",
      "Alejandro Ribeiro",
      "Juan Bazerque"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19094v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19094v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19091v1",
    "title": "CREM: Compression-Driven Representation Enhancement for Multimodal Retrieval and Comprehension",
    "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable success in comprehension tasks such as visual description and visual question answering. However, their direct application to embedding-based tasks like retrieval remains challenging due to the discrepancy between output formats and optimization objectives. Previous approaches often employ contrastive fine-tuning to adapt MLLMs for retrieval, but at the cost of losing their generative capabilities. We argue that both generative and embedding tasks fundamentally rely on shared cognitive mechanisms, specifically cross-modal representation alignment and contextual comprehension. To this end, we propose CREM (Compression-driven Representation Enhanced Model), with a unified framework that enhances multimodal representations for retrieval while preserving generative ability. Specifically, we introduce a compression-based prompt design with learnable chorus tokens to aggregate multimodal semantics and a compression-driven training strategy that integrates contrastive and generative objectives through compression-aware attention. Extensive experiments demonstrate that CREM achieves state-of-the-art retrieval performance on MMEB while maintaining strong generative performance on multiple comprehension benchmarks. Our findings highlight that generative supervision can further improve the representational quality of MLLMs under the proposed compression-driven paradigm.",
    "authors": [
      "Lihao Liu",
      "Yan Wang",
      "Biao Yang",
      "Da Li",
      "Jiangxia Cao",
      "Yuxiao Luo",
      "Xiang Chen",
      "Xiangyu Wu",
      "Wei Yuan",
      "Fan Yang",
      "Guiguang Ding",
      "Tingting Gao",
      "Guorui Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19091v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19091v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19025v1",
    "title": "Routing-Aware Explanations for Mixture of Experts Graph Models in Malware Detection",
    "summary": "Mixture-of-Experts (MoE) offers flexible graph reasoning by combining multiple views of a graph through a learned router. We investigate routing-aware explanations for MoE graph models in malware detection using control flow graphs (CFGs). Our architecture builds diversity at two levels. At the node level, each layer computes multiple neighborhood statistics and fuses them with an MLP, guided by a degree reweighting factor rho and a pooling choice lambda in {mean, std, max}, producing distinct node representations that capture complementary structural cues in CFGs. At the readout level, six experts, each tied to a specific (rho, lambda) view, output graph-level logits that the router weights into a final prediction. Post-hoc explanations are generated with edge-level attributions per expert and aggregated using the router gates so the rationale reflects both what each expert highlights and how strongly it is selected. Evaluated against single-expert GNN baselines such as GCN, GIN, and GAT on the same CFG dataset, the proposed MoE achieves strong detection accuracy while yielding stable, faithful attributions under sparsity-based perturbations. The results indicate that making the router explicit and combining multi-statistic node encoding with expert-level diversity can improve the transparency of MoE decisions for malware analysis.",
    "authors": [
      "Hossein Shokouhinejad",
      "Roozbeh Razavi-Far",
      "Griffin Higgins",
      "Ali. A Ghorbani"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19025v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19025v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.19289v1",
    "title": "AdsorbFlow: energy-conditioned flow matching enables fast and realistic adsorbate placement",
    "summary": "Identifying low-energy adsorption geometries on catalytic surfaces is a practical bottleneck for computational heterogeneous catalysis: the difficulty lies not only in the cost of density functional theory (DFT) but in proposing initial placements that relax into the correct energy basins. Conditional denoising diffusion has improved success rates, yet requires $\\sim$100 iterative steps per sample.   Here we introduce AdsorbFlow, a deterministic generative model that learns an energy-conditioned vector field on the rigid-body configuration space of adsorbate translation and rotation via conditional flow matching. Energy information enters through classifier-free guidance conditioning -- not energy-gradient guidance -- and sampling reduces to integrating an ODE in as few as 5 steps.   On OC20-Dense with full DFT single-point verification, AdsorbFlow with an EquiformerV2 backbone achieves 61.4% SR@10 and 34.1% SR@1 -- surpassing AdsorbDiff (31.8% SR@1, 41.0% SR@10) at every evaluation level and AdsorbML (47.7% SR@10) -- while using 20 times fewer generative steps and achieving the lowest anomaly rate among generative methods (6.8%). On 50 out-of-distribution systems, AdsorbFlow retains 58.0% SR@10 with a MLFF-to-DFT gap of only 4~percentage points. These results establish that deterministic transport is both faster and more accurate than stochastic denoising for adsorbate placement.",
    "authors": [
      "Jiangjie Qiu",
      "Wentao Li",
      "Honghao Chen",
      "Leyi Zhao",
      "Xiaonan Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19289v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19289v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19172v1",
    "title": "Online Realizable Regression and Applications for ReLU Networks",
    "summary": "Realizable online regression can behave very differently from online classification. Even without any margin or stochastic assumptions, realizability may enforce horizon-free (finite) cumulative loss under metric-like losses, even when the analogous classification problem has an infinite mistake bound. We study realizable online regression in the adversarial model under losses that satisfy an approximate triangle inequality (approximate pseudo-metrics). Recent work of Attias et al. shows that the minimax realizable cumulative loss is characterized by the scaled Littlestone/online dimension $\\mathbb{D}_{\\mathrm{onl}}$, but this quantity can be difficult to analyze.   Our main contribution is a generic potential method that upper bounds $\\mathbb{D}_{\\mathrm{onl}}$ by a concrete Dudley-type entropy integral that depends only on covering numbers of the hypothesis class under the induced sup pseudo-metric. We define an \\emph{entropy potential} $\u03a6(\\mathcal{H})=\\int_{0}^{diam(\\mathcal{H})} \\log N(\\mathcal{H},\\varepsilon)\\,d\\varepsilon$, where $N(\\mathcal{H},\\varepsilon)$ is the $\\varepsilon$-covering number of $\\mathcal{H}$, and show that for every $c$-approximate pseudo-metric loss, $\\mathbb{D}_{\\mathrm{onl}}(\\mathcal{H})\\le O(c)\\,\u03a6(\\mathcal{H})$. In particular, polynomial metric entropy implies $\u03a6(\\mathcal{H})<\\infty$ and hence a horizon-free realizable cumulative-loss bound with transparent dependence on effective dimension.   We illustrate the method on two families. We prove a sharp $q$-vs.-$d$ dichotomy for realizable online learning (finite and efficiently achievable $\u0398_{d,q}(L^d)$ total loss for $L$-Lipschitz regression iff $q>d$, otherwise infinite), and for bounded-norm $k$-ReLU networks separate regression (finite loss, even $\\widetilde O(k^2)$, and $O(1)$ for one ReLU) from classification (impossible already for $k=2,d=1$).",
    "authors": [
      "Ilan Doron-Arad",
      "Idan Mehalel",
      "Elchanan Mossel"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19172v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19172v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19158v1",
    "title": "DoAtlas-1: A Causal Compilation Paradigm for Clinical AI",
    "summary": "Medical foundation models generate narrative explanations but cannot quantify intervention effects, detect evidence conflicts, or validate literature claims, limiting clinical auditability. We propose causal compilation, a paradigm that transforms medical evidence from narrative text into executable code. The paradigm standardizes heterogeneous research evidence into structured estimand objects, each explicitly specifying intervention contrast, effect scale, time horizon, and target population, supporting six executable causal queries: do-calculus, counterfactual reasoning, temporal trajectories, heterogeneous effects, mechanistic decomposition, and joint interventions. We instantiate this paradigm in DoAtlas-1, compiling 1,445 effect kernels from 754 studies through effect standardization, conflict-aware graph construction, and real-world validation (Human Phenotype Project, 10,000 participants). The system achieves 98.5% canonicalization accuracy and 80.5% query executability. This paradigm shifts medical AI from text generation to executable, auditable, and verifiable causal reasoning.",
    "authors": [
      "Yulong Li",
      "Jianxu Chen",
      "Xiwei Liu",
      "Chuanyue Suo",
      "Rong Xia",
      "Zhixiang Lu",
      "Yichen Li",
      "Xinlin Zhuang",
      "Niranjana Arun Menon",
      "Yutong Xie",
      "Eran Segal",
      "Imran Razzak"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19158v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19158v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19156v1",
    "title": "Artefact-Aware Fungal Detection in Dermatophytosis: A Real-Time Transformer-Based Approach for KOH Microscopy",
    "summary": "Dermatophytosis is commonly assessed using potassium hydroxide (KOH) microscopy, yet accurate recognition of fungal hyphae is hindered by artefacts, heterogeneous keratin clearance, and notable inter-observer variability. This study presents a transformer-based detection framework using the RT-DETR model architecture to achieve precise, query-driven localization of fungal structures in high-resolution KOH images. A dataset of 2,540 routinely acquired microscopy images was manually annotated using a multi-class strategy to explicitly distinguish fungal elements from confounding artefacts. The model was trained with morphology-preserving augmentations to maintain the structural integrity of thin hyphae. Evaluation on an independent test set demonstrated robust object-level performance, with a recall of 0.9737, precision of 0.8043, and an AP@0.50 of 93.56%. When aggregated for image-level diagnosis, the model achieved 100% sensitivity and 98.8% accuracy, correctly identifying all positive cases without missing a single diagnosis. Qualitative outputs confirmed the robust localization of low-contrast hyphae even in artefact-rich fields. These results highlight that an artificial intelligence (AI) system can serve as a highly reliable, automated screening tool, effectively bridging the gap between image-level analysis and clinical decision-making in dermatomycology.",
    "authors": [
      "Rana Gursoy",
      "Abdurrahim Yilmaz",
      "Baris Kizilyaprak",
      "Esmahan Caglar",
      "Burak Temelkuran",
      "Huseyin Uvet",
      "Ayse Esra Koku Aksu",
      "Gulsum Gencoglan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19156v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19156v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19131v1",
    "title": "Test-Time Learning of Causal Structure from Interventional Data",
    "summary": "Supervised causal learning has shown promise in causal discovery, yet it often struggles with generalization across diverse interventional settings, particularly when intervention targets are unknown. To address this, we propose TICL (Test-time Interventional Causal Learning), a novel method that synergizes Test-Time Training with Joint Causal Inference. Specifically, we design a self-augmentation strategy to generate instance-specific training data at test time, effectively avoiding distribution shifts. Furthermore, by integrating joint causal inference, we developed a PC-inspired two-phase supervised learning scheme, which effectively leverages self-augmented training data while ensuring theoretical identifiability. Extensive experiments on bnlearn benchmarks demonstrate TICL's superiority in multiple aspects of causal discovery and intervention target detection.",
    "authors": [
      "Wei Chen",
      "Rui Ding",
      "Bojun Huang",
      "Yang Zhang",
      "Qiang Fu",
      "Yuxuan Liang",
      "Han Shi",
      "Dongmei Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19131v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19131v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19069v1",
    "title": "Asking the Right Questions: Improving Reasoning with Generated Stepping Stones",
    "summary": "Recent years have witnessed tremendous progress in enabling LLMs to solve complex reasoning tasks such as math and coding. As we start to apply LLMs to harder tasks that they may not be able to solve in one shot, it is worth paying attention to their ability to construct intermediate stepping stones that prepare them to better solve the tasks. Examples of stepping stones include simplifications, alternative framings, or subproblems. We study properties and benefits of stepping stones in the context of modern reasoning LLMs via ARQ (\\textbf{A}king the \\textbf{R}ight \\textbf{Q}uestions), our simple framework which introduces a question generator to the default reasoning pipeline. We first show that good stepping stone questions exist and are transferrable, meaning that good questions can be generated, and they substantially help LLMs of various capabilities in solving the target tasks. We next frame stepping stone generation as a post-training task and show that we can fine-tune LLMs to generate more useful stepping stones by SFT and RL on synthetic data.",
    "authors": [
      "Hengyuan Hu",
      "Tingchen Fu",
      "Minqi Jiang",
      "Alexander H Miller",
      "Yoram Bachrach",
      "Jakob Nicolaus Foerster"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19069v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19069v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19008v1",
    "title": "Capable but Unreliable: Canonical Path Deviation as a Causal Mechanism of Agent Failure in Long-Horizon Tasks",
    "summary": "Why do language agents fail on tasks they are capable of solving? We argue that many such failures are reliability failures caused by stochastic drift from a task's latent solution structure, not capability failures. Every well-defined tool-use task imposes a canonical solution path (i.e., a convergent set of tool invocations shared across successful runs) and agent success depends critically on whether a trajectory stays within this path's operating envelope. We establish this causally using a natural experiment that holds model capability and task difficulty fixed by construction. We analyze trajectories from the Toolathlon benchmark: 22 frontier models each attempt 108 real-world tool-use tasks across 3 independent runs, yielding 515 model$\\times$task units where the same model succeeds on some runs and fails on others due to LLM sampling stochasticity alone. Within these units, successful runs adhere significantly more closely to the canonical solution path than failed runs ($+$0.060 Jaccard, $p<0.0001$, $n=488$ units, 95% CI [+0.043, +0.077]). This result survives six robustness checks including cross-model-family leave-one-out validation. Critically, the causal mechanism is gradual and self-reinforcing: the adherence gap is statistically indistinguishable from zero through the first 50% of the trajectory, ruling out early-branching selection bias, and each off-canonical tool call raises the probability that the next call is also off-canonical by 22.7 percentage points ($\\hat\u03b2=+0.227$, $p<0.0001$), more than doubling the baseline rate. These findings imply that agent reliability cannot be improved by capability scaling alone, but offer a highly actionable intervention: a simple monitor that restarts the bottom tercile of runs based on mid-trajectory canonical adherence lifts success rates by $+$8.8 percentage points among intervened runs.",
    "authors": [
      "Wilson Y. Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19008v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19008v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19000v1",
    "title": "MagicAgent: Towards Generalized Agent Planning",
    "summary": "The evolution of Large Language Models (LLMs) from passive text processors to autonomous agents has established planning as a core component of modern intelligence. However, achieving generalized planning remains elusive, not only by the scarcity of high-quality interaction data but also by inherent conflicts across heterogeneous planning tasks. These challenges result in models that excel at isolated tasks yet struggle to generalize, while existing multi-task training attempts suffer from gradient interference. In this paper, we present \\textbf{MagicAgent}, a series of foundation models specifically designed for generalized agent planning. We introduce a lightweight and scalable synthetic data framework that generates high-quality trajectories across diverse planning tasks, including hierarchical task decomposition, tool-augmented planning, multi-constraint scheduling, procedural logic orchestration, and long-horizon tool execution. To mitigate training conflicts, we propose a two-stage training paradigm comprising supervised fine-tuning followed by multi-objective reinforcement learning over both static datasets and dynamic environments. Empirical results demonstrate that MagicAgent-32B and MagicAgent-30B-A3B deliver superior performance, achieving accuracies of $75.1\\%$ on Worfbench, $55.9\\%$ on NaturalPlan, $57.5\\%$ on $\u03c4^2$-Bench, $86.9\\%$ on BFCL-v3, and $81.2\\%$ on ACEBench, as well as strong results on our in-house MagicEval benchmarks. These results substantially outperform existing sub-100B models and even surpass leading closed-source models.",
    "authors": [
      "Xuhui Ren",
      "Shaokang Dong",
      "Chen Yang",
      "Qing Gao",
      "Yunbin Zhao",
      "Yongsheng Liu",
      "Xinwei Geng",
      "Xiang Li",
      "Demei Yan",
      "Yanqing Li",
      "Chenhao Huang",
      "Dingwei Zhu",
      "Junjie Ye",
      "Boxuan Yue",
      "Yingnan Fu",
      "Mengzhe Lv",
      "Zezeng Feng",
      "Boshen Zhou",
      "Bocheng Wang",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Tao Gui",
      "Qi Zhang",
      "Yunke Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19000v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19000v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.19332v1",
    "title": "Training-Free Cross-Architecture Merging for Graph Neural Networks",
    "summary": "Model merging has emerged as a powerful paradigm for combining the capabilities of distinct expert models without the high computational cost of retraining, yet current methods are fundamentally constrained to homogeneous architectures. For GNNs, however, message passing is topology-dependent and sensitive to misalignment, making direct parameter-space merging unreliable. To bridge this gap, we introduce H-GRAMA (Heterogeneous Graph Routing and Message Alignment), a training-free framework that lifts merging from parameter space to operator space. We formalize Universal Message Passing Mixture (UMPM), a shared operator family that expresses heterogeneous GNN layers in a common functional language. H-GRAMA enables cross-architecture GNN merging (e.g., GCN to GAT) without retraining, retaining high specialist accuracy in most cases in compatible depth settings and achieving inference speedups of 1.2x to 1.9x over ensembles.",
    "authors": [
      "Rishabh Bhattacharya",
      "Vikaskumar Kalsariya",
      "Naresh Manwani"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19332v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19332v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19304v1",
    "title": "Safe and Interpretable Multimodal Path Planning for Multi-Agent Cooperation",
    "summary": "Successful cooperation among decentralized agents requires each agent to quickly adapt its plan to the behavior of other agents. In scenarios where agents cannot confidently predict one another's intentions and plans, language communication can be crucial for ensuring safety. In this work, we focus on path-level cooperation in which agents must adapt their paths to one another in order to avoid collisions or perform physical collaboration such as joint carrying. In particular, we propose a safe and interpretable multimodal path planning method, CaPE (Code as Path Editor), which generates and updates path plans for an agent based on the environment and language communication from other agents. CaPE leverages a vision-language model (VLM) to synthesize a path editing program verified by a model-based planner, grounding communication to path plan updates in a safe and interpretable way. We evaluate our approach in diverse simulated and real-world scenarios, including multi-robot and human-robot cooperation in autonomous driving, household, and joint carrying tasks. Experimental results demonstrate that CaPE can be integrated into different robotic systems as a plug-and-play module, greatly enhancing a robot's ability to align its plan to language communication from other robots or humans. We also show that the combination of the VLM-based path editing program synthesis and model-based planning safety enables robots to achieve open-ended cooperation while maintaining safety and interpretability.",
    "authors": [
      "Haojun Shi",
      "Suyu Ye",
      "Katherine M. Guerrerio",
      "Jianzhi Shen",
      "Yifan Yin",
      "Daniel Khashabi",
      "Chien-Ming Huang",
      "Tianmin Shu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19304v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19304v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19261v1",
    "title": "DGPO: RL-Steered Graph Diffusion for Neural Architecture Generation",
    "summary": "Reinforcement learning fine-tuning has proven effective for steering generative diffusion models toward desired properties in image and molecular domains. Graph diffusion models have similarly been applied to combinatorial structure generation, including neural architecture search (NAS). However, neural architectures are directed acyclic graphs (DAGs) where edge direction encodes functional semantics such as data flow-information that existing graph diffusion methods, designed for undirected structures, discard. We propose Directed Graph Policy Optimization (DGPO), which extends reinforcement learning fine-tuning of discrete graph diffusion models to DAGs via topological node ordering and positional encoding. Validated on NAS-Bench-101 and NAS-Bench-201, DGPO matches the benchmark optimum on all three NAS-Bench-201 tasks (91.61%, 73.49%, 46.77%). The central finding is that the model learns transferable structural priors: pretrained on only 7% of the search space, it generates near-oracle architectures after fine-tuning, within 0.32 percentage points of the full-data model and extrapolating 7.3 percentage points beyond its training ceiling. Bidirectional control experiments confirm genuine reward-driven steering, with inverse optimization reaching near random-chance accuracy (9.5%). These results demonstrate that reinforcement learning-steered discrete diffusion, once extended to handle directionality, provides a controllable generative framework for directed combinatorial structures.",
    "authors": [
      "Aleksei Liuliakov",
      "Luca Hermes",
      "Barbara Hammer"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19261v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19261v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19253v1",
    "title": "Alternating Bi-Objective Optimization for Explainable Neuro-Fuzzy Systems",
    "summary": "Fuzzy systems show strong potential in explainable AI due to their rule-based architecture and linguistic variables. Existing approaches navigate the accuracy-explainability trade-off either through evolutionary multi-objective optimization (MOO), which is computationally expensive, or gradient-based scalarization, which cannot recover non-convex Pareto regions. We propose X-ANFIS, an alternating bi-objective gradient-based optimization scheme for explainable adaptive neuro-fuzzy inference systems. Cauchy membership functions are used for stable training under semantically controlled initializations, and a differentiable explainability objective is introduced and decoupled from the performance objective through alternating gradient passes. Validated in approximately 5,000 experiments on nine UCI regression datasets, X-ANFIS consistently achieves target distinguishability while maintaining competitive predictive accuracy, recovering solutions beyond the convex hull of the MOO Pareto front.",
    "authors": [
      "Qusai Khaled",
      "Uzay Kaymak",
      "Laura Genga"
    ],
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19253v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19253v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19196v1",
    "title": "An Interpretable Data-Driven Model of the Flight Dynamics of Hawks",
    "summary": "Despite significant analysis of bird flight, generative physics models for flight dynamics do not currently exist. Yet the underlying mechanisms responsible for various flight manoeuvres are important for understanding how agile flight can be accomplished. Even in a simple flight, multiple objectives are at play, complicating analysis of the overall flight mechanism. Using the data-driven method of dynamic mode decomposition (DMD) on motion capture recordings of hawks, we show that multiple behavioral states such as flapping, turning, landing, and gliding, can be modeled by simple and interpretable modal structures (i.e. the underlying wing-tail shape) which can be linearly combined to reproduce the experimental flight observations. Moreover, the DMD model can be used to extrapolate naturalistic flapping. Flight is highly individual, with differences in style across the hawks, but we find they share a common set of dynamic modes. The DMD model is a direct fit to data, unlike traditional models constructed from physics principles which can rarely be tested on real data and whose assumptions are typically invalid in real flight. The DMD approach gives a highly accurate reconstruction of the flight dynamics with only three parameters needed to characterize flapping, and a fourth to integrate turning manoeuvres. The DMD analysis further shows that the underlying mechanism of flight, much like simplest walking models, displays a parametric coupling between dominant modes suggesting efficiency for locomotion.",
    "authors": [
      "Lydia France",
      "Karl Lapo",
      "J. Nathan Kutz"
    ],
    "categories": [
      "q-bio.QM",
      "cs.CE",
      "cs.LG",
      "physics.flu-dyn"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19196v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19196v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19142v1",
    "title": "Celo2: Towards Learned Optimization Free Lunch",
    "summary": "Learned optimizers are powerful alternatives to hand-designed update rules like Adam, yet they have seen limited practical adoption since they often fail to meta-generalize beyond their training distribution and incur high meta-training cost. For instance, prior work, VeLO, scaled meta-training to 4,000 TPU months ($\\sim$10$\\times$ GPT-3 compute) to meta-train a general-purpose optimizer but it failed to generalize beyond 600M parameters tasks. In this work, we present a surprising finding: by crafting a simple normalized optimizer architecture and augmenting meta-training, it becomes feasible to meta-train a performant general-purpose learned update rule on a tiny fraction of VeLO compute, 4.5 GPU hours to be precise. Our learned update rule scales stably to a billion-scale pretraining task (GPT-3 XL 1.3B) which is six orders of magnitude larger than its meta-training distribution. Furthermore, it shows strong performance across diverse out-of-distribution tasks and is compatible with modern optimization harness that includes orthogonalization, distinct update rules for input-output and hidden weights, and decoupled weight decay. In all, this work paves the way for practically applicable learnable optimization algorithms, unlocking exploration of richer meta-training and data curation recipes to further improve performance.",
    "authors": [
      "Abhinav Moudgil",
      "Boris Knyazev",
      "Eugene Belilovsky"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19142v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19142v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19068v1",
    "title": "TimeRadar: A Domain-Rotatable Foundation Model for Time Series Anomaly Detection",
    "summary": "Current time series foundation models (TSFMs) primarily focus on learning prevalent and regular patterns within a predefined time or frequency domain to enable supervised downstream tasks (e.g., forecasting). Consequently, they are often ineffective for inherently unsupervised downstream tasks-such as time series anomaly detection (TSAD), which aims to identify rare, irregular patterns. This limitation arises because such abnormal patterns can closely resemble the regular patterns when presented in the same time/frequency domain. To address this issue, we introduce TimeRadar, an innovative TSFM built in a fractional time-frequency domain to support generalist TSAD across diverse unseen datasets. Our key insight is that rotating a time series into a data-dependent fractional time-frequency representation can adaptively differentiate the normal and abnormal signals across different datasets. To this end, a novel component, namely Fractionally modulated Time-Frequency Reconstruction (FTFRecon), is proposed in TimeRadar to leverage a learnable fractional order to rotate the time series to the most pronounced angle between a continuous time and frequency domain for accurate data reconstruction. This provides adaptive data reconstruction in an optimal time-frequency domain for each data input, enabling effective differentiation of the unbounded abnormal patterns from the regular ones across datasets, including unseen datasets. To allow TimeRadar to model local abnormality that is not captured by the global data reconstruction, we further introduce a Contextual Deviation Learning (CDL) component to model the local deviation of the input relative to its contextual time series data in the rotatable domain.",
    "authors": [
      "Hui He",
      "Hezhe Qiao",
      "Yutong Chen",
      "Kun Yi",
      "Guansong Pang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19068v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19068v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19017v1",
    "title": "Why ReLU? A Bit-Model Dichotomy for Deep Network Training",
    "summary": "Theoretical analyses of Empirical Risk Minimization (ERM) are standardly framed within the Real-RAM model of computation. In this setting, training even simple neural networks is known to be $\\exists \\mathbb{R}$-complete -- a complexity class believed to be harder than NP, that characterizes the difficulty of solving systems of polynomial inequalities over the real numbers. However, this algebraic framework diverges from the reality of digital computation with finite-precision hardware. In this work, we analyze the theoretical complexity of ERM under a realistic bit-level model ($\\mathsf{ERM}_{\\text{bit}}$), where network parameters and inputs are constrained to be rational numbers with polynomially bounded bit-lengths. Under this model, we reveal a sharp dichotomy in tractability governed by the network's activation function. We prove that for deep networks with {\\em any} polynomial activations with rational coefficients and degree at least $2$, the bit-complexity of training is severe: deciding $\\mathsf{ERM}_{\\text{bit}}$ is $\\#P$-Hard, hence believed to be strictly harder than NP-complete problems. Furthermore, we show that determining the sign of a single partial derivative of the empirical loss function is intractable (unlikely in BPP), and deciding a specific bit in the gradient is $\\#P$-Hard. This provides a complexity-theoretic perspective for the phenomenon of exploding and vanishing gradients. In contrast, we show that for piecewise-linear activations such as ReLU, the precision requirements remain manageable: $\\mathsf{ERM}_{\\text{bit}}$ is contained within NP (specifically NP-complete), and standard backpropagation runs in polynomial time. Our results demonstrate that finite-precision constraints are not merely implementation details but fundamental determinants of learnability.",
    "authors": [
      "Ilan Doron-Arad",
      "Elchanan Mossel"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19017v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19017v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.19285v1",
    "title": "MRI Contrast Enhancement Kinetics World Model",
    "summary": "Clinical MRI contrast acquisition suffers from inefficient information yield, which presents as a mismatch between the risky and costly acquisition protocol and the fixed and sparse acquisition sequence. Applying world models to simulate the contrast enhancement kinetics in the human body enables continuous contrast-free dynamics. However, the low temporal resolution in MRI acquisition restricts the training of world models, leading to a sparsely sampled dataset. Directly training a generative model to capture the kinetics leads to two limitations: (a) Due to the absence of data on missing time, the model tends to overfit to irrelevant features, leading to content distortion. (b) Due to the lack of continuous temporal supervision, the model fails to learn the continuous kinetics law over time, causing temporal discontinuities. For the first time, we propose MRI Contrast Enhancement Kinetics World model (MRI CEKWorld) with SpatioTemporal Consistency Learning (STCL). For (a), guided by the spatial law that patient-level structures remain consistent during enhancement, we propose Latent Alignment Learning (LAL) that constructs a patient-specific template to constrain contents to align with this template. For (b), guided by the temporal law that the kinetics follow a consistent smooth trend, we propose Latent Difference Learning (LDL) which extends the unobserved intervals by interpolation and constrains smooth variations in the latent space among interpolated sequences. Extensive experiments on two datasets show our MRI CEKWorld achieves better realistic contents and kinetics. Codes will be available at https://github.com/DD0922/MRI-Contrast-Enhancement-Kinetics-World-Model.",
    "authors": [
      "Jindi Kong",
      "Yuting He",
      "Cong Xia",
      "Rongjun Ge",
      "Shuo Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19285v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19285v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19274v1",
    "title": "DD-CAM: Minimal Sufficient Explanations for Vision Models Using Delta Debugging",
    "summary": "We introduce a gradient-free framework for identifying minimal, sufficient, and decision-preserving explanations in vision models by isolating the smallest subset of representational units whose joint activation preserves predictions. Unlike existing approaches that aggregate all units, often leading to cluttered saliency maps, our approach, DD-CAM, identifies a 1-minimal subset whose joint activation suffices to preserve the prediction (i.e., removing any unit from the subset alters the prediction). To efficiently isolate minimal sufficient subsets, we adapt delta debugging, a systematic reduction strategy from software debugging, and configure its search strategy based on unit interactions in the classifier head: testing individual units for models with non-interacting units and testing unit combinations for models in which unit interactions exist. We then generate minimal, prediction-preserving saliency maps that highlight only the most essential features. Our experimental evaluation demonstrates that our approach can produce more faithful explanations and achieve higher localization accuracy than the state-of-the-art CAM-based approaches.",
    "authors": [
      "Krishna Khadka",
      "Yu Lei",
      "Raghu N. Kacker",
      "D. Richard Kuhn"
    ],
    "categories": [
      "cs.CV",
      "cs.SE"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19274v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19274v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19265v1",
    "title": "Spectral bias in physics-informed and operator learning: Analysis and mitigation guidelines",
    "summary": "Solving partial differential equations (PDEs) by neural networks as well as Kolmogorov-Arnold Networks (KANs), including physics-informed neural networks (PINNs), physics-informed KANs (PIKANs), and neural operators, are known to exhibit spectral bias, whereby low-frequency components of the solution are learned significantly faster than high-frequency modes. While spectral bias is often treated as an intrinsic representational limitation of neural architectures, its interaction with optimization dynamics and physics-based loss formulations remains poorly understood. In this work, we provide a systematic investigation of spectral bias in physics-informed and operator learning frameworks, with emphasis on the coupled roles of network architecture, activation functions, loss design, and optimization strategy. We quantify spectral bias through frequency-resolved error metrics, Barron-norm diagnostics, and higher-order statistical moments, enabling a unified analysis across elliptic, hyperbolic, and dispersive PDEs. Through diverse benchmark problems, including the Korteweg-de Vries, wave and steady-state diffusion-reaction equations, turbulent flow reconstruction, and earthquake dynamics, we demonstrate that spectral bias is not simply representational but fundamentally dynamical. In particular, second-order optimization methods substantially alter the spectral learning order, enabling earlier and more accurate recovery of high-frequency modes for all PDE types. For neural operators, we further show that spectral bias is dependent on the neural operator architecture and can also be effectively mitigated through spectral-aware loss formulations without increasing the inference cost.",
    "authors": [
      "Siavash Khodakarami",
      "Vivek Oommen",
      "Nazanin Ahmadi Daryakenari",
      "Maxim Beekenkamp",
      "George Em Karniadakis"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19265v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19265v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19198v1",
    "title": "Prompt Tuning for CLIP on the Pretrained Manifold",
    "summary": "Prompt tuning introduces learnable prompt vectors that adapt pretrained vision-language models to downstream tasks in a parameter-efficient manner. However, under limited supervision, prompt tuning alters pretrained representations and drives downstream features away from the pretrained manifold toward directions that are unfavorable for transfer. This drift degrades generalization. To address this limitation, we propose ManiPT, a framework that performs prompt tuning on the pretrained manifold. ManiPT introduces cosine consistency constraints in both the text and image modalities to confine the learned representations within the pretrained geometric neighborhood. Furthermore, we introduce a structural bias that enforces incremental corrections, guiding the adaptation along transferable directions to mitigate reliance on shortcut learning. From a theoretical perspective, ManiPT alleviates overfitting tendencies under limited data. Our experiments cover four downstream settings: unseen-class generalization, few-shot classification, cross-dataset transfer, and domain generalization. Across these settings, ManiPT achieves higher average performance than baseline methods. Notably, ManiPT provides an explicit perspective on how prompt tuning overfits under limited supervision.",
    "authors": [
      "Xi Yang",
      "Yuanrong Xu",
      "Weigang Zhang",
      "Guangming Lu",
      "David Zhang",
      "Jie Wen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19198v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19198v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19174v1",
    "title": "TurkicNLP: An NLP Toolkit for Turkic Languages",
    "summary": "Natural language processing for the Turkic language family, spoken by over 200 million people across Eurasia, remains fragmented, with most languages lacking unified tooling and resources. We present TurkicNLP, an open-source Python library providing a single, consistent NLP pipeline for Turkic languages across four script families: Latin, Cyrillic, Perso-Arabic, and Old Turkic Runic. The library covers tokenization, morphological analysis, part-of-speech tagging, dependency parsing, named entity recognition, bidirectional script transliteration, cross-lingual sentence embeddings, and machine translation through one language-agnostic API. A modular multi-backend architecture integrates rule-based finite-state transducers and neural models transparently, with automatic script detection and routing between script variants. Outputs follow the CoNLL-U standard for full interoperability and extension. Code and documentation are hosted at https://github.com/turkic-nlp/turkicnlp .",
    "authors": [
      "Sherzod Hakimov"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19174v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19174v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19161v1",
    "title": "Flash-VAED: Plug-and-Play VAE Decoders for Efficient Video Generation",
    "summary": "Latent diffusion models have enabled high-quality video synthesis, yet their inference remains costly and time-consuming. As diffusion transformers become increasingly efficient, the latency bottleneck inevitably shifts to VAE decoders. To reduce their latency while maintaining quality, we propose a universal acceleration framework for VAE decoders that preserves full alignment with the original latent distribution. Specifically, we propose (1) an independence-aware channel pruning method to effectively mitigate severe channel redundancy, and (2) a stage-wise dominant operator optimization strategy to address the high inference cost of the widely used causal 3D convolutions in VAE decoders. Based on these innovations, we construct a Flash-VAED family. Moreover, we design a three-phase dynamic distillation framework that efficiently transfers the capabilities of the original VAE decoder to Flash-VAED. Extensive experiments on Wan and LTX-Video VAE decoders demonstrate that our method outperforms baselines in both quality and speed, achieving approximately a 6$\\times$ speedup while maintaining the reconstruction performance up to 96.9%. Notably, Flash-VAED accelerates the end-to-end generation pipeline by up to 36% with negligible quality drops on VBench-2.0.",
    "authors": [
      "Lunjie Zhu",
      "Yushi Huang",
      "Xingtong Ge",
      "Yufei Xue",
      "Zhening Liu",
      "Yumeng Zhang",
      "Zehong Lin",
      "Jun Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19161v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19161v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19130v1",
    "title": "Detecting labeling bias using influence functions",
    "summary": "Labeling bias arises during data collection due to resource limitations or unconscious bias, leading to unequal label error rates across subgroups or misrepresentation of subgroup prevalence. Most fairness constraints assume training labels reflect the true distribution, rendering them ineffective when labeling bias is present; leaving a challenging question, that \\textit{how can we detect such labeling bias?} In this work, we investigate whether influence functions can be used to detect labeling bias. Influence functions estimate how much each training sample affects a model's predictions by leveraging the gradient and Hessian of the loss function -- when labeling errors occur, influence functions can identify wrongly labeled samples in the training set, revealing the underlying failure mode. We develop a sample valuation pipeline and test it first on the MNIST dataset, then scaled to the more complex CheXpert medical imaging dataset. To examine label noise, we introduced controlled errors by flipping 20\\% of the labels for one class in the dataset. Using a diagonal Hessian approximation, we demonstrated promising results, successfully detecting nearly 90\\% of mislabeled samples in MNIST. On CheXpert, mislabeled samples consistently exhibit higher influence scores. These results highlight the potential of influence functions for identifying label errors.",
    "authors": [
      "Frida J\u00f8rgensen",
      "Nina Weng",
      "Siavash Bigdeli"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19130v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19130v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19126v1",
    "title": "Robust Predictive Uncertainty and Double Descent in Contaminated Bayesian Random Features",
    "summary": "We propose a robust Bayesian formulation of random feature (RF) regression that accounts explicitly for prior and likelihood misspecification via Huber-style contamination sets. Starting from the classical equivalence between ridge-regularized RF training and Bayesian inference with Gaussian priors and likelihoods, we replace the single prior and likelihood with $\u03b5$- and $\u03b7$-contaminated credal sets, respectively, and perform inference using pessimistic generalized Bayesian updating. We derive explicit and tractable bounds for the resulting lower and upper posterior predictive densities. These bounds show that, when contamination is moderate, prior and likelihood ambiguity effectively acts as a direct contamination of the posterior predictive distribution, yielding uncertainty envelopes around the classical Gaussian predictive. We introduce an Imprecise Highest Density Region (IHDR) for robust predictive uncertainty quantification and show that it admits an efficient outer approximation via an adjusted Gaussian credible interval. We further obtain predictive variance bounds (under a mild truncation approximation for the upper bound) and prove that they preserve the leading-order proportional-growth asymptotics known for RF models. Together, these results establish a robustness theory for Bayesian random features: predictive uncertainty remains computationally tractable, inherits the classical double-descent phase structure, and is improved by explicit worst-case guarantees under bounded prior and likelihood misspecification.",
    "authors": [
      "Michele Caprio",
      "Katerina Papagiannouli",
      "Siu Lun Chau",
      "Sayan Mukherjee"
    ],
    "categories": [
      "cs.LG",
      "math.PR",
      "math.ST"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19126v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19126v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19086v1",
    "title": "Restoration-Guided Kuzushiji Character Recognition Framework under Seal Interference",
    "summary": "Kuzushiji was one of the most popular writing styles in pre-modern Japan and was widely used in both personal letters and official documents. However, due to its highly cursive forms and extensive glyph variations, most modern Japanese readers cannot directly interpret Kuzushiji characters. Therefore, recent research has focused on developing automated Kuzushiji character recognition methods, which have achieved satisfactory performance on relatively clean Kuzushiji document images. However, existing methods struggle to maintain recognition accuracy under seal interference (e.g., when seals overlap characters), despite the frequent occurrence of seals in pre-modern Japanese documents. To address this challenge, we propose a three-stage restoration-guided Kuzushiji character recognition (RG-KCR) framework specifically designed to mitigate seal interference. We construct datasets for evaluating Kuzushiji character detection (Stage 1) and classification (Stage 3). Experimental results show that the YOLOv12-medium model achieves a precision of 98.0% and a recall of 93.3% on the constructed test set. We quantitatively evaluate the restoration performance of Stage 2 using PSNR and SSIM. In addition, we conduct an ablation study to demonstrate that Stage 2 improves the Top-1 accuracy of Metom, a Vision Transformer (ViT)-based Kuzushiji classifier employed in Stage 3, from 93.45% to 95.33%. The implementation code of this work is available at https://ruiyangju.github.io/RG-KCR.",
    "authors": [
      "Rui-Yang Ju",
      "Kohei Yamashita",
      "Hirotaka Kameko",
      "Shinsuke Mori"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19086v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19086v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19065v1",
    "title": "Agentic Problem Frames: A Systematic Approach to Engineering Reliable Domain Agents",
    "summary": "Large Language Models (LLMs) are evolving into autonomous agents, yet current \"frameless\" development--relying on ambiguous natural language without engineering blueprints--leads to critical risks such as scope creep and open-loop failures. To ensure industrial-grade reliability, this study proposes Agentic Problem Frames (APF), a systematic engineering framework that shifts focus from internal model intelligence to the structured interaction between the agent and its environment.   The APF establishes a dynamic specification paradigm where intent is concretized at runtime through domain knowledge injection. At its core, the Act-Verify-Refine (AVR) loop functions as a closed-loop control system that transforms execution results into verified knowledge assets, driving system behavior toward asymptotic convergence to mission requirements (R). To operationalize this, this study introduces the Agentic Job Description (AJD), a formal specification tool that defines jurisdictional boundaries, operational contexts, and epistemic evaluation criteria.   The efficacy of this framework is validated through two contrasting case studies: a delegated proxy model for business travel and an autonomous supervisor model for industrial equipment management. By applying AJD-based specification and APF modeling to these scenarios, the analysis demonstrates how operational scenarios are systematically controlled within defined boundaries. These cases provide a conceptual proof that agent reliability stems not from a model's internal reasoning alone, but from the rigorous engineering structures that anchor stochastic AI within deterministic business processes, thereby enabling the development of verifiable and dependable domain agents.",
    "authors": [
      "Chanjin Park"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19065v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19065v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.19329v1",
    "title": "Dynamic Elasticity Between Forest Loss and Carbon Emissions: A Subnational Panel Analysis of the United States",
    "summary": "Accurate quantification of the relationship between forest loss and associated carbon emissions is critical for both environmental monitoring and policy evaluation. Although many studies have documented spatial patterns of forest degradation, there is limited understanding of the dynamic elasticity linking tree cover loss to carbon emissions at subnational scales. In this paper, we construct a comprehensive panel dataset of annual forest loss and carbon emission estimates for U.S. subnational administrative units from 2001 to 2023, based on the Hansen Global Forest Change dataset. We apply fixed effects and dynamic panel regression techniques to isolate within-region variation and account for temporal persistence in emissions. Our results show that forest loss has a significant positive short-run elasticity with carbon emissions, and that emissions exhibit strong persistence over time. Importantly, the estimated long-run elasticity, accounting for autoregressive dynamics, is substantially larger than the short-run effect, indicating cumulative impacts of repeated forest loss events. These findings highlight the importance of modeling temporal dynamics when assessing environmental responses to land cover change. The dynamic elasticity framework proposed here offers a robust and interpretable tool for analyzing environmental change processes, and can inform both regional monitoring systems and carbon accounting frameworks.",
    "authors": [
      "Keonvin Park"
    ],
    "categories": [
      "stat.AP",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19329v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19329v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19215v1",
    "title": "Understanding Empirical Unlearning with Combinatorial Interpretability",
    "summary": "While many recent methods aim to unlearn or remove knowledge from pretrained models, seemingly erased knowledge often persists and can be recovered in various ways. Because large foundation models are far from interpretable, understanding whether and how such knowledge persists remains a significant challenge. To address this, we turn to the recently developed framework of combinatorial interpretability. This framework, designed for two-layer neural networks, enables direct inspection of the knowledge encoded in the model weights. We reproduce baseline unlearning methods within the combinatorial interpretability setting and examine their behavior along two dimensions: (i) whether they truly remove knowledge of a target concept (the concept we wish to remove) or merely inhibit its expression while retaining the underlying information, and (ii) how easily the supposedly erased knowledge can be recovered through various fine-tuning operations. Our results shed light within a fully interpretable setting on how knowledge can persist despite unlearning and when it might resurface.",
    "authors": [
      "Shingo Kodama",
      "Niv Cohen",
      "Micah Adler",
      "Nir Shavit"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19215v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19215v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19202v1",
    "title": "UniE2F: A Unified Diffusion Framework for Event-to-Frame Reconstruction with Video Foundation Models",
    "summary": "Event cameras excel at high-speed, low-power, and high-dynamic-range scene perception. However, as they fundamentally record only relative intensity changes rather than absolute intensity, the resulting data streams suffer from a significant loss of spatial information and static texture details. In this paper, we address this limitation by leveraging the generative prior of a pre-trained video diffusion model to reconstruct high-fidelity video frames from sparse event data. Specifically, we first establish a baseline model by directly applying event data as a condition to synthesize videos. Then, based on the physical correlation between the event stream and video frames, we further introduce the event-based inter-frame residual guidance to enhance the accuracy of video frame reconstruction. Furthermore, we extend our method to video frame interpolation and prediction in a zero-shot manner by modulating the reverse diffusion sampling process, thereby creating a unified event-to-frame reconstruction framework. Experimental results on real-world and synthetic datasets demonstrate that our method significantly outperforms previous approaches both quantitatively and qualitatively. We also refer the reviewers to the video demo contained in the supplementary material for video results. The code will be publicly available at https://github.com/CS-GangXu/UniE2F.",
    "authors": [
      "Gang Xu",
      "Zhiyu Zhu",
      "Junhui Hou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19202v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19202v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19188v1",
    "title": "PositionOCR: Augmenting Positional Awareness in Multi-Modal Models via Hybrid Specialist Integration",
    "summary": "In recent years, Multi-modal Large Language Models (MLLMs) have achieved strong performance in OCR-centric Visual Question Answering (VQA) tasks, illustrating their capability to process heterogeneous data and exhibit adaptability across varied contexts. However, these MLLMs rely on a Large Language Model (LLM) as the decoder, which is primarily designed for linguistic processing, and thus inherently lacks the positional reasoning required for precise visual tasks, such as text spotting and text grounding. Additionally, the extensive parameters of MLLMs necessitate substantial computational resources and large-scale data for effective training. Conversely, text spotting specialists achieve state-of-the-art coordinate predictions but lack semantic reasoning capabilities. This dichotomy motivates our key research question: Can we synergize the efficiency of specialists with the contextual power of LLMs to create a positionally-accurate MLLM? To overcome these challenges, we introduce PositionOCR, a parameter-efficient hybrid architecture that seamlessly integrates a text spotting model's positional strengths with an LLM's contextual reasoning. Comprising 131M trainable parameters, this framework demonstrates outstanding multi-modal processing capabilities, particularly excelling in tasks such as text grounding and text spotting, consistently surpassing traditional MLLMs.",
    "authors": [
      "Chen Duan",
      "Zhentao Guo",
      "Pei Fu",
      "Zining Wang",
      "Kai Zhou",
      "Pengfei Yan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19188v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19188v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19180v1",
    "title": "VLM-Guided Group Preference Alignment for Diffusion-based Human Mesh Recovery",
    "summary": "Human mesh recovery (HMR) from a single RGB image is inherently ambiguous, as multiple 3D poses can correspond to the same 2D observation. Recent diffusion-based methods tackle this by generating various hypotheses, but often sacrifice accuracy. They yield predictions that are either physically implausible or drift from the input image, especially under occlusion or in cluttered, in-the-wild scenes. To address this, we introduce a dual-memory augmented HMR critique agent with self-reflection to produce context-aware quality scores for predicted meshes. These scores distill fine-grained cues about 3D human motion structure, physical feasibility, and alignment with the input image. We use these scores to build a group-wise HMR preference dataset. Leveraging this dataset, we propose a group preference alignment framework for finetuning diffusion-based HMR models. This process injects the rich preference signals into the model, guiding it to generate more physically plausible and image-consistent human meshes. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art approaches.",
    "authors": [
      "Wenhao Shen",
      "Hao Wang",
      "Wanqi Yin",
      "Fayao Liu",
      "Xulei Yang",
      "Chao Liang",
      "Zhongang Cai",
      "Guosheng Lin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19180v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19180v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19143v1",
    "title": "Incremental Learning of Sparse Attention Patterns in Transformers",
    "summary": "This paper introduces a high-order Markov chain task to investigate how transformers learn to integrate information from multiple past positions with varying statistical significance. We demonstrate that transformers learn this task incrementally: each stage is defined by the acquisition of specific information through sparse attention patterns. Notably, we identify a shift in learning dynamics from competitive, where heads converge on the most statistically dominant pattern, to cooperative, where heads specialize in distinct patterns. We model these dynamics using simplified differential equations that characterize the trajectory and prove stage-wise convergence results. Our analysis reveals that transformers ascend a complexity ladder by passing through simpler, misspecified hypothesis classes before reaching the full model class. We further show that early stopping acts as an implicit regularizer, biasing the model toward these simpler classes. These results provide a theoretical foundation for the emergence of staged learning and complex behaviors in transformers, offering insights into generalization for natural language processing and algorithmic reasoning.",
    "authors": [
      "O\u011fuz Kaan Y\u00fcksel",
      "Rodrigo Alvarez Lucendo",
      "Nicolas Flammarion"
    ],
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19143v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19143v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19035v1",
    "title": "OpenVO: Open-World Visual Odometry with Temporal Dynamics Awareness",
    "summary": "We introduce OpenVO, a novel framework for Open-world Visual Odometry (VO) with temporal awareness under limited input conditions. OpenVO effectively estimates real-world-scale ego-motion from monocular dashcam footage with varying observation rates and uncalibrated cameras, enabling robust trajectory dataset construction from rare driving events recorded in dashcam. Existing VO methods are trained on fixed observation frequency (e.g., 10Hz or 12Hz), completely overlooking temporal dynamics information. Many prior methods also require calibrated cameras with known intrinsic parameters. Consequently, their performance degrades when (1) deployed under unseen observation frequencies or (2) applied to uncalibrated cameras. These significantly limit their generalizability to many downstream tasks, such as extracting trajectories from dashcam footage. To address these challenges, OpenVO (1) explicitly encodes temporal dynamics information within a two-frame pose regression framework and (2) leverages 3D geometric priors derived from foundation models. We validate our method on three major autonomous-driving benchmarks - KITTI, nuScenes, and Argoverse 2 - achieving more than 20 performance improvement over state-of-the-art approaches. Under varying observation rate settings, our method is significantly more robust, achieving 46%-92% lower errors across all metrics. These results demonstrate the versatility of OpenVO for real-world 3D reconstruction and diverse downstream applications.",
    "authors": [
      "Phuc D. A. Nguyen",
      "Anh N. Nhu",
      "Ming C. Lin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19035v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19035v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19022v1",
    "title": "An interpretable framework using foundation models for fish sex identification",
    "summary": "Accurate sex identification in fish is vital for optimizing breeding and management strategies in aquaculture, particularly for species at the risk of extinction. However, most existing methods are invasive or stressful and may cause additional mortality, posing severe risks to threatened or endangered fish populations. To address these challenges, we propose FishProtoNet, a robust, non-invasive computer vision-based framework for sex identification of delta smelt (Hypomesus transpacificus), an endangered fish species native to California, across its full life cycle. Unlike the traditional deep learning methods, FishProtoNet provides interpretability through learned prototype representations while improving robustness by leveraging foundation models to reduce the influence of background noise. Specifically, the FishProtoNet framework consists of three key components: fish regions of interest (ROIs) extraction using visual foundation model, feature extraction from fish ROIs and fish sex identification based on an interpretable prototype network. FishProtoNet demonstrates strong performance in delta smelt sex identification during early spawning and post-spawning stages, achieving the accuracies of 74.40% and 81.16% and corresponding F1 scores of 74.27% and 79.43% respectively. In contrast, delta smelt sex identification at the subadult stage remains challenging for current computer vision methods, likely due to less pronounced morphological differences in immature fish. The source code of FishProtoNet is publicly available at: https://github.com/zhengmiao1/Fish_sex_identification",
    "authors": [
      "Zheng Miao",
      "Tien-Chieh Hung"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19022v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19022v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.19330v1",
    "title": "CTS-Bench: Benchmarking Graph Coarsening Trade-offs for GNNs in Clock Tree Synthesis",
    "summary": "Graph Neural Networks (GNNs) are increasingly explored for physical design analysis in Electronic Design Automation, particularly for modeling Clock Tree Synthesis behavior such as clock skew and buffering complexity. However, practical deployment remains limited due to the prohibitive memory and runtime cost of operating on raw gate-level netlists. Graph coarsening is commonly used to improve scalability, yet its impact on CTS-critical learning objectives is not well characterized. This paper introduces CTS-Bench, a benchmark suite for systematically evaluating the trade-offs between graph coarsening, prediction accuracy, and computational efficiency in GNN-based CTS analysis. CTS-Bench consists of 4,860 converged physical design solutions spanning five architectures and provides paired raw gate-level and clustered graph representations derived from post-placement designs. Using clock skew prediction as a representative CTS task, we demonstrate a clear accuracy-efficiency trade-off. While graph coarsening reduces GPU memory usage by up to 17.2x and accelerates training by up to 3x, it also removes structural information essential for modeling clock distribution, frequently resulting in negative $R^2$ scores under zero-shot evaluation. Our findings indicate that generic graph clustering techniques can fundamentally compromise CTS learning objectives, even when global physical metrics remain unchanged. CTS-Bench enables principled evaluation of CTS-aware graph coarsening strategies, supports benchmarking of GNN architectures and accelerators under realistic physical design constraints, and provides a foundation for developing learning-assisted CTS analysis and optimization techniques.",
    "authors": [
      "Barsat Khadka",
      "Kawsher Roxy",
      "Md Rubel Ahmed"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19330v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19330v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.19241v1",
    "title": "Scaling Laws for Precision in High-Dimensional Linear Regression",
    "summary": "Low-precision training is critical for optimizing the trade-off between model quality and training costs, necessitating the joint allocation of model size, dataset size, and numerical precision. While empirical scaling laws suggest that quantization impacts effective model and data capacities or acts as an additive error, the theoretical mechanisms governing these effects remain largely unexplored. In this work, we initiate a theoretical study of scaling laws for low-precision training within a high-dimensional sketched linear regression framework. By analyzing multiplicative (signal-dependent) and additive (signal-independent) quantization, we identify a critical dichotomy in their scaling behaviors. Our analysis reveals that while both schemes introduce an additive error and degrade the effective data size, they exhibit distinct effects on effective model size: multiplicative quantization maintains the full-precision model size, whereas additive quantization reduces the effective model size. Numerical experiments validate our theoretical findings. By rigorously characterizing the complex interplay among model scale, dataset size, and quantization error, our work provides a principled theoretical basis for optimizing training protocols under practical hardware constraints.",
    "authors": [
      "Dechen Zhang",
      "Xuan Tang",
      "Yingyu Liang",
      "Difan Zou"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19241v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19241v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.19123v1",
    "title": "StreetTree: A Large-Scale Global Benchmark for Fine-Grained Tree Species Classification",
    "summary": "The fine-grained classification of street trees is a crucial task for urban planning, streetscape management, and the assessment of urban ecosystem services. However, progress in this field has been significantly hindered by the lack of large-scale, geographically diverse, and publicly available benchmark datasets specifically designed for street trees. To address this critical gap, we introduce StreetTree, the world's first large-scale benchmark dataset dedicated to fine-grained street tree classification. The dataset contains over 12 million images covering more than 8,300 common street tree species, collected from urban streetscapes across 133 countries spanning five continents, and supplemented with expert-verified observational data. StreetTree poses substantial challenges for pretrained vision models under complex urban environments: high inter-species visual similarity, long-tailed natural distributions, significant intra-class variations caused by seasonal changes, and diverse imaging conditions such as lighting, occlusions from buildings, and varying camera angles. In addition, we provide a hierarchical taxonomy (order-family-genus-species) to support research in hierarchical classification and representation learning. Through extensive experiments with various visual models, we establish strong baselines and reveal the limitations of existing methods in handling such real-world complexities. We believe that StreetTree will serve as a key resource for the refined management and research of urban street trees, while also driving new advancements at the intersection of computer vision and urban science.",
    "authors": [
      "Jiapeng Li",
      "Yingjing Huang",
      "Fan Zhang",
      "Yu liu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19123v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19123v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.19053v1",
    "title": "TeFlow: Enabling Multi-frame Supervision for Self-Supervised Feed-forward Scene Flow Estimation",
    "summary": "Self-supervised feed-forward methods for scene flow estimation offer real-time efficiency, but their supervision from two-frame point correspondences is unreliable and often breaks down under occlusions. Multi-frame supervision has the potential to provide more stable guidance by incorporating motion cues from past frames, yet naive extensions of two-frame objectives are ineffective because point correspondences vary abruptly across frames, producing inconsistent signals. In the paper, we present TeFlow, enabling multi-frame supervision for feed-forward models by mining temporally consistent supervision. TeFlow introduces a temporal ensembling strategy that forms reliable supervisory signals by aggregating the most temporally consistent motion cues from a candidate pool built across multiple frames. Extensive evaluations demonstrate that TeFlow establishes a new state-of-the-art for self-supervised feed-forward methods, achieving performance gains of up to 33\\% on the challenging Argoverse 2 and nuScenes datasets. Our method performs on par with leading optimization-based methods, yet speeds up 150 times. The code is open-sourced at https://github.com/KTH-RPL/OpenSceneFlow along with trained model weights.",
    "authors": [
      "Qingwen Zhang",
      "Chenhan Jiang",
      "Xiaomeng Zhu",
      "Yunqi Miao",
      "Yushan Zhang",
      "Olov Andersson",
      "Patric Jensfelt"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19053v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19053v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.19033v1",
    "title": "A Markovian View of Iterative-Feedback Loops in Image Generative Models: Neural Resonance and Model Collapse",
    "summary": "AI training datasets will inevitably contain AI-generated examples, leading to ``feedback'' in which the output of one model impacts the training of another. It is known that such iterative feedback can lead to model collapse, yet the mechanisms underlying this degeneration remain poorly understood. Here we show that a broad class of feedback processes converges to a low-dimensional invariant structure in latent space, a phenomenon we call neural resonance. By modeling iterative feedback as a Markov Chain, we show that two conditions are needed for this resonance to occur: ergodicity of the feedback process and directional contraction of the latent representation. By studying diffusion models on MNIST and ImageNet, as well as CycleGAN and an audio feedback experiment, we map how local and global manifold geometry evolve, and we introduce an eight-pattern taxonomy of collapse behaviors. Neural resonance provides a unified explanation for long-term degenerate behavior in generative models and provides practical diagnostics for identifying, characterizing, and eventually mitigating collapse.",
    "authors": [
      "Vibhas Kumar Vats",
      "David J. Crandall",
      "Samuel Goree"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19033v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19033v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.19001v1",
    "title": "A Benchmark and Knowledge-Grounded Framework for Advanced Multimodal Personalization Study",
    "summary": "The powerful reasoning of modern Vision Language Models open a new frontier for advanced personalization study. However, progress in this area is critically hampered by the lack of suitable benchmarks. To address this gap, we introduce Life-Bench, a comprehensive, synthetically generated multimodal benchmark built on simulated user digital footprints. Life-Bench features over questions evaluating a wide spectrum of capabilities, from persona understanding to complex reasoning over historical data. These capabilities expand far beyond prior benchmarks, reflecting the critical demands essential for real-world applications. Furthermore, we propose LifeGraph, an end-to-end framework that organizes personal context into a knowledge graph to facilitate structured retrieval and reasoning. Our experiments on Life-Bench reveal that existing methods falter significantly on complex personalized tasks, exposing a large performance headroom, especially in relational, temporal and aggregative reasoning. While LifeGraph closes this gap by leveraging structured knowledge and demonstrates a promising direction, these advanced personalization tasks remain a critical open challenge, motivating new research in this area.",
    "authors": [
      "Xia Hu",
      "Honglei Zhuang",
      "Brian Potetz",
      "Alireza Fathi",
      "Bo Hu",
      "Babak Samari",
      "Howard Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19001v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19001v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.18998v1",
    "title": "Benchmark Test-Time Scaling of General LLM Agents",
    "summary": "LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench.",
    "authors": [
      "Xiaochuan Li",
      "Ryan Ming",
      "Pranav Setlur",
      "Abhijay Paladugu",
      "Andy Tang",
      "Hao Kang",
      "Shuai Shao",
      "Rong Jin",
      "Chenyan Xiong"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18998v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18998v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.18997v1",
    "title": "Implicit Bias and Convergence of Matrix Stochastic Mirror Descent",
    "summary": "We investigate Stochastic Mirror Descent (SMD) with matrix parameters and vector-valued predictions, a framework relevant to multi-class classification and matrix completion problems. Focusing on the overparameterized regime, where the total number of parameters exceeds the number of training samples, we prove that SMD with matrix mirror functions $\u03c8(\\cdot)$ converges exponentially to a global interpolator. Furthermore, we generalize classical implicit bias results of vector SMD by demonstrating that the matrix SMD algorithm converges to the unique solution minimizing the Bregman divergence induced by $\u03c8(\\cdot)$ from initialization subject to interpolating the data. These findings reveal how matrix mirror maps dictate inductive bias in high-dimensional, multi-output problems.",
    "authors": [
      "Danil Akhtiamov",
      "Reza Ghane",
      "Babak Hassibi"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.OC"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18997v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18997v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.19345v1",
    "title": "Smooth Gate Functions for Soft Advantage Policy Optimization",
    "summary": "Group Relative Policy Optimization (GRPO) has significantly advanced the training of large language models and enhanced their reasoning capabilities, while it remains susceptible to instability due to the use of hard clipping. Soft Adaptive Policy Optimization (SAPO) addresses this limitation by replacing clipping with a smooth sigmoid-based gate function, which leads to more stable updates. We have decided to push this theory further and investigate the impact of different gate functions on both training stability and final model performance. We formalize the key properties that admissible gates should satisfy and identify several families of such functions for empirical evaluation. This paper presents an analysis of our findings based on experiments conducted with the Qwen2.5-7B-Instruct model on mathematical reasoning tasks. These results provide practical guidance for designing smoother and more robust policy optimization objectives for large language model training.",
    "authors": [
      "Egor Denisov",
      "Svetlana Glazyrina",
      "Maksim Kryzhanovskiy",
      "Roman Ischenko"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19345v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19345v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.19217v1",
    "title": "Questions beyond Pixels: Integrating Commonsense Knowledge in Visual Question Generation for Remote Sensing",
    "summary": "With the rapid development of remote sensing image archives, asking questions about images has become an effective way of gathering specific information or performing semantic image retrieval. However, current automatically generated questions tend to be simplistic and template-based, which hinders the deployment of question answering or visual dialogue systems for real-world applications. To enrich and diversify the questions with both image content and commonsense knowledge, we propose a Knowledge-aware Remote Sensing Visual Question Generation model (KRSVQG). The proposed model incorporates related knowledge triplets from external knowledge sources to broaden the question content, while employing image captioning as an intermediary representation to ground questions to the corresponding images. Moreover, KRSVQG utilizes a vision-language pre-training and fine-tuning strategy, enabling the model's adaptation to low data regimes. To evaluate the proposed KRSVQG model, we construct two knowledge-aware remote sensing visual question generation datasets: the NWPU-300 dataset and the TextRS-300 dataset. Evaluations, including metrics and human assessment, demonstrate that KRSVQG outperforms existing methods and leads to rich questions, grounded in both image and domain knowledge. As a key practice in vision-language research, knowledge-aware visual question generation advances the understanding of image content beyond pixels, facilitating the development of knowledge-enriched vision-language systems with vision-grounded human commonsense.",
    "authors": [
      "Siran Li",
      "Li Mi",
      "Javiera Castillo-Navarro",
      "Devis Tuia"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19217v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19217v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.19117v1",
    "title": "Keep it SymPL: Symbolic Projective Layout for Allocentric Spatial Reasoning in Vision-Language Models",
    "summary": "Perspective-aware spatial reasoning involves understanding spatial relationships from specific viewpoints-either egocentric (observer-centered) or allocentric (object-centered). While vision-language models (VLMs) perform well in egocentric settings, their performance deteriorates when reasoning from allocentric viewpoints, where spatial relations must be inferred from the perspective of objects within the scene. In this study, we address this underexplored challenge by introducing Symbolic Projective Layout (SymPL), a framework that reformulates allocentric reasoning into symbolic-layout forms that VLMs inherently handle well. By leveraging four key factors-projection, abstraction, bipartition, and localization-SymPL converts allocentric questions into structured symbolic-layout representations. Extensive experiments demonstrate that this reformulation substantially improves performance in both allocentric and egocentric tasks, enhances robustness under visual illusions and multi-view scenarios, and that each component contributes critically to these gains. These results show that SymPL provides an effective and principled approach for addressing complex perspective-aware spatial reasoning.",
    "authors": [
      "Jaeyun Jang",
      "Seunghui Shin",
      "Taeho Park",
      "Hyoseok Hwang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19117v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19117v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.19112v1",
    "title": "Universal 3D Shape Matching via Coarse-to-Fine Language Guidance",
    "summary": "Establishing dense correspondences between shapes is a crucial task in computer vision and graphics, while prior approaches depend on near-isometric assumptions and homogeneous subject types (i.e., only operate for human shapes). However, building semantic correspondences for cross-category objects remains challenging and has received relatively little attention. To achieve this, we propose UniMatch, a semantic-aware, coarse-to-fine framework for constructing dense semantic correspondences between strongly non-isometric shapes without restricting object categories. The key insight is to lift \"coarse\" semantic cues into \"fine\" correspondence, which is achieved through two stages. In the \"coarse\" stage, we perform class-agnostic 3D segmentation to obtain non-overlapping semantic parts and prompt multimodal large language models (MLLMs) to identify part names. Then, we employ pretrained vision language models (VLMs) to extract text embeddings, enabling the construction of matched semantic parts. In the \"fine\" stage, we leverage these coarse correspondences to guide the learning of dense correspondences through a dedicated rank-based contrastive scheme. Thanks to class-agnostic segmentation, language guiding, and rank-based contrastive learning, our method is versatile for universal object categories and requires no predefined part proposals, enabling universal matching for inter-class and non-isometric shapes. Extensive experiments demonstrate UniMatch consistently outperforms competing methods in various challenging scenarios.",
    "authors": [
      "Qinfeng Xiao",
      "Guofeng Mei",
      "Bo Yang",
      "Liying Zhang",
      "Jian Zhang",
      "Kit-lun Yick"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19112v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19112v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.19064v1",
    "title": "L3DR: 3D-aware LiDAR Diffusion and Rectification",
    "summary": "Range-view (RV) based LiDAR diffusion has recently made huge strides towards 2D photo-realism. However, it neglects 3D geometry realism and often generates various RV artifacts such as depth bleeding and wavy surfaces. We design L3DR, a 3D-aware LiDAR Diffusion and Rectification framework that can regress and cancel RV artifacts in 3D space and restore local geometry accurately. Our theoretical and empirical analysis reveals that 3D models are inherently superior to 2D models in generating sharp and authentic boundaries. Leveraging such analysis, we design a 3D residual regression network that rectifies RV artifacts and achieves superb geometry realism by predicting point-level offsets in 3D space. On top of that, we design a Welsch Loss that helps focus on local geometry and ignore anomalous regions effectively. Extensive experiments over multiple benchmarks including KITTI, KITTI360, nuScenes and Waymo show that the proposed L3DR achieves state-of-the-art generation and superior geometry-realism consistently. In addition, L3DR is generally applicable to different LiDAR diffusion models with little computational overhead.",
    "authors": [
      "Quan Liu",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19064v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19064v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.19004v1",
    "title": "MoBind: Motion Binding for Fine-Grained IMU-Video Pose Alignment",
    "summary": "We aim to learn a joint representation between inertial measurement unit (IMU) signals and 2D pose sequences extracted from video, enabling accurate cross-modal retrieval, temporal synchronization, subject and body-part localization, and action recognition. To this end, we introduce MoBind, a hierarchical contrastive learning framework designed to address three challenges: (1) filtering out irrelevant visual background, (2) modeling structured multi-sensor IMU configurations, and (3) achieving fine-grained, sub-second temporal alignment. To isolate motion-relevant cues, MoBind aligns IMU signals with skeletal motion sequences rather than raw pixels. We further decompose full-body motion into local body-part trajectories, pairing each with its corresponding IMU to enable semantically grounded multi-sensor alignment. To capture detailed temporal correspondence, MoBind employs a hierarchical contrastive strategy that first aligns token-level temporal segments, then fuses local (body-part) alignment with global (body-wide) motion aggregation. Evaluated on mRi, TotalCapture, and EgoHumans, MoBind consistently outperforms strong baselines across all four tasks, demonstrating robust fine-grained temporal alignment while preserving coarse semantic consistency across modalities. Code is available at https://github.com/bbvisual/ MoBind.",
    "authors": [
      "Duc Duy Nguyen",
      "Tat-Jun Chin",
      "Minh Hoai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19004v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19004v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.18993v1",
    "title": "SeaCache: Spectral-Evolution-Aware Cache for Accelerating Diffusion Models",
    "summary": "Diffusion models are a strong backbone for visual generation, but their inherently sequential denoising process leads to slow inference. Previous methods accelerate sampling by caching and reusing intermediate outputs based on feature distances between adjacent timesteps. However, existing caching strategies typically rely on raw feature differences that entangle content and noise. This design overlooks spectral evolution, where low-frequency structure appears early and high-frequency detail is refined later. We introduce Spectral-Evolution-Aware Cache (SeaCache), a training-free cache schedule that bases reuse decisions on a spectrally aligned representation. Through theoretical and empirical analysis, we derive a Spectral-Evolution-Aware (SEA) filter that preserves content-relevant components while suppressing noise. Employing SEA-filtered input features to estimate redundancy leads to dynamic schedules that adapt to content while respecting the spectral priors underlying the diffusion model. Extensive experiments on diverse visual generative models and the baselines show that SeaCache achieves state-of-the-art latency-quality trade-offs.",
    "authors": [
      "Jiwoo Chung",
      "Sangeek Hyun",
      "MinKyu Lee",
      "Byeongju Han",
      "Geonho Cha",
      "Dongyoon Wee",
      "Youngjun Hong",
      "Jae-Pil Heo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18993v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18993v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.19319v1",
    "title": "Health+: Empowering Individuals via Unifying Health Data",
    "summary": "Managing personal health data is a challenge in today's fragmented and institution-centric healthcare ecosystem. Individuals often lack meaningful control over their medical records, which are scattered across incompatible systems and formats. This vision paper presents Health+, a user-centric, multimodal health data management system that empowers individuals (including those with limited technical expertise) to upload, query, and share their data across modalities (e.g., text, images, reports). Rather than aiming for institutional overhaul, Health+ emphasizes individual agency by providing intuitive interfaces and intelligent recommendations for data access and sharing. At the system level, it tackles the complexity of storing, integrating, and securing heterogeneous health records, ensuring both efficiency and privacy. By unifying multimodal data and prioritizing patients, Health+ lays the foundation for a more connected, interpretable, and user-controlled health information ecosystem.",
    "authors": [
      "Sujaya Maiyya",
      "Shantanu Sharma",
      "Avinash Kumar"
    ],
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CR",
      "cs.DB",
      "cs.DC"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19319v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19319v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19298v1",
    "title": "ALPACA: A Reinforcement Learning Environment for Medication Repurposing and Treatment Optimization in Alzheimer's Disease",
    "summary": "Evaluating personalized, sequential treatment strategies for Alzheimer's disease (AD) using clinical trials is often impractical due to long disease horizons and substantial inter-patient heterogeneity. To address these constraints, we present the Alzheimer's Learning Platform for Adaptive Care Agents (ALPACA), an open-source, Gym-compatible reinforcement learning (RL) environment for systematically exploring personalized treatment strategies using existing therapies. ALPACA is powered by the Continuous Action-conditioned State Transitions (CAST) model trained on longitudinal trajectories from the Alzheimer's Disease Neuroimaging Initiative (ADNI), enabling medication-conditioned simulation of disease progression under alternative treatment decisions. We show that CAST autoregressively generates realistic medication-conditioned trajectories and that RL policies trained in ALPACA outperform no-treatment and behavior-cloned clinician baselines on memory-related outcomes. Interpretability analyses further indicated that the learned policies relied on clinically meaningful patient features when selecting actions. Overall, ALPACA provides a reusable in silico testbed for studying individualized sequential treatment decision-making for AD.",
    "authors": [
      "Nolan Brady",
      "Tom Yeh"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19298v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19298v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19239v1",
    "title": "Attention Deficits in Language Models: Causal Explanations for Procedural Hallucinations",
    "summary": "Large language models can follow complex procedures yet fail at a seemingly trivial final step: reporting a value they themselves computed moments earlier. We study this phenomenon as \\emph{procedural hallucination}: failure to execute a verifiable, prompt-grounded specification even when the correct value is present in context.   In long-context binding tasks with a known single-token candidate set, we find that many errors are readout-stage routing failures. Specifically, failures decompose into Stage~2A (gating) errors, where the model does not enter answer mode, and Stage~2B (binding) errors, where it enters answer mode but selects the wrong candidate (often due to recency bias). In the hard regime, Stage~2B accounts for most errors across model families in our tasks (Table~1).   On Stage~2B error trials, a linear probe on the final-layer residual stream recovers the correct value far above chance (e.g., 74\\% vs.\\ 2\\% on Qwen2.5-3B; Table~2), indicating that the answer is encoded but not used. We formalize ``present but not used'' via available vs.\\ used mutual information and pseudo-prior interventions, yielding output-computable diagnostics and information-budget certificates.   Finally, an oracle checkpointing intervention that restates the true binding near the query can nearly eliminate Stage~2B failures at long distance (e.g., Qwen2.5-3B $0/400 \\rightarrow 399/400$ at $k = 1024$; Table~8).",
    "authors": [
      "Ahmed Karim",
      "Fatima Sheaib",
      "Zein Khamis",
      "Maggie Chlon",
      "Jad Awada",
      "Leon Chlon"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19239v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19239v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19206v1",
    "title": "GS-CLIP: Zero-shot 3D Anomaly Detection by Geometry-Aware Prompt and Synergistic View Representation Learning",
    "summary": "Zero-shot 3D Anomaly Detection is an emerging task that aims to detect anomalies in a target dataset without any target training data, which is particularly important in scenarios constrained by sample scarcity and data privacy concerns. While current methods adapt CLIP by projecting 3D point clouds into 2D representations, they face challenges. The projection inherently loses some geometric details, and the reliance on a single 2D modality provides an incomplete visual understanding, limiting their ability to detect diverse anomaly types. To address these limitations, we propose the Geometry-Aware Prompt and Synergistic View Representation Learning (GS-CLIP) framework, which enables the model to identify geometric anomalies through a two-stage learning process. In stage 1, we dynamically generate text prompts embedded with 3D geometric priors. These prompts contain global shape context and local defect information distilled by our Geometric Defect Distillation Module (GDDM). In stage 2, we introduce Synergistic View Representation Learning architecture that processes rendered and depth images in parallel. A Synergistic Refinement Module (SRM) subsequently fuses the features of both streams, capitalizing on their complementary strengths. Comprehensive experimental results on four large-scale public datasets show that GS-CLIP achieves superior performance in detection. Code can be available at https://github.com/zhushengxinyue/GS-CLIP.",
    "authors": [
      "Zehao Deng",
      "An Liu",
      "Yan Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19206v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19206v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19159v1",
    "title": "Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM",
    "summary": "Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.",
    "authors": [
      "Francesca Bianco",
      "Derek Shiller"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19159v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19159v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19141v1",
    "title": "Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians",
    "summary": "\"AI psychosis\" or \"delusional spiraling\" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called \"sycophancy.\" In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.",
    "authors": [
      "Kartik Chandra",
      "Max Kleiman-Weiner",
      "Jonathan Ragan-Kelley",
      "Joshua B. Tenenbaum"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19141v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19141v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19133v1",
    "title": "A Dataset for Named Entity Recognition and Relation Extraction from Art-historical Image Descriptions",
    "summary": "This paper introduces FRAME (Fine-grained Recognition of Art-historical Metadata and Entities), a manually annotated dataset of art-historical image descriptions for Named Entity Recognition (NER) and Relation Extraction (RE). Descriptions were collected from museum catalogs, auction listings, open-access platforms, and scholarly databases, then filtered to ensure that each text focuses on a single artwork and contains explicit statements about its material, composition, or iconography. FRAME provides stand-off annotations in three layers: a metadata layer for object-level properties, a content layer for depicted subjects and motifs, and a co-reference layer linking repeated mentions. Across layers, entity spans are labeled with 37 types and connected by typed RE links between mentions. Entity types are aligned with Wikidata to support Named Entity Linking (NEL) and downstream knowledge-graph construction. The dataset is released as UIMA XMI Common Analysis Structure (CAS) files with accompanying images and bibliographic metadata, and can be used to benchmark and fine-tune NER and RE systems, including zero- and few-shot setups with Large Language Models (LLMs).",
    "authors": [
      "Stefanie Schneider",
      "Miriam G\u00f6ldl",
      "Julian Stalter",
      "Ricarda Vollmer"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19133v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19133v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19101v1",
    "title": "Value Entanglement: Conflation Between Different Kinds of Good In (Some) Large Language Models",
    "summary": "Value alignment of Large Language Models (LLMs) requires us to empirically measure these models' actual, acquired representation of value. Among the characteristics of value representation in humans is that they distinguish among value of different kinds. We investigate whether LLMs likewise distinguish three different kinds of good: moral, grammatical, and economic. By probing model behavior, embeddings, and residual stream activations, we report pervasive cases of value entanglement: a conflation between these distinct representations of value. Specifically, both grammatical and economic valuation was found to be overly influenced by moral value, relative to human norms. This conflation was repaired by selective ablation of the activation vectors associated with morality.",
    "authors": [
      "Seong Hah Cho",
      "Junyi Li",
      "Anna Leshinskaya"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19101v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19101v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19071v1",
    "title": "Defining Explainable AI for Requirements Analysis",
    "summary": "Explainable Artificial Intelligence (XAI) has become popular in the last few years. The Artificial Intelligence (AI) community in general, and the Machine Learning (ML) community in particular, is coming to the realisation that in many applications, for AI to be trusted, it must not only demonstrate good performance in its decisionmaking, but it also must explain these decisions and convince us that it is making the decisions for the right reasons. However, different applications have different requirements on the information required of the underlying AI system in order to convince us that it is worthy of our trust. How do we define these requirements?   In this paper, we present three dimensions for categorising the explanatory requirements of different applications. These are Source, Depth and Scope. We focus on the problem of matching up the explanatory requirements of different applications with the capabilities of underlying ML techniques to provide them. We deliberately avoid including aspects of explanation that are already well-covered by the existing literature and we focus our discussion on ML although the principles apply to AI more broadly.",
    "authors": [
      "Raymond Sheh",
      "Isaac Monteath"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19071v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19071v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.19109v1",
    "title": "Post-Routing Arithmetic in Llama-3: Last-Token Result Writing and Rotation-Structured Digit Directions",
    "summary": "We study three-digit addition in Meta-Llama-3-8B (base) under a one-token readout to characterize how   arithmetic answers are finalized after cross-token routing becomes causally irrelevant.   Causal residual patching and cumulative attention ablations localize a sharp boundary near layer~17:   beyond it, the decoded sum is controlled almost entirely by the last input token and late-layer self-attention   is largely dispensable.   In this post-routing regime, digit(-sum) direction dictionaries vary with a next-higher-digit context but are   well-related by an approximately orthogonal map inside a shared low-rank subspace (low-rank Procrustes alignment).   Causal digit editing matches this geometry: naive cross-context transfer fails, while rotating directions through the   learned map restores strict counterfactual edits; negative controls do not recover.",
    "authors": [
      "Yao Yan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19109v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19109v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2602.19308v1",
    "title": "WildOS: Open-Vocabulary Object Search in the Wild",
    "summary": "Autonomous navigation in complex, unstructured outdoor environments requires robots to operate over long ranges without prior maps and limited depth sensing. In such settings, relying solely on geometric frontiers for exploration is often insufficient. In such settings, the ability to reason semantically about where to go and what is safe to traverse is crucial for robust, efficient exploration. This work presents WildOS, a unified system for long-range, open-vocabulary object search that combines safe geometric exploration with semantic visual reasoning. WildOS builds a sparse navigation graph to maintain spatial memory, while utilizing a foundation-model-based vision module, ExploRFM, to score frontier nodes of the graph. ExploRFM simultaneously predicts traversability, visual frontiers, and object similarity in image space, enabling real-time, onboard semantic navigation tasks. The resulting vision-scored graph enables the robot to explore semantically meaningful directions while ensuring geometric safety. Furthermore, we introduce a particle-filter-based method for coarse localization of the open-vocabulary target query, that estimates candidate goal positions beyond the robot's immediate depth horizon, enabling effective planning toward distant goals. Extensive closed-loop field experiments across diverse off-road and urban terrains demonstrate that WildOS enables robust navigation, significantly outperforming purely geometric and purely vision-based baselines in both efficiency and autonomy. Our results highlight the potential of vision foundation models to drive open-world robotic behaviors that are both semantically informed and geometrically grounded. Project Page: https://leggedrobotics.github.io/wildos/",
    "authors": [
      "Hardik Shah",
      "Erica Tevere",
      "Deegan Atha",
      "Marcel Kaufmann",
      "Shehryar Khattak",
      "Manthan Patel",
      "Marco Hutter",
      "Jonas Frey",
      "Patrick Spieler"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19308v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19308v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.19224v1",
    "title": "Knowledge-aware Visual Question Generation for Remote Sensing Images",
    "summary": "With the rapid development of remote sensing image archives, asking questions about images has become an effective way of gathering specific information or performing image retrieval. However, automatically generated image-based questions tend to be simplistic and template-based, which hinders the real deployment of question answering or visual dialogue systems. To enrich and diversify the questions, we propose a knowledge-aware remote sensing visual question generation model, KRSVQG, that incorporates external knowledge related to the image content to improve the quality and contextual understanding of the generated questions. The model takes an image and a related knowledge triplet from external knowledge sources as inputs and leverages image captioning as an intermediary representation to enhance the image grounding of the generated questions. To assess the performance of KRSVQG, we utilized two datasets that we manually annotated: NWPU-300 and TextRS-300. Results on these two datasets demonstrate that KRSVQG outperforms existing methods and leads to knowledge-enriched questions, grounded in both image and domain knowledge.",
    "authors": [
      "Siran Li",
      "Li Mi",
      "Javiera Castillo-Navarro",
      "Devis Tuia"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19224v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19224v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.19170v1",
    "title": "BriMA: Bridged Modality Adaptation for Multi-Modal Continual Action Quality Assessment",
    "summary": "Action Quality Assessment (AQA) aims to score how well an action is performed and is widely used in sports analysis, rehabilitation assessment, and human skill evaluation. Multi-modal AQA has recently achieved strong progress by leveraging complementary visual and kinematic cues, yet real-world deployments often suffer from non-stationary modality imbalance, where certain modalities become missing or intermittently available due to sensor failures or annotation gaps. Existing continual AQA methods overlook this issue and assume that all modalities remain complete and stable throughout training, which restricts their practicality. To address this challenge, we introduce Bridged Modality Adaptation (BriMA), an innovative approach to multi-modal continual AQA under modality-missing conditions. BriMA consists of a memory-guided bridging imputation module that reconstructs missing modalities using both task-agnostic and task-specific representations, and a modality-aware replay mechanism that prioritizes informative samples based on modality distortion and distribution drift. Experiments on three representative multi-modal AQA datasets (RG, Fis-V, and FS1000) show that BriMA consistently improves performance under different modality-missing conditions, achieving 6--8\\% higher correlation and 12--15\\% lower error on average. These results demonstrate a step toward robust multi-modal AQA systems under real-world deployment constraints.",
    "authors": [
      "Kanglei Zhou",
      "Chang Li",
      "Qingyi Pan",
      "Liyuan Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19170v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19170v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.19134v1",
    "title": "Mapping Networks",
    "summary": "The escalating parameter counts in modern deep learning models pose a fundamental challenge to efficient training and resolution of overfitting. We address this by introducing the \\emph{Mapping Networks} which replace the high dimensional weight space by a compact, trainable latent vector based on the hypothesis that the trained parameters of large networks reside on smooth, low-dimensional manifolds. Henceforth, the Mapping Theorem enforced by a dedicated Mapping Loss, shows the existence of a mapping from this latent space to the target weight space both theoretically and in practice. Mapping Networks significantly reduce overfitting and achieve comparable to better performance than target network across complex vision and sequence tasks, including Image Classification, Deepfake Detection etc, with $\\mathbf{99.5\\%}$, i.e., around $500\\times$ reduction in trainable parameters.",
    "authors": [
      "Lord Sen",
      "Shyamapada Mukherjee"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19134v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19134v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.18996v1",
    "title": "Learning Cross-View Object Correspondence via Cycle-Consistent Mask Prediction",
    "summary": "We study the task of establishing object-level visual correspondence across different viewpoints in videos, focusing on the challenging egocentric-to-exocentric and exocentric-to-egocentric scenarios. We propose a simple yet effective framework based on conditional binary segmentation, where an object query mask is encoded into a latent representation to guide the localization of the corresponding object in a target video. To encourage robust, view-invariant representations, we introduce a cycle-consistency training objective: the predicted mask in the target view is projected back to the source view to reconstruct the original query mask. This bidirectional constraint provides a strong self-supervisory signal without requiring ground-truth annotations and enables test-time training (TTT) at inference. Experiments on the Ego-Exo4D and HANDAL-X benchmarks demonstrate the effectiveness of our optimization objective and TTT strategy, achieving state-of-the-art performance. The code is available at https://github.com/shannany0606/CCMP.",
    "authors": [
      "Shannan Yan",
      "Leqi Zheng",
      "Keyu Lv",
      "Jingchen Ni",
      "Hongyang Wei",
      "Jiajun Zhang",
      "Guangting Wang",
      "Jing Lyu",
      "Chun Yuan",
      "Fengyun Rao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18996v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18996v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2602.19083v1",
    "title": "ChordEdit: One-Step Low-Energy Transport for Image Editing",
    "summary": "The advent of one-step text-to-image (T2I) models offers unprecedented synthesis speed. However, their application to text-guided image editing remains severely hampered, as forcing existing training-free editors into a single inference step fails. This failure manifests as severe object distortion and a critical loss of consistency in non-edited regions, resulting from the high-energy, erratic trajectories produced by naive vector arithmetic on the models' structured fields. To address this problem, we introduce ChordEdit, a model agnostic, training-free, and inversion-free method that facilitates high-fidelity one-step editing. We recast editing as a transport problem between the source and target distributions defined by the source and target text prompts. Leveraging dynamic optimal transport theory, we derive a principled, low-energy control strategy. This strategy yields a smoothed, variance-reduced editing field that is inherently stable, facilitating the field to be traversed in a single, large integration step. A theoretically grounded and experimentally validated approach allows ChordEdit to deliver fast, lightweight and precise edits, finally achieving true real-time editing on these challenging models.",
    "authors": [
      "Liangsi Lu",
      "Xuhang Chen",
      "Minzhe Guo",
      "Shichu Li",
      "Jingchao Wang",
      "Yang Shi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19083v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19083v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2602.18990v1",
    "title": "IDSelect: A RL-Based Cost-Aware Selection Agent for Video-based Multi-Modal Person Recognition",
    "summary": "Video-based person recognition achieves robust identification by integrating face, body, and gait. However, current systems waste computational resources by processing all modalities with fixed heavyweight ensembles regardless of input complexity. To address these limitations, we propose IDSelect, a reinforcement learning-based cost-aware selector that chooses one pre-trained model per modality per-sequence to optimize the accuracy-efficiency trade-off. Our key insight is that an input-conditioned selector can discover complementary model choices that surpass fixed ensembles while using substantially fewer resources. IDSelect trains a lightweight agent end-to-end using actor-critic reinforcement learning with budget-aware optimization. The reward balances recognition accuracy with computational cost, while entropy regularization prevents premature convergence. At inference, the policy selects the most probable model per modality and fuses modality-specific similarities for the final score. Extensive experiments on challenging video-based datasets demonstrate IDSelect's superior efficiency: on CCVID, it achieves 95.9% Rank-1 accuracy with 92.4% less computation than strong baselines while improving accuracy by 1.8%; on MEVID, it reduces computation by 41.3% while maintaining competitive performance.",
    "authors": [
      "Yuyang Ji",
      "Yixuan Shen",
      "Kien Nguyen",
      "Lifeng Zhou",
      "Feng Liu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18990v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18990v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2602.19323v1",
    "title": "DefenseSplat: Enhancing the Robustness of 3D Gaussian Splatting via Frequency-Aware Filtering",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for real-time and high-fidelity 3D reconstruction from posed images. However, recent studies reveal its vulnerability to adversarial corruptions in input views, where imperceptible yet consistent perturbations can drastically degrade rendering quality, increase training and rendering time, and inflate memory usage, even leading to server denial-of-service. In our work, to mitigate this issue, we begin by analyzing the distinct behaviors of adversarial perturbations in the low- and high-frequency components of input images using wavelet transforms. Based on this observation, we design a simple yet effective frequency-aware defense strategy that reconstructs training views by filtering high-frequency noise while preserving low-frequency content. This approach effectively suppresses adversarial artifacts while maintaining the authenticity of the original scene. Notably, it does not significantly impair training on clean data, achieving a desirable trade-off between robustness and performance on clean inputs. Through extensive experiments under a wide range of attack intensities on multiple benchmarks, we demonstrate that our method substantially enhances the robustness of 3DGS without access to clean ground-truth supervision. By highlighting and addressing the overlooked vulnerabilities of 3D Gaussian Splatting, our work paves the way for more robust and secure 3D reconstructions.",
    "authors": [
      "Yiran Qiao",
      "Yiren Lu",
      "Yunlai Zhou",
      "Rui Yang",
      "Linlin Hou",
      "Yu Yin",
      "Jing Ma"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19323v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19323v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2602.19278v1",
    "title": "A Two-Stage Detection-Tracking Framework for Stable Apple Quality Inspection in Dense Conveyor-Belt Environments",
    "summary": "Industrial fruit inspection systems must operate reliably under dense multi-object interactions and continuous motion, yet most existing works evaluate detection or classification at the image level without ensuring temporal stability in video streams. We present a two-stage detection-tracking framework for stable multi-apple quality inspection in conveyor-belt environments. An orchard-trained YOLOv8 model performs apple localization, followed by ByteTrack multi-object tracking to maintain persistent identities. A ResNet18 defect classifier, fine-tuned on a healthy-defective fruit dataset, is applied to cropped apple regions. Track-level aggregation is introduced to enforce temporal consistency and reduce prediction oscillation across frames. We define video-level industrial metrics such as track-level defect ratio and temporal consistency to evaluate system robustness under realistic processing conditions. Results demonstrate improved stability compared to frame-wise inference, suggesting that integrating tracking is essential for practical automated fruit grading systems.",
    "authors": [
      "Keonvin Park",
      "Aditya Pal",
      "Jin Hong Mok"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19278v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19278v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2602.19055v1",
    "title": "Automated Disentangling Analysis of Skin Colour for Lesion Images",
    "summary": "Machine-learning models working on skin images often have degraded performance when the skin colour captured in images (SCCI) differs between training and deployment. Such differences arise from entangled environmental factors (e.g., illumination, camera settings), and intrinsic factors (e.g., skin tone) that cannot be accurately described by a single \"skin tone\" scalar. To mitigate such colour mismatch, we propose a skin-colour disentangling framework that adapts disentanglement-by-compression to learn a structured, manipulable latent space for SCCI from unlabelled dermatology images. To prevent information leakage that hinders proper learning of dark colour features, we introduce a randomized, mostly monotonic decolourization mapping. To suppress unintended colour shifts of localized patterns (e.g., ink marks, scars) during colour manipulation, we further propose a geometry-aligned post-processing step. Together, these components enable faithful counterfactual editing and answering an essential question: \"What would this skin condition look like under a different SCCI?\", as well as direct colour transfer between images and controlled traversal along physically meaningful directions (e.g., blood perfusion, camera white balance), enabling educational visualization of skin conditions under varying SCCI. We demonstrate that dataset-level augmentation and colour normalization based on our framework achieve competitive lesion classification performance.",
    "authors": [
      "Wenbo Yang",
      "Eman Rezk",
      "Walaa M. Moursi",
      "Zhou Wang"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.19055v1",
    "pdf_url": "https://arxiv.org/pdf/2602.19055v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2602.18989v1",
    "title": "All Constant Mutation Rates for the $(1+1)$ Evolutionary Algorithm",
    "summary": "For every mutation rate $p \\in (0, 1)$, and for all $\\varepsilon > 0$, there is a fitness function $f : \\{0,1\\}^n \\to \\mathbb{R}$ with a unique maximum for which the optimal mutation rate for the $(1+1)$ evolutionary algorithm on $f$ is in $(p-\\varepsilon, p+\\varepsilon)$. In other words, the set of optimal mutation rates for the $(1+1)$ EA is dense in the interval $[0, 1]$. To show that, this paper introduces DistantSteppingStones, a fitness function which consists of large plateaus separated by large fitness valleys.",
    "authors": [
      "Andrew James Kelley"
    ],
    "categories": [
      "cs.NE"
    ],
    "published": "2026-02-22",
    "url": "https://arxiv.org/abs/2602.18989v1",
    "pdf_url": "https://arxiv.org/pdf/2602.18989v1.pdf",
    "date": "2026-02-24",
    "source": "arxiv",
    "research_score": 0.42
  },
  {
    "model_id": "laion/exp-uns-tezos-160x_glm_4_7_traces_jupiter",
    "author": "unknown",
    "title": "laion/exp-uns-tezos-160x_glm_4_7_traces_jupiter",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen3",
      "text-generation",
      "llama-factory",
      "full",
      "generated_from_trainer",
      "conversational",
      "base_model:Qwen/Qwen3-8B",
      "base_model:finetune:Qwen/Qwen3-8B",
      "license:apache-2.0",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2026-02-23T14:17:36.000Z",
    "last_modified": "2026-02-24T02:28:45.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/laion/exp-uns-tezos-160x_glm_4_7_traces_jupiter",
    "date": "2026-02-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "SuperDan/Af-M2.5",
    "author": "unknown",
    "title": "SuperDan/Af-M2.5",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "minimax_m2",
      "text-generation",
      "conversational",
      "custom_code",
      "license:other",
      "endpoints_compatible",
      "fp8",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2026-02-24T02:26:45.000Z",
    "last_modified": "2026-02-24T02:26:47.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/SuperDan/Af-M2.5",
    "date": "2026-02-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "Umora/Liya",
    "author": "unknown",
    "title": "Umora/Liya",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "diffusers",
      "text-to-image",
      "lora",
      "template:diffusion-lora",
      "base_model:valiantcat/Wan2.1-I2V-Twerk-Lora",
      "base_model:adapter:valiantcat/Wan2.1-I2V-Twerk-Lora",
      "license:apache-2.0",
      "region:us"
    ],
    "pipeline_tag": "text-to-image",
    "library": "diffusers",
    "created_at": "2026-02-23T19:32:19.000Z",
    "last_modified": "2026-02-24T02:23:07.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/Umora/Liya",
    "date": "2026-02-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  },
  {
    "model_id": "NathanRoll/writing-rlvr-qwen2.5-1.5b",
    "author": "unknown",
    "title": "NathanRoll/writing-rlvr-qwen2.5-1.5b",
    "downloads": 0,
    "likes": 0,
    "tags": [
      "transformers",
      "safetensors",
      "qwen2",
      "text-generation",
      "generated_from_trainer",
      "trl",
      "grpo",
      "conversational",
      "arxiv:2402.03300",
      "base_model:Qwen/Qwen2.5-1.5B-Instruct",
      "base_model:finetune:Qwen/Qwen2.5-1.5B-Instruct",
      "text-generation-inference",
      "endpoints_compatible",
      "region:us"
    ],
    "pipeline_tag": "text-generation",
    "library": "transformers",
    "created_at": "2026-02-23T05:13:57.000Z",
    "last_modified": "2026-02-24T02:22:15.000Z",
    "days_since_creation": 0,
    "url": "https://huggingface.co/NathanRoll/writing-rlvr-qwen2.5-1.5b",
    "date": "2026-02-24",
    "source": "huggingface_model",
    "type": "model",
    "research_score": 0.4
  }
]