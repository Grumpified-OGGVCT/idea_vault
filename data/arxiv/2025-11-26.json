[
  {
    "arxiv_id": "2511.20577v1",
    "title": "MSTN: Fast and Efficient Multivariate Time Series Model",
    "summary": "Real-world time-series data is highly non stationary and complex in dynamics that operate across multiple timescales, ranging from fast, short-term changes to slow, long-term trends. Most existing models rely on fixed-scale structural priors, such as patch-based tokenization, fixed frequency transformations, or frozen backbone architectures. This often leads to over-regularization of temporal dynamics, which limits their ability to adaptively model the full spectrum of temporal variations and impairs their performance on unpredictable, Sudden, high-magnitude events. To address this, we introduce the Multi-scale Temporal Network (MSTN), a novel deep learning architecture founded on a hierarchical multi-scale and sequence modeling principle. The MSTN framework integrates: (i) a multi-scale convolutional encoder that constructs a hierarchical feature pyramid for local patterns (ii) a sequence modeling component for long-range temporal dependencies. We empirically validate this with BiLSTM and Transformer variants, establishing a flexible foundation for future architectural advancements. and (iii) a gated fusion mechanism augmented with squeeze-and-excitation (SE) and multi-head temporal attention (MHTA) for dynamic, context-aware feature integration. This design enables MSTN to adaptively model temporal patterns from milliseconds to long-range dependencies within a unified framework. Extensive evaluations across time-series long-horizon forecasting, imputation, classification and generalizability study demonstrate that MSTN achieves competitive state-of-the-art (SOTA) performance, showing improvements over contemporary approaches including EMTSF, LLM4TS, HiMTM, TIME-LLM, MTST, SOFTS, iTransformer, TimesNet, and PatchTST. In total, MSTN establishes new SOTA performance on 24 of 32 benchmark datasets, demonstrating its consistent performance across diverse temporal tasks.",
    "authors": [
      "Sumit S Shevtekar",
      "Chandresh K Maurya",
      "Gourab Sil"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20577v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20577v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 1.0
  },
  {
    "arxiv_id": "2511.20409v1",
    "title": "A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines",
    "summary": "Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).",
    "authors": [
      "Md Abdullah Al Kafi",
      "Raka Moni",
      "Sumit Kumar Banshal"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20409v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20409v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.96
  },
  {
    "arxiv_id": "2511.20643v1",
    "title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
    "summary": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.",
    "authors": [
      "Adhiraj Ghosh",
      "Vishaal Udandarao",
      "Thao Nguyen",
      "Matteo Farina",
      "Mehdi Cherti",
      "Jenia Jitsev",
      "Sewoong Oh",
      "Elisa Ricci",
      "Ludwig Schmidt",
      "Matthias Bethge"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20643v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20643v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.91
  },
  {
    "arxiv_id": "2511.20333v1",
    "title": "NNGPT: Rethinking AutoML with Large Language Models",
    "summary": "Building self-improving AI systems remains a fundamental challenge in the AI domain. We present NNGPT, an open-source framework that turns a large language model (LLM) into a self-improving AutoML engine for neural network development, primarily for computer vision. Unlike previous frameworks, NNGPT extends the dataset of neural networks by generating new models, enabling continuous fine-tuning of LLMs based on closed-loop system of generation, assessment, and self-improvement. It integrates within one unified workflow five synergistic LLM-based pipelines: zero-shot architecture synthesis, hyperparameter optimization (HPO), code-aware accuracy/early-stop prediction, retrieval-augmented synthesis of scope-closed PyTorch blocks (NN-RAG), and reinforcement learning. Built on the LEMUR dataset as an audited corpus with reproducible metrics, NNGPT emits from a single prompt and validates network architecture, preprocessing code, and hyperparameters, executes them end-to-end, and learns from result. The PyTorch adapter makes NNGPT framework-agnostic, enabling strong performance: NN-RAG achieves 73% executability on 1,289 targets, 3-shot prompting boosts accuracy on common datasets, and hash-based deduplication saves hundreds of runs. One-shot prediction matches search-based AutoML, reducing the need for numerous trials. HPO on LEMUR achieves RMSE 0.60, outperforming Optuna (0.64), while the code-aware predictor reaches RMSE 0.14 with Pearson r=0.78. The system has already generated over 5K validated models, proving NNGPT as an autonomous AutoML engine. Upon acceptance, the code, prompts, and checkpoints will be released for public access to enable reproducibility and facilitate community usage.",
    "authors": [
      "Roman Kochnev",
      "Waleed Khalid",
      "Tolgay Atinc Uzun",
      "Xi Zhang",
      "Yashkumar Sanjaybhai Dhameliya",
      "Furui Qin",
      "Chandini Vysyaraju",
      "Raghuvir Duvvuri",
      "Avi Goyal",
      "Dmitry Ignatov",
      "Radu Timofte"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20333v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20333v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2511.20297v1",
    "title": "Improving Language Agents through BREW",
    "summary": "Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\\%$ improvement in task precision, $10-15\\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.",
    "authors": [
      "Shashank Kirtania",
      "Param Biyani",
      "Priyanshu Gupta",
      "Yasharth Bajpai",
      "Roshni Iyer",
      "Sumit Gulwani",
      "Gustavo Soares"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20297v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20297v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.89
  },
  {
    "arxiv_id": "2511.20586v1",
    "title": "PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic",
    "summary": "Trustworthiness has become a key requirement for the deployment of artificial intelligence systems in safety-critical applications. Conventional evaluation metrics such as accuracy and precision fail to capture uncertainty or the reliability of model predictions, particularly under adversarial or degraded conditions. This paper introduces the \\emph{Parallel Trust Assessment System (PaTAS)}, a framework for modeling and propagating trust in neural networks using Subjective Logic (SL). PaTAS operates in parallel with standard neural computation through \\emph{Trust Nodes} and \\emph{Trust Functions} that propagate input, parameter, and activation trust across the network. The framework defines a \\emph{Parameter Trust Update} mechanism to refine parameter reliability during training and an \\emph{Inference-Path Trust Assessment (IPTA)} method to compute instance-specific trust at inference. Experiments on real-world and adversarial datasets demonstrate that PaTAS produces interpretable, symmetric, and convergent trust estimates that complement accuracy and expose reliability gaps in poisoned, biased, or uncertain data scenarios. The results show that PaTAS effectively distinguishes between benign and adversarial inputs and identifies cases where model confidence diverges from actual reliability. By enabling transparent and quantifiable trust reasoning within neural architectures, PaTAS provides a principled foundation for evaluating model reliability across the AI lifecycle.",
    "authors": [
      "Koffi Ismael Ouattara",
      "Ioannis Krontiris",
      "Theo Dimitrakos",
      "Dennis Eisermann",
      "Frank Kargl"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20586v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20586v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.82
  },
  {
    "arxiv_id": "2511.20650v1",
    "title": "MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities",
    "summary": "Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.",
    "authors": [
      "Tooba Tehreem Sheikh",
      "Jean Lahoud",
      "Rao Muhammad Anwer",
      "Fahad Shahbaz Khan",
      "Salman Khan",
      "Hisham Cholakkal"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20650v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20650v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20641v1",
    "title": "Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition",
    "summary": "Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.",
    "authors": [
      "Wei Tang",
      "Zuo-Zheng Wang",
      "Kun Zhang",
      "Tong Wei",
      "Min-Ling Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20641v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20641v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20549v1",
    "title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
    "summary": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.",
    "authors": [
      "Guanjie Chen",
      "Shirui Huang",
      "Kai Liu",
      "Jianchen Zhu",
      "Xiaoye Qu",
      "Peng Chen",
      "Yu Cheng",
      "Yifu Sun"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20549v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20549v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20277v1",
    "title": "HVAdam: A Full-Dimension Adaptive Optimizer",
    "summary": "Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity   , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.",
    "authors": [
      "Yiheng Zhang",
      "Shaowu Wu",
      "Yuanzhuo Xu",
      "Jiajun Wu",
      "Shang Xu",
      "Steve Drew",
      "Xiaoguang Niu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20277v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20277v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20211v1",
    "title": "OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation",
    "summary": "Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.",
    "authors": [
      "Hao Yu",
      "Jiabo Zhan",
      "Zile Wang",
      "Jinglin Wang",
      "Huaisong Zhang",
      "Hongyu Li",
      "Xinrui Chen",
      "Yongxian Wei",
      "Chun Yuan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20211v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20211v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20172v1",
    "title": "Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management",
    "summary": "The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.",
    "authors": [
      "Xinjun Yang",
      "Qingda Hu",
      "Junru Li",
      "Feifei Li",
      "Yuqi Zhou",
      "Yicong Zhu",
      "Qiuru Lin",
      "Jian Dai",
      "Yang Kong",
      "Jiayu Zhang",
      "Guoqiang Xu",
      "Qiang Liu"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20172v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20172v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20141v1",
    "title": "IDAP++: Advancing Divergence-Based Pruning via Filter-Level and Layer-Level Optimization",
    "summary": "This paper presents a novel approach to neural network compression that addresses redundancy at both the filter and architectural levels through a unified framework grounded in information flow analysis. Building on the concept of tensor flow divergence, which quantifies how information is transformed across network layers, we develop a two-stage optimization process. The first stage employs iterative divergence-aware pruning to identify and remove redundant filters while preserving critical information pathways. The second stage extends this principle to higher-level architecture optimization by analyzing layer-wise contributions to information propagation and selectively eliminating entire layers that demonstrate minimal impact on network performance. The proposed method naturally adapts to diverse architectures, including convolutional networks, transformers, and hybrid designs, providing a consistent metric for comparing the structural importance across different layer types. Experimental validation across multiple modern architectures and datasets reveals that this combined approach achieves substantial model compression while maintaining competitive accuracy. The presented approach achieves parameter reduction results that are globally comparable to those of state-of-the-art solutions and outperforms them across a wide range of modern neural network architectures, from convolutional models to transformers. The results demonstrate how flow divergence serves as an effective guiding principle for both filter-level and layer-level optimization, offering practical benefits for deployment in resource-constrained environments.",
    "authors": [
      "Aleksei Samarin",
      "Artem Nazarenko",
      "Egor Kotenko",
      "Valentin Malykh",
      "Alexander Savelev",
      "Aleksei Toropov"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20141v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20141v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.20201v1",
    "title": "GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering",
    "summary": "We propose GHR-VQA, Graph-guided Hierarchical Relational Reasoning for Video Question Answering (Video QA), a novel human-centric framework that incorporates scene graphs to capture intricate human-object interactions within video sequences. Unlike traditional pixel-based methods, each frame is represented as a scene graph and human nodes across frames are linked to a global root, forming the video-level graph and enabling cross-frame reasoning centered on human actors. The video-level graphs are then processed by Graph Neural Networks (GNNs), transforming them into rich, context-aware embeddings for efficient processing. Finally, these embeddings are integrated with question features in a hierarchical network operating across different abstraction levels, enhancing both local and global understanding of video content. This explicit human-rooted structure enhances interpretability by decomposing actions into human-object interactions and enables a more profound understanding of spatiotemporal dynamics. We validate our approach on the Action Genome Question Answering (AGQA) dataset, achieving significant performance improvements, including a 7.3% improvement in object-relation reasoning over the state of the art.",
    "authors": [
      "Dionysia Danai Brilli",
      "Dimitrios Mallis",
      "Vassilis Pitsikalis",
      "Petros Maragos"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20201v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20201v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2511.20471v1",
    "title": "Universe of Thoughts: Enabling Creative Reasoning with Large Language Models",
    "summary": "Reasoning based on Large Language Models (LLMs) has garnered increasing attention due to outstanding performance of these models in mathematical and complex logical tasks. Beginning with the Chain-of-Thought (CoT) prompting technique, numerous reasoning methods have emerged that decompose problems into smaller, sequential steps (or thoughts). However, existing reasoning models focus on conventional problem-solving and do not necessarily generate creative solutions by ``creative reasoning''. In domains where the solution space is expansive and conventional solutions are suboptimal, such as drug discovery or business strategization, creative reasoning to discover innovative solutions is crucial. To address this gap, first we introduce a computational framework for creative reasoning inspired by established cognitive science principles. With this framework, we propose three core creative reasoning paradigms, namely, \\textit{combinational}, \\textit{exploratory}, and \\textit{transformative} reasoning, where each offers specific directions for systematic exploration of the universe of thoughts to generate creative solutions. Next, to materialize this framework using LLMs, we introduce the \\textit{Universe of Thoughts} (or \\textit{UoT}, for short), a novel set of methods to implement the aforementioned three creative processes. Finally, we introduce three novel tasks that necessitate creative problem-solving, along with an evaluation benchmark to assess creativity from three orthogonal perspectives: feasibility as constraint, and utility and novelty as metrics. With a comparative analysis against the state-of-the-art (SOTA) reasoning techniques as well as representative commercial models with reasoning capability, we show that UoT demonstrates superior performance in creative reasoning.",
    "authors": [
      "Yuto Suzuki",
      "Farnoush Banaei-Kashani"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20471v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20471v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.20418v1",
    "title": "StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections",
    "summary": "Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\\textit{11.6%}$ HOTA improvement at $\\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.",
    "authors": [
      "Matvei Shelukhan",
      "Timur Mamedov",
      "Karina Kvanchiani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20418v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20418v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.20168v1",
    "title": "On the Limits of Momentum in Decentralized and Federated Optimization",
    "summary": "Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\\left(1/t\\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.",
    "authors": [
      "Riccardo Zaccone",
      "Sai Praneeth Karimireddy",
      "Carlo Masone"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20168v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20168v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.20626v1",
    "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training",
    "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.",
    "authors": [
      "Wei He",
      "Kai Han",
      "Hang Zhou",
      "Hanting Chen",
      "Zhicheng Liu",
      "Xinghao Chen",
      "Yunhe Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20626v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20626v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.20285v1",
    "title": "SMoG: Schema Matching on Graph",
    "summary": "Schema matching is a critical task in data integration, particularly in the medical domain where disparate Electronic Health Record (EHR) systems must be aligned to standard models like OMOP CDM. While Large Language Models (LLMs) have shown promise in schema matching, they suffer from hallucination and lack of up-to-date domain knowledge. Knowledge Graphs (KGs) offer a solution by providing structured, verifiable knowledge. However, existing KG-augmented LLM approaches often rely on inefficient complex multi-hop queries or storage-intensive vector-based retrieval methods. This paper introduces SMoG (Schema Matching on Graph), a novel framework that leverages iterative execution of simple 1-hop SPARQL queries, inspired by successful strategies in Knowledge Graph Question Answering (KGQA). SMoG enhances explainability and reliability by generating human-verifiable query paths while significantly reducing storage requirements by directly querying SPARQL endpoints. Experimental results on real-world medical datasets demonstrate that SMoG achieves performance comparable to state-of-the-art baselines, validating its effectiveness and efficiency in KG-augmented schema matching.",
    "authors": [
      "Mingyu Jeon",
      "Jaeyoung Suh",
      "Suwan Cho"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20285v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20285v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.20233v1",
    "title": "REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance",
    "summary": "The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.",
    "authors": [
      "Chuyi Kong",
      "Gao Wei",
      "Jing Ma",
      "Hongzhan Lin",
      "Zhiyuan Fan"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20233v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20233v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.20222v1",
    "title": "Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation",
    "summary": "In critical web applications such as e-commerce and recommendation systems, multimodal graphs integrating rich visual and textual attributes are increasingly central, yet their large scale introduces substantial computational burdens for training Graph Neural Networks (GNNs). While Graph Condensation (GC) offers a promising solution by synthesizing smaller datasets, existing methods falter in the multimodal setting. We identify a dual challenge causing this failure: (1) conflicting gradients arising from semantic misalignments between modalities, and (2) the GNN's message-passing architecture pathologically amplifying this gradient noise across the graph structure. To address this, we propose Structurally-Regularized Gradient Matching (SR-GM), a novel condensation framework tailored for multimodal graphs. SR-GM introduces two synergistic components: first, a gradient decoupling mechanism that resolves inter-modality conflicts at their source via orthogonal projection; and second, a structural damping regularizer that acts directly on the gradient field. By leveraging the graph's Dirichlet energy, this regularizer transforms the topology from a noise amplifier into a stabilizing force during optimization. Extensive experiments demonstrate that SR-GM significantly improves accuracy and accelerates convergence compared to baseline methods. Ablation studies confirm that addressing both gradient conflict and structural amplification in tandem is essential for achieving superior performance. Moreover, the condensed multimodal graphs exhibit strong cross-architecture generalization and promise to accelerate applications like Neural Architecture Search. This research provides a scalable methodology for multimodal graph-based learning in resource-constrained environments.",
    "authors": [
      "Lian Shen",
      "Zhendan Chen",
      "Yinhui jiang",
      "Meijia Song",
      "Ziming Su",
      "Juan Liu",
      "Xiangrong Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20222v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20222v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.20564v1",
    "title": "E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems",
    "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.",
    "authors": [
      "Rui Xue",
      "Shichao Zhu",
      "Liang Qin",
      "Guangmou Pan",
      "Yang Song",
      "Tianfu Wu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20564v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20564v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2511.20380v1",
    "title": "Differentiable Attenuation Filters for Feedback Delay Networks",
    "summary": "We introduce a novel method for designing attenuation filters in digital audio reverberation systems based on Feedback Delay Networks (FDNs). Our approach uses Second Order Sections (SOS) of Infinite Impulse Response (IIR) filters arranged as parametric equalizers (PEQ), enabling fine control over frequency-dependent reverberation decay. Unlike traditional graphic equalizer designs, which require numerous filters per delay line, we propose a scalable solution where the number of filters can be adjusted. The frequency, gain, and quality factor (Q) parameters are shared parameters across delay lines and only the gain is adjusted based on delay length. This design not only reduces the number of optimization parameters, but also remains fully differentiable and compatible with gradient-based learning frameworks. Leveraging principles of analog filter design, our method allows for efficient and accurate filter fitting using supervised learning. Our method delivers a flexible and differentiable design, achieving state-of-the-art performance while significantly reducing computational cost.",
    "authors": [
      "Ilias Ibnyahya",
      "Joshua D. Reiss"
    ],
    "categories": [
      "cs.SD",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20380v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20380v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.20359v1",
    "title": "From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations",
    "summary": "Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.",
    "authors": [
      "Zhiqing Guo",
      "Dongdong Xi",
      "Songlin Li",
      "Gaobo Yang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20359v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20359v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.20615v1",
    "title": "Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities",
    "summary": "This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.",
    "authors": [
      "Seyede Niloofar Hosseini",
      "Ali Mojibi",
      "Mahdi Mohseni",
      "Navid Arjmand",
      "Alireza Taheri"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20615v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20615v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.20340v1",
    "title": "Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios",
    "summary": "Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.",
    "authors": [
      "Luohe Shi",
      "Zuchao Li",
      "Lefei Zhang",
      "Baoyuan Qi",
      "Guoming Liu",
      "Hai Zhao"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20340v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20340v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.20639v1",
    "title": "Latent Collaboration in Multi-Agent Systems",
    "summary": "Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.",
    "authors": [
      "Jiaru Zou",
      "Xiyuan Yang",
      "Ruizhong Qiu",
      "Gaotang Li",
      "Katherine Tieu",
      "Pan Lu",
      "Ke Shen",
      "Hanghang Tong",
      "Yejin Choi",
      "Jingrui He",
      "James Zou",
      "Mengdi Wang",
      "Ling Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20639v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20639v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.20551v1",
    "title": "Time-Domain Linear Model-based Framework for Passive Acoustic Mapping of Cavitation Activity",
    "summary": "Passive acoustic mapping enables the spatial mapping and temporal monitoring of cavitation activity, playing a crucial role in therapeutic ultrasound applications. Most conventional beamforming methods, whether implemented in the time or frequency domains, suffer from limited axial resolution due to the absence of a reference emission onset time. While frequency-domain methods, the most efficient of which are based on the cross-spectral matrix, require long signals for accurate estimation, time-domain methods typically achieve lower spatial resolution. To address these limitations, we propose a linear model-based beamforming framework fully formulated in the time domain. The linear forward model relates a discretized spatiotemporal distribution of cavitation activity to the temporal signals recorded by a probe, explicitly accounting for time-of-flight delays dictated by the acquisition geometry. This model is then inverted using regularization techniques that exploit prior knowledge of cavitation activity in both spatial and temporal domains. Experimental results show that the proposed framework achieves enhanced or competitive cavitation map quality while using only 20\\% of the data typically required by frequency-domain methods. This highlights the substantial gain in data efficiency and the flexibility of our spatiotemporal regularization to adapt to diverse passive cavitation scenarios, outperforming state-of-the-art techniques.",
    "authors": [
      "Tatiana Gelvez-Barrera",
      "Barbara Nicolas",
      "Denis Kouamé",
      "Bruno Gilles",
      "Adrian Basarab"
    ],
    "categories": [
      "eess.SP",
      "cs.AI",
      "eess.IV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20551v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20551v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.20534v1",
    "title": "Bridging the Language Gap: Synthetic Voice Diversity via Latent Mixup for Equitable Speech Recognition",
    "summary": "Modern machine learning models for audio tasks often exhibit superior performance on English and other well-resourced languages, primarily due to the abundance of available training data. This disparity leads to an unfair performance gap for low-resource languages, where data collection is both challenging and costly. In this work, we introduce a novel data augmentation technique for speech corpora designed to mitigate this gap. Through comprehensive experiments, we demonstrate that our method significantly improves the performance of automatic speech recognition systems on low-resource languages. Furthermore, we show that our approach outperforms existing augmentation strategies, offering a practical solution for enhancing speech technology in underrepresented linguistic communities.",
    "authors": [
      "Wesley Bian",
      "Xiaofeng Lin",
      "Guang Cheng"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20534v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20534v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.20543v1",
    "title": "Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media",
    "summary": "The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.",
    "authors": [
      "Alhasan Abdellatif",
      "Hannah P. Menke",
      "Ahmed H. Elsheikh",
      "Florian Doster",
      "Kamaljit Singh"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20543v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20543v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.20501v1",
    "title": "A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences",
    "summary": "Accurate extraction and segmentation of the cerebral arteries from digital subtraction angiography (DSA) sequences is essential for developing reliable clinical management models of complex cerebrovascular diseases. Conventional loss functions often rely solely on pixel-wise overlap, overlooking the geometric and physical consistency of vascular boundaries, which can lead to fragmented or unstable vessel predictions. To overcome this limitation, we propose a novel \\textit{Physics-Informed Loss} (PIL) that models the interaction between the predicted and ground-truth boundaries as an elastic process inspired by dislocation theory in materials physics. This formulation introduces a physics-based regularization term that enforces smooth contour evolution and structural consistency, allowing the network to better capture fine vascular geometry. The proposed loss is integrated into several segmentation architectures, including U-Net, U-Net++, SegFormer, and MedFormer, and evaluated on two public benchmarks: DIAS and DSCA. Experimental results demonstrate that PIL consistently outperforms conventional loss functions such as Cross-Entropy, Dice, Active Contour, and Surface losses, achieving superior sensitivity, F1 score, and boundary coherence. These findings confirm that the incorporation of physics-based boundary interactions into deep neural networks improves both the precision and robustness of vascular segmentation in dynamic angiographic imaging. The implementation of the proposed method is publicly available at https://github.com/irfantahir301/Physicsis_loss.",
    "authors": [
      "Muhammad Irfan",
      "Nasir Rahim",
      "Khalid Mahmood Malik"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20501v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20501v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.20397v1",
    "title": "Model-Based Learning of Whittle indices",
    "summary": "We present BLINQ, a new model-based algorithm that learns the Whittle indices of an indexable, communicating and unichain Markov Decision Process (MDP). Our approach relies on building an empirical estimate of the MDP and then computing its Whittle indices using an extended version of a state-of-the-art existing algorithm. We provide a proof of convergence to the Whittle indices we want to learn as well as a bound on the time needed to learn them with arbitrary precision. Moreover, we investigate its computational complexity. Our numerical experiments suggest that BLINQ significantly outperforms existing Q-learning approaches in terms of the number of samples needed to get an accurate approximation. In addition, it has a total computational cost even lower than Q-learning for any reasonably high number of samples. These observations persist even when the Q-learning algorithms are speeded up using pre-trained neural networks to predict Q-values.",
    "authors": [
      "Joël Charles-Rebuffé",
      "Nicolas Gast",
      "Bruno Gaujal"
    ],
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.NA"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20397v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20397v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.20293v1",
    "title": "Forgetting by Pruning: Data Deletion in Join Cardinality Estimation",
    "summary": "Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.",
    "authors": [
      "Chaowei He",
      "Yuanjun Liu",
      "Qingzhi Ma",
      "Shenyuan Ren",
      "Xizhao Luo",
      "Lei Zhao",
      "An Liu"
    ],
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20293v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20293v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.20278v1",
    "title": "DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion",
    "summary": "Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.",
    "authors": [
      "Yinghui Li",
      "Qianyu Zhou",
      "Di Shao",
      "Hao Yang",
      "Ye Zhu",
      "Richard Dazeley",
      "Xuequan Lu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20278v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20278v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.20257v1",
    "title": "Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling",
    "summary": "Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.",
    "authors": [
      "Zhiguo Zhang",
      "Xiaoliang Ma",
      "Daniel Schlesinger"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20257v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20257v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.20623v1",
    "title": "Copyright Detection in Large Language Models: An Ethical Approach to Generative AI Development",
    "summary": "The widespread use of Large Language Models (LLMs) raises critical concerns regarding the unauthorized inclusion of copyrighted content in training data. Existing detection frameworks, such as DE-COP, are computationally intensive, and largely inaccessible to independent creators. As legal scrutiny increases, there is a pressing need for a scalable, transparent, and user-friendly solution. This paper introduce an open-source copyright detection platform that enables content creators to verify whether their work was used in LLM training datasets. Our approach enhances existing methodologies by facilitating ease of use, improving similarity detection, optimizing dataset validation, and reducing computational overhead by 10-30% with efficient API calls. With an intuitive user interface and scalable backend, this framework contributes to increasing transparency in AI development and ethical compliance, facilitating the foundation for further research in responsible AI development and copyright enforcement.",
    "authors": [
      "David Szczecina",
      "Senan Gaffori",
      "Edmond Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20623v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20623v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20609v1",
    "title": "Adaptive Hopfield Network: Rethinking Similarities in Associative Memory",
    "summary": "Associative memory models are content-addressable memory systems fundamental to biological intelligence and are notable for their high interpretability. However, existing models evaluate the quality of retrieval based on proximity, which cannot guarantee that the retrieved pattern has the strongest association with the query, failing correctness. We reframe this problem by proposing that a query is a generative variant of a stored memory pattern, and define a variant distribution to model this subtle context-dependent generative process. Consequently, correct retrieval should return the memory pattern with the maximum a posteriori probability of being the query's origin. This perspective reveals that an ideal similarity measure should approximate the likelihood of each stored pattern generating the query in accordance with variant distribution, which is impossible for fixed and pre-defined similarities used by existing associative memories. To this end, we develop adaptive similarity, a novel mechanism that learns to approximate this insightful but unknown likelihood from samples drawn from context, aiming for correct retrieval. We theoretically prove that our proposed adaptive similarity achieves optimal correct retrieval under three canonical and widely applicable types of variants: noisy, masked, and biased. We integrate this mechanism into a novel adaptive Hopfield network (A-Hop), and empirical results show that it achieves state-of-the-art performance across diverse tasks, including memory retrieval, tabular classification, image classification, and multiple instance learning.",
    "authors": [
      "Shurong Wang",
      "Yuqi Pan",
      "Zhuoyang Shen",
      "Meng Zhang",
      "Hongwei Wang",
      "Guoqi Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20609v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20609v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20468v1",
    "title": "DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs",
    "summary": "Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed",
    "authors": [
      "Yuanhao Li",
      "Mingshan Liu",
      "Hongbo Wang",
      "Yiding Zhang",
      "Yifei Ma",
      "Wei Tan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20468v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20468v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20462v1",
    "title": "STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow",
    "summary": "Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.",
    "authors": [
      "Jiatao Gu",
      "Ying Shen",
      "Tianrong Chen",
      "Laurent Dinh",
      "Yuyang Wang",
      "Miguel Angel Bautista",
      "David Berthelot",
      "Josh Susskind",
      "Shuangfei Zhai"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20462v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20462v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20344v1",
    "title": "The Curious Case of Analogies: Investigating Analogical Reasoning in Large Language Models",
    "summary": "Analogical reasoning is at the core of human cognition, serving as an important foundation for a variety of intellectual activities. While prior work has shown that LLMs can represent task patterns and surface-level concepts, it remains unclear whether these models can encode high-level relational concepts and apply them to novel situations through structured comparisons. In this work, we explore this fundamental aspect using proportional and story analogies, and identify three key findings. First, LLMs effectively encode the underlying relationships between analogous entities; both attributive and relational information propagate through mid-upper layers in correct cases, whereas reasoning failures reflect missing relational information within these layers. Second, unlike humans, LLMs often struggle not only when relational information is missing, but also when attempting to apply it to new entities. In such cases, strategically patching hidden representations at critical token positions can facilitate information transfer to a certain extent. Lastly, successful analogical reasoning in LLMs is marked by strong structural alignment between analogous situations, whereas failures often reflect degraded or misplaced alignment. Overall, our findings reveal that LLMs exhibit emerging but limited capabilities in encoding and applying high-level relational concepts, highlighting both parallels and gaps with human cognition.",
    "authors": [
      "Taewhoo Lee",
      "Minju Song",
      "Chanwoong Yoon",
      "Jungwoo Park",
      "Jaewoo Kang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20344v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20344v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20253v1",
    "title": "Zoo3D: Zero-Shot 3D Object Detection at Scene Level",
    "summary": "3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .",
    "authors": [
      "Andrey Lemeshko",
      "Bulat Gabdullin",
      "Nikita Drozdov",
      "Anton Konushin",
      "Danila Rukhovich",
      "Maksim Kolodiazhnyi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20253v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20253v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20225v1",
    "title": "DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning",
    "summary": "Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.",
    "authors": [
      "Bo Han",
      "Zhuoming Li",
      "Xiaoyu Wang",
      "Yaxin Hou",
      "Hui Liu",
      "Junhui Hou",
      "Yuheng Jia"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20225v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20225v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20190v1",
    "title": "SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA",
    "summary": "Video text-based visual question answering (Video TextVQA) task aims to answer questions about videos by leveraging the visual text appearing within the videos. This task poses significant challenges, requiring models to accurately perceive and comprehend scene text that varies in scale, orientation, and clarity across frames, while effectively integrating temporal and semantic context to generate precise answers. Moreover, the model must identify question-relevant textual cues and filter out redundant or irrelevant information to ensure answering is guided by the most relevant and informative cues. To address these challenges, we propose SFA, a training-free framework and the first Video-LLM-based method tailored for Video TextVQA, motivated by the human process of answering questions. By adaptively scanning video frames, selectively focusing on key regions, and directly amplifying them, SFA effectively guides the Video-LLM's attention toward essential cues, enabling it to generate more accurate answers. SFA achieves new state-of-the-art results across several public Video TextVQA datasets and surpasses previous methods by a substantial margin, demonstrating its effectiveness and generalizability.",
    "authors": [
      "Haibin He",
      "Qihuang Zhong",
      "Juhua Liu",
      "Bo Du",
      "Peng Wang",
      "Jing Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20190v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20190v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20179v1",
    "title": "Human-computer interactions predict mental health",
    "summary": "Scalable assessments of mental illness, the leading driver of disability worldwide, remain a critical roadblock toward accessible and equitable care. Here, we show that human-computer interactions encode multiple dimensions of self-reported mental health and their changes over time.   We introduce MAILA, a MAchine-learning framework for Inferring Latent mental states from digital Activity. We trained MAILA to predict 1.3 million mental-health self-reports from 20,000 cursor and touchscreen recordings recorded in 9,000 online participants. The dataset includes 2,000 individuals assessed longitudinally, 1,500 diagnosed with depression, and 500 with obsessive-compulsive disorder. MAILA tracks dynamic mental states along three orthogonal dimensions, generalizes across contexts, and achieves near-ceiling accuracy when predicting group-level mental health. The model translates from general to clinical populations, identifies individuals living with mental illness, and captures signatures of psychological function that are not conveyed by language.   Our results demonstrate how everyday human-computer interactions can power passive, reliable, dynamic, and maximally scalable mental health assessments. The ability to decode mental states at zero marginal cost sets new benchmarks for precision medicine and public health, while raising important questions about privacy, agency, and autonomy online.",
    "authors": [
      "Veith Weilnhammer",
      "Jefferson Ortega",
      "David Whitney"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20179v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20179v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.20382v1",
    "title": "MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers",
    "summary": "Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.",
    "authors": [
      "Audrey Pei-Hsuan Chen"
    ],
    "categories": [
      "cs.LG",
      "q-bio.GN"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20382v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20382v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.20196v1",
    "title": "Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning",
    "summary": "Multimodal Large Language Models (MLLMs) achieve remarkable capabilities but can inadvertently memorize privacy-sensitive information. Although existing unlearning methods can remove such knowledge, they fail to achieve benign forgetting because they often degrade the model's general image understanding performance. To address this, we propose the Sculpted Memory Forgetting Adapter (SMFA), which confines forgetting to targeted memory regions while preserving overall capabilities. SMFA first fine-tunes the model to replace sensitive responses with refusals, yielding a memory forgetting adapter, and then applies a retaining anchor-guided masking mechanism to prevent interference with unrelated knowledge and understanding ability. To systematically evaluate selective MLLM unlearning, we introduce S-MLLMUn Bench, the first benchmark designed to jointly assess the removal of sensitive knowledge and retention of general visual understanding. Extensive experiments show that, unlike prior methods, SMFA achieves precise and controllable unlearning while maintaining the model's foundational image understanding.",
    "authors": [
      "Zhen Zeng",
      "Leijiang Gu",
      "Zhangling Duan",
      "Feng Li",
      "Zenglin Shi",
      "Cees G. M. Snoek",
      "Meng Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20196v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20196v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.20182v1",
    "title": "KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP",
    "summary": "Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.",
    "authors": [
      "Adilet Metinov",
      "Gulida M. Kudakeeva",
      "Gulnara D. Kabaeva"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20182v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20182v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.20629v1",
    "title": "MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models",
    "summary": "Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.",
    "authors": [
      "Chieh-Yun Chen",
      "Zhonghao Wang",
      "Qi Chen",
      "Zhifan Ye",
      "Min Shi",
      "Yue Zhao",
      "Yinan Zhao",
      "Hui Qu",
      "Wei-An Lin",
      "Yiru Shen",
      "Ajinkya Kale",
      "Irfan Essa",
      "Humphrey Shi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20629v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20629v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20613v1",
    "title": "Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning",
    "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized AI-assisted code generation. This rapid development of LLMs has outpaced our ability to properly benchmark them. Prevailing benchmarks emphasize unit-test pass rates and syntactic correctness. Such metrics understate the difficulty of many real-world problems that require planning, optimization, and strategic interaction. We introduce a multi-agent reasoning-driven benchmark based on a real-world logistics optimization problem (Auction, Pickup, and Delivery Problem) that couples competitive auctions with capacity-constrained routing. The benchmark requires building agents that can (i) bid strategically under uncertainty and (ii) optimize planners that deliver tasks while maximizing profit. We evaluate 40 LLM-coded agents (by a wide range of state-of-the-art LLMs under multiple prompting methodologies, including vibe coding) against 17 human-coded agents developed before the advent of LLMs. Our results over 12 double all-play-all tournaments and $\\sim 40$k matches demonstrate (i) a clear superiority of human(graduate students)-coded agents: the top 5 spots are consistently won by human-coded agents, (ii) the majority of LLM-coded agents (33 out of 40) are beaten by very simple baselines, and (iii) given the best human solution as an input and prompted to improve upon, the best performing LLM makes the solution significantly worse instead of improving it. Our results highlight a gap in LLMs' ability to produce code that works competitively in the real-world, and motivate new evaluations that emphasize reasoning-driven code synthesis in real-world scenarios.",
    "authors": [
      "Panayiotis Danassis",
      "Naman Goel"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20613v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20613v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20547v1",
    "title": "From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding",
    "summary": "Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.",
    "authors": [
      "Farjana Sultana Mim",
      "Shuchin Aeron",
      "Eric Miller",
      "Kristen Wendell"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20547v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20547v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20509v1",
    "title": "DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning",
    "summary": "Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\\mathcal{O}(1/\\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.",
    "authors": [
      "Mihaela Hudişteanu",
      "Edwige Cyffers",
      "Nikita P. Kalinin"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20509v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20509v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20459v1",
    "title": "Generation, Evaluation, and Explanation of Novelists' Styles with Single-Token Prompts",
    "summary": "Recent advances in large language models have created new opportunities for stylometry, the study of writing styles and authorship. Two challenges, however, remain central: training generative models when no paired data exist, and evaluating stylistic text without relying only on human judgment. In this work, we present a framework for both generating and evaluating sentences in the style of 19th-century novelists. Large language models are fine-tuned with minimal, single-token prompts to produce text in the voices of authors such as Dickens, Austen, Twain, Alcott, and Melville. To assess these generative models, we employ a transformer-based detector trained on authentic sentences, using it both as a classifier and as a tool for stylistic explanation. We complement this with syntactic comparisons and explainable AI methods, including attention-based and gradient-based analyses, to identify the linguistic cues that drive stylistic imitation. Our findings show that the generated text reflects the authors' distinctive patterns and that AI-based evaluation offers a reliable alternative to human assessment. All artifacts of this work are published online.",
    "authors": [
      "Mosab Rezaei",
      "Mina Rajaei Moghadam",
      "Abdul Rahman Shaikh",
      "Hamed Alhoori",
      "Reva Freedman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20459v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20459v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20250v1",
    "title": "Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation",
    "summary": "Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.",
    "authors": [
      "Daniel Kienzle",
      "Katja Ludwig",
      "Julian Lorenz",
      "Shin'ichi Satoh",
      "Rainer Lienhart"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20250v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20250v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20220v1",
    "title": "Communication-Efficient Learning for Satellite Constellations",
    "summary": "Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.",
    "authors": [
      "Ruxandra-Stefania Tudose",
      "Moritz H. W. Grüss",
      "Grace Ra Kim",
      "Karl H. Johansson",
      "Nicola Bastianello"
    ],
    "categories": [
      "cs.LG",
      "eess.SY",
      "math.OC"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20220v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20220v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20200v1",
    "title": "Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025",
    "summary": "This report presents the solution and results of our team MSRA\\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution",
    "authors": [
      "Yitian Huang",
      "Yuxuan Lei",
      "Jianxun Lian",
      "Hao Liao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20200v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20200v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.20636v1",
    "title": "Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model",
    "summary": "Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.",
    "authors": [
      "Ziyue Wang",
      "Yayati Jadhav",
      "Peter Pak",
      "Amir Barati Farimani"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20636v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20636v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20592v1",
    "title": "Latent Diffusion Inversion Requires Understanding the Latent Space",
    "summary": "The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\\% and substantial increases in TPR@1\\%FPR (6.42\\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.",
    "authors": [
      "Mingxing Rao",
      "Bowen Qu",
      "Daniel Moyer"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20592v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20592v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20561v1",
    "title": "Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward",
    "summary": "Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox",
    "authors": [
      "Yuwei Niu",
      "Weiyang Jin",
      "Jiaqi Liao",
      "Chaoran Feng",
      "Peng Jin",
      "Bin Lin",
      "Zongjian Li",
      "Bin Zhu",
      "Weihao Yu",
      "Li Yuan"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20561v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20561v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20531v1",
    "title": "Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models",
    "summary": "Visual Language Models (VLMs) are powerful generative tools but often produce factually inaccurate outputs due to a lack of robust reasoning capabilities. While extensive research has been conducted on integrating external knowledge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seamlessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leveraging structured knowledge graphs for multi-hop verification using image-captioning task to illustrate our framework. Our approach enables systematic reasoning across multiple steps, including visual entity recognition, knowledge graph traversal, and fact-based caption refinement. We evaluate the framework using hierarchical, triple-based and bullet-point based knowledge representations, analyzing their effectiveness in factual accuracy and logical inference. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions revealing key insights into reasoning patterns and failure modes. This work demonstrates the potential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.",
    "authors": [
      "Shamima Hossain"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20531v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20531v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20507v1",
    "title": "The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models",
    "summary": "Large language models (LLMs) have emerged as a candidate \"model organism\" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.",
    "authors": [
      "Nathan Roll",
      "Jill Kries",
      "Flora Jin",
      "Catherine Wang",
      "Ann Marie Finley",
      "Meghan Sumner",
      "Cory Shain",
      "Laura Gwilliams"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20507v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20507v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20362v1",
    "title": "PRISM: Periodic Representation with multIscale and Similarity graph Modelling for enhanced crystal structure property prediction",
    "summary": "Crystal structures are characterised by repeating atomic patterns within unit cells across three-dimensional space, posing unique challenges for graph-based representation learning. Current methods often overlook essential periodic boundary conditions and multiscale interactions inherent to crystalline structures. In this paper, we introduce PRISM, a graph neural network framework that explicitly integrates multiscale representations and periodic feature encoding by employing a set of expert modules, each specialised in encoding distinct structural and chemical aspects of periodic systems. Extensive experiments across crystal structure-based benchmarks demonstrate that PRISM improves state-of-the-art predictive accuracy, significantly enhancing crystal property prediction.",
    "authors": [
      "Àlex Solé",
      "Albert Mosella-Montoro",
      "Joan Cardona",
      "Daniel Aravena",
      "Silvia Gómez-Coca",
      "Eliseo Ruiz",
      "Javier Ruiz-Hidalgo"
    ],
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20362v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20362v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20302v1",
    "title": "CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation",
    "summary": "In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.",
    "authors": [
      "Shilei Cao",
      "Ziyang Gong",
      "Hehai Lin",
      "Yang Liu",
      "Jiashun Cheng",
      "Xiaoxing Hu",
      "Haoyuan Liang",
      "Guowen Li",
      "Chengwei Qin",
      "Hong Cheng",
      "Xue Yang",
      "Juepeng Zheng",
      "Haohuan Fu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20302v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20302v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20223v1",
    "title": "V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs",
    "summary": "Adversarial attacks have evolved from simply disrupting predictions on conventional task-specific models to the more complex goal of manipulating image semantics on Large Vision-Language Models (LVLMs). However, existing methods struggle with controllability and fail to precisely manipulate the semantics of specific concepts in the image. We attribute this limitation to semantic entanglement in the patch-token representations on which adversarial attacks typically operate: global context aggregated by self-attention in the vision encoder dominates individual patch features, making them unreliable handles for precise local semantic manipulation. Our systematic investigation reveals a key insight: value features (V) computed within the transformer attention block serve as much more precise handles for manipulation. We show that V suppresses global-context channels, allowing it to retain high-entropy, disentangled local semantic information. Building on this discovery, we propose V-Attack, a novel method designed for precise local semantic attacks. V-Attack targets the value features and introduces two core components: (1) a Self-Value Enhancement module to refine V's intrinsic semantic richness, and (2) a Text-Guided Value Manipulation module that leverages text prompts to locate source concept and optimize it toward a target concept. By bypassing the entangled patch features, V-Attack achieves highly effective semantic control. Extensive experiments across diverse LVLMs, including LLaVA, InternVL, DeepseekVL and GPT-4o, show that V-Attack improves the attack success rate by an average of 36% over state-of-the-art methods, exposing critical vulnerabilities in modern visual-language understanding. Our code and data are available https://github.com/Summu77/V-Attack.",
    "authors": [
      "Sen Nie",
      "Jie Zhang",
      "Jianxin Yan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20223v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20223v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20154v1",
    "title": "Alzheimers Disease Progression Prediction Based on Manifold Mapping of Irregularly Sampled Longitudinal Data",
    "summary": "The uncertainty of clinical examinations frequently leads to irregular observation intervals in longitudinal imaging data, posing challenges for modeling disease progression.Most existing imaging-based disease prediction models operate in Euclidean space, which assumes a flat representation of data and fails to fully capture the intrinsic continuity and nonlinear geometric structure of irregularly sampled longitudinal images. To address the challenge of modeling Alzheimers disease (AD) progression from irregularly sampled longitudinal structural Magnetic Resonance Imaging (sMRI) data, we propose a Riemannian manifold mapping, a Time-aware manifold Neural ordinary differential equation, and an Attention-based riemannian Gated recurrent unit (R-TNAG) framework. Our approach first projects features extracted from high-dimensional sMRI into a manifold space to preserve the intrinsic geometry of disease progression. On this representation, a time-aware Neural Ordinary Differential Equation (TNODE) models the continuous evolution of latent states between observations, while an Attention-based Riemannian Gated Recurrent Unit (ARGRU) adaptively integrates historical and current information to handle irregular intervals. This joint design improves temporal consistency and yields robust AD trajectory prediction under irregular sampling.Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art models in both disease status prediction and cognitive score regression. Ablation studies verify the contributions of each module, highlighting their complementary roles in enhancing predictive accuracy. Moreover, the model exhibits stable performance across varying sequence lengths and missing data rates, indicating strong temporal generalizability. Cross-dataset validation further confirms its robustness and applicability in diverse clinical settings.",
    "authors": [
      "Xin Hong",
      "Ying Shi",
      "Yinhao Li",
      "Yen-Wei Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20154v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20154v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.20647v1",
    "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization",
    "summary": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.",
    "authors": [
      "Tahira Kazimi",
      "Connor Dunlop",
      "Pinar Yanardag"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20647v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20647v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.20500v1",
    "title": "From One Attack Domain to Another: Contrastive Transfer Learning with Siamese Networks for APT Detection",
    "summary": "Advanced Persistent Threats (APT) pose a major cybersecurity challenge due to their stealth, persistence, and adaptability. Traditional machine learning detectors struggle with class imbalance, high dimensional features, and scarce real world traces. They often lack transferability-performing well in the training domain but degrading in novel attack scenarios. We propose a hybrid transfer framework that integrates Transfer Learning, Explainable AI (XAI), contrastive learning, and Siamese networks to improve cross-domain generalization. An attention-based autoencoder supports knowledge transfer across domains, while Shapley Additive exPlanations (SHAP) select stable, informative features to reduce dimensionality and computational cost. A Siamese encoder trained with a contrastive objective aligns source and target representations, increasing anomaly separability and mitigating feature drift. We evaluate on real-world traces from the DARPA Transparent Computing (TC) program and augment with synthetic attack scenarios to test robustness. Across source to target transfers, the approach delivers improved detection scores with classical and deep baselines, demonstrating a scalable, explainable, and transferable solution for APT detection.",
    "authors": [
      "Sidahmed Benabderrahmane",
      "Talal Rahwan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20500v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20500v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.20295v1",
    "title": "Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations",
    "summary": "Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions. They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers. State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers. The generation of CFEs for video classifiers remains largely underexplored. For the counterfactual videos to be useful, they have to be physically plausible, temporally coherent, and exhibit smooth motion trajectories. Existing CFE image-based methods, designed to explain image classifiers, lack the capacity to generate temporally coherent, smooth and physically plausible video CFEs. To address this, we propose Back To The Feature (BTTF), an optimization framework that generates video CFEs. Our method introduces two novel features, 1) an optimization scheme to retrieve the initial latent noise conditioned by the first frame of the input video, 2) a two-stage optimization strategy to enable the search for counterfactual videos in the vicinity of the input video. Both optimization processes are guided solely by the target classifier, ensuring the explanation is faithful. To accelerate convergence, we also introduce a progressive optimization strategy that incrementally increases the number of denoising steps. Extensive experiments on video datasets such as Shape-Moving (motion classification), MEAD (emotion classification), and NTU RGB+D (action classification) show that our BTTF effectively generates valid, visually similar and realistic counterfactual videos that provide concrete insights into the classifier's decision-making mechanism.",
    "authors": [
      "Chao Wang",
      "Chengan Che",
      "Xinyue Chen",
      "Sophia Tsoka",
      "Luis C. Garcia-Peraza-Herrera"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20295v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20295v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.20224v1",
    "title": "DUO-TOK: Dual-Track Semantic Music Tokenizer for Vocal-Accompaniment Generation",
    "summary": "Duo-Tok is a source-aware dual-codebook tokenizer for vocal-accompaniment music that targets the growing tension between reconstruction quality and language-model (LM) learnability in modern lyrics-to-song systems. Existing codecs either prioritize high-fidelity reconstruction with difficult-to-model acoustic tokens or compress aggressively into semantic tokens that are LM-friendly but lossy, and they rarely make the tokenizer itself aware of dual-track structure. Duo-Tok follows a four-stage, SSL-centered pipeline: we first pretrain a BEST-RQ-style encoder on large-scale audio, then stabilize and factorize the representation with Gaussian replacement noise and multi-task supervision, before freezing the encoder to learn SimVQ-based dual codebooks with hard routing for vocals and accompaniment, and finally training latent diffusion decoders on top of the discrete tokens. Duo-Tok at 0.75 kbps shifts the empirical reconstruction-generation Pareto frontier, achieving the best music-tagging AP and the lowest vocabulary-normalized LM perplexity among compared codecs while maintaining reconstruction quality comparable to state-of-the-art music tokenizers.",
    "authors": [
      "Rui Lin",
      "Zhiyue Wu",
      "Jiahe Le",
      "Kangdi Wang",
      "Weixiong Chen",
      "Junyu Dai",
      "Tao Jiang"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20224v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20224v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.20194v1",
    "title": "In-Context Compositional Learning via Sparse Coding Transformer",
    "summary": "Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.",
    "authors": [
      "Wei Chen",
      "Jingxi Yu",
      "Zichen Miao",
      "Qiang Qiu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20194v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20194v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.20604v1",
    "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
    "summary": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.",
    "authors": [
      "Yixin Liu",
      "Pengfei Liu",
      "Arman Cohan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20604v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20604v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20456v1",
    "title": "Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks",
    "summary": "Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.   This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.",
    "authors": [
      "Shreevanth Krishnaa Gopalakrishnan",
      "Stephen Hailes"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20456v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20456v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20403v1",
    "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework",
    "summary": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.",
    "authors": [
      "Andrea Lops",
      "Fedelucio Narducci",
      "Azzurra Ragone",
      "Michelantonio Trizio",
      "Claudio Barto"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20403v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20403v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20349v1",
    "title": "Complexity Reduction Study Based on RD Costs Approximation for VVC Intra Partitioning",
    "summary": "In this paper, a complexity study is conducted for Versatile Video Codec (VVC) intra partitioning to accelerate the exhaustive search involved in Rate-Distortion Optimization (RDO) process. To address this problem, two main machine learning techniques are proposed and compared. Unlike existing methods, the proposed approaches are size independent and incorporate the Rate-Distortion (RD) costs of neighboring blocks as input features. The first method is a regression based technique that predicts normalized RD costs of a given Coding Unit (CU). As partitioning possesses the Markov property, the associated decision-making problem can be modeled as a Markov Decision Process (MDP) and solved by Reinforcement Learning (RL). The second approach is a RL agent learned from trajectories of CU decision across two depths with Deep Q-Network (DQN) algorithm. Then a pre-determined thresholds are applied for both methods to select a suitable split for the current CU.",
    "authors": [
      "M. E. A. Kherchouche",
      "F. Galpin",
      "T. Dumas",
      "F. Schnitzler",
      "D. Menard",
      "L. Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20349v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20349v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20335v1",
    "title": "ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation",
    "summary": "Estimating homography from a single image remains a challenging yet practically valuable task, particularly in domains like retail, where only one viewpoint is typically available for shelf monitoring and product alignment. In this paper, we present a deep learning framework that predicts a 4-point parameterized homography matrix to rectify shelf images captured from arbitrary angles. Our model leverages a ConvNeXt-based backbone for enhanced feature representation and adopts normalized coordinate regression for improved stability. To address data scarcity and promote generalization, we introduce a novel augmentation strategy by modeling and sampling synthetic homographies. Our method achieves a mean corner error of 1.298 pixels on the test set. When compared with both classical computer vision and deep learning-based approaches, our method demonstrates competitive performance in both accuracy and inference speed. Together, these results establish our approach as a robust and efficient solution for realworld single-view rectification. To encourage further research in this domain, we will make our dataset, ShelfRectSet, and code publicly available",
    "authors": [
      "Onur Berk Tore",
      "Ibrahim Samil Yalciner",
      "Server Calap"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20335v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20335v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20312v1",
    "title": "Data Augmentation Techniques to Reverse-Engineer Neural Network Weights from Input-Output Queries",
    "summary": "Network weights can be reverse-engineered given enough informative samples of a network's input-output function. In a teacher-student setup, this translates into collecting a dataset of the teacher mapping -- querying the teacher -- and fitting a student to imitate such mapping. A sensible choice of queries is the dataset the teacher is trained on. But current methods fail when the teacher parameters are more numerous than the training data, because the student overfits to the queries instead of aligning its parameters to the teacher. In this work, we explore augmentation techniques to best sample the input-output mapping of a teacher network, with the goal of eliciting a rich set of representations from the teacher hidden layers. We discover that standard augmentations such as rotation, flipping, and adding noise, bring little to no improvement to the identification problem. We design new data augmentation techniques tailored to better sample the representational space of the network's hidden layers. With our augmentations we extend the state-of-the-art range of recoverable network sizes. To test their scalability, we show that we can recover networks of up to 100 times more parameters than training data-points.",
    "authors": [
      "Alexander Beiser",
      "Flavio Martinelli",
      "Wulfram Gerstner",
      "Johanni Brea"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20312v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20312v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20305v1",
    "title": "RIS-Assisted Downlink Pinching-Antenna Systems: GNN-Enabled Optimization Approaches",
    "summary": "This paper investigates a reconfigurable intelligent surface (RIS)-assisted multi-waveguide pinching-antenna (PA) system (PASS) for multi-user downlink information transmission, motivated by the unknown impact of the integration of emerging PASS and RIS on wireless communications. First, we formulate sum rate (SR) and energy efficiency (EE) maximization problems in a unified framework, subject to constraints on the movable region of PAs, total power budget, and tunable phase of RIS elements. Then, by leveraging a graph-structured topology of the RIS-assisted PASS, a novel three-stage graph neural network (GNN) is proposed, which learns PA positions based on user locations, and RIS phase shifts according to composite channel conditions at the first two stages, respectively, and finally determines beamforming vectors. Specifically, the proposed GNN is achieved through unsupervised training, together with three implementation strategies for its integration with convex optimization, thus offering trade-offs between inference time and solution optimality. Extensive numerical results are provided to validate the effectiveness of the proposed GNN, and to support its unique attributes of viable generalization capability, good performance reliability, and real-time applicability. Moreover, the impact of key parameters on RIS-assisted PASS is illustrated and analyzed.",
    "authors": [
      "Changpeng He",
      "Yang Lu",
      "Yanqing Xu",
      "Chong-Yung Chi",
      "Bo Ai",
      "Arumugam Nallanathan"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20305v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20305v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20279v1",
    "title": "SelfMOTR: Revisiting MOTR with Self-Generating Detection Priors",
    "summary": "Despite progress toward end-to-end tracking with transformer architectures, poor detection performance and the conflict between detection and association in a joint architecture remain critical concerns. Recent approaches aim to mitigate these issues by (i) employing advanced denoising or label assignment strategies, or (ii) incorporating detection priors from external object detectors via distillation or anchor proposal techniques. Inspired by the success of integrating detection priors and by the key insight that MOTR-like models are secretly strong detection models, we introduce SelfMOTR, a novel tracking transformer that relies on self-generated detection priors. Through extensive analysis and ablation studies, we uncover and demonstrate the hidden detection capabilities of MOTR-like models, and present a practical set of tools for leveraging them effectively. On DanceTrack, SelfMOTR achieves strong performance, competing with recent state-of-the-art end-to-end tracking methods.",
    "authors": [
      "Fabian Gülhan",
      "Emil Mededovic",
      "Yuli Wu",
      "Johannes Stegmaier"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20279v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20279v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20237v1",
    "title": "Quantum-Enhanced Reinforcement Learning for Accelerating Newton-Raphson Convergence with Ising Machines: A Case Study for Power Flow Analysis",
    "summary": "The Newton-Raphson (NR) method is widely used for solving power flow (PF) equations due to its quadratic convergence. However, its performance deteriorates under poor initialization or extreme operating scenarios, e.g., high levels of renewable energy penetration. Traditional NR initialization strategies often fail to address these challenges, resulting in slow convergence or even divergence. We propose the use of reinforcement learning (RL) to optimize the initialization of NR, and introduce a novel quantum-enhanced RL environment update mechanism to mitigate the significant computational cost of evaluating power system states over a combinatorially large action space at each RL timestep by formulating the voltage adjustment task as a quadratic unconstrained binary optimization problem. Specifically, quantum/digital annealers are integrated into the RL environment update to evaluate state transitions using a problem Hamiltonian designed for PF. Results demonstrate significant improvements in convergence speed, a reduction in NR iteration counts, and enhanced robustness under different operating conditions.",
    "authors": [
      "Zeynab Kaseb",
      "Matthias Moller",
      "Lindsay Spoor",
      "Jerry J. Guo",
      "Yu Xiang",
      "Peter Palensky",
      "Pedro P. Vergara"
    ],
    "categories": [
      "eess.SY",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20237v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20237v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.20469v1",
    "title": "Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features",
    "summary": "Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories. Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns. This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos. We propose temporal-spatial descriptors inspired by Laban Movement Analysis. These features capture local joint dynamics such as velocity, acceleration, and angular movement of the upper body, enabling a structured representation of spatial coordination. To further encode rhythmic and periodic aspects of movement, we integrate Fast Fourier Transform features that characterize movement patterns in the frequency domain. The proposed approach achieves robust classification of different dance styles with low computational effort, as complex model architectures are not required, and shows that interpretable motion representations can effectively capture stylistic nuances.",
    "authors": [
      "Ben Hamscher",
      "Arnold Brosch",
      "Nicolas Binninger",
      "Maksymilian Jan Dejna",
      "Kira Maag"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20469v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20469v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20415v1",
    "title": "MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts",
    "summary": "Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.",
    "authors": [
      "Zilong Huang",
      "Jun He",
      "Xiaobin Huang",
      "Ziyi Xiong",
      "Yang Luo",
      "Junyan Ye",
      "Weijia Li",
      "Yiping Chen",
      "Ting Han"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20415v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20415v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20347v1",
    "title": "Soft Adaptive Policy Optimization",
    "summary": "Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.",
    "authors": [
      "Chang Gao",
      "Chujie Zheng",
      "Xiong-Hui Chen",
      "Kai Dang",
      "Shixuan Liu",
      "Bowen Yu",
      "An Yang",
      "Shuai Bai",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20347v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20347v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20273v1",
    "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
    "summary": "Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.",
    "authors": [
      "Areeb Ahmad",
      "Abhinav Joshi",
      "Ashutosh Modi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20273v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20273v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20189v1",
    "title": "Learning Subgroups with Maximum Treatment Effects without Causal Heuristics",
    "summary": "Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.",
    "authors": [
      "Lincen Yang",
      "Zhong Li",
      "Matthijs van Leeuwen",
      "Saber Salehkaleybar"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20189v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20189v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20175v1",
    "title": "Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware",
    "summary": "Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.",
    "authors": [
      "Federico Paredes-Valles",
      "Yoshitaka Miyatani",
      "Kirk Y. W. Scheper"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20175v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20175v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20120v1",
    "title": "\"When Data is Scarce, Prompt Smarter\"... Approaches to Grammatical Error Correction in Low-Resource Settings",
    "summary": "Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.",
    "authors": [
      "Somsubhra De",
      "Harsh Kumar",
      "Arun Prakash A"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20120v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20120v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.20640v1",
    "title": "MotionV2V: Editing Motion in a Video",
    "summary": "While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a \"motion edit\" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating \"motion counterfactuals\", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V",
    "authors": [
      "Ryan Burgert",
      "Charles Herrmann",
      "Forrester Cole",
      "Michael S Ryoo",
      "Neal Wadhwa",
      "Andrey Voynov",
      "Nataniel Ruiz"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20640v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20640v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20597v1",
    "title": "BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents",
    "summary": "The integration of artificial intelligence (AI) agents into web browsers introduces security challenges that go beyond traditional web application threat models. Prior work has identified prompt injection as a new attack vector for web agents, yet the resulting impact within real-world environments remains insufficiently understood.   In this work, we examine the landscape of prompt injection attacks and synthesize a benchmark of attacks embedded in realistic HTML payloads. Our benchmark goes beyond prior work by emphasizing injections that can influence real-world actions rather than mere text outputs, and by presenting attack payloads with complexity and distractor frequency similar to what real-world agents encounter. We leverage this benchmark to conduct a comprehensive empirical evaluation of existing defenses, assessing their effectiveness across a suite of frontier AI models. We propose a multi-layered defense strategy comprising both architectural and model-based defenses to protect against evolving prompt injection attacks. Our work offers a blueprint for designing practical, secure web agents through a defense-in-depth approach.",
    "authors": [
      "Kaiyuan Zhang",
      "Mark Tenenholtz",
      "Kyle Polley",
      "Jerry Ma",
      "Denis Yarats",
      "Ninghui Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20597v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20597v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20558v1",
    "title": "Spatio-Temporal Hierarchical Causal Models",
    "summary": "The abundance of fine-grained spatio-temporal data, such as traffic sensor networks, offers vast opportunities for scientific discovery. However, inferring causal relationships from such observational data remains challenging, particularly due to unobserved confounders that are specific to units (e.g., geographical locations) yet influence outcomes over time. Most existing methods for spatio-temporal causal inference assume that all confounders are observed, an assumption that is often violated in practice. In this paper, we introduce Spatio-Temporal Hierarchical Causal Models (ST-HCMs), a novel graphical framework that extends hierarchical causal modeling to the spatio-temporal domain. At the core of our approach is the Spatio-Temporal Collapse Theorem, which shows that a complex ST-HCM converges to a simpler flat causal model as the amount of subunit data increases. This theoretical result enables a general procedure for causal identification, allowing ST-HCMs to recover causal effects even in the presence of unobserved, time-invariant unit-level confounders, a scenario where standard non-hierarchical models fail. We validate the effectiveness of our framework on both synthetic and real-world datasets, demonstrating its potential for robust causal inference in complex dynamic systems.",
    "authors": [
      "Xintong Li",
      "Haoran Zhang",
      "Xiao Zhou"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20558v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20558v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20510v1",
    "title": "FRAGMENTA: End-to-end Fragmentation-based Generative Model with Agentic Tuning for Drug Lead Optimization",
    "summary": "Molecule generation using generative AI is vital for drug discovery, yet class-specific datasets often contain fewer than 100 training examples. While fragment-based models handle limited data better than atom-based approaches, existing heuristic fragmentation limits diversity and misses key fragments. Additionally, model tuning typically requires slow, indirect collaboration between medicinal chemists and AI engineers. We introduce FRAGMENTA, an end-to-end framework for drug lead optimization comprising: 1) a novel generative model that reframes fragmentation as a \"vocabulary selection\" problem, using dynamic Q-learning to jointly optimize fragmentation and generation; and 2) an agentic AI system that refines objectives via conversational feedback from domain experts. This system removes the AI engineer from the loop and progressively learns domain knowledge to eventually automate tuning. In real-world cancer drug discovery experiments, FRAGMENTA's Human-Agent configuration identified nearly twice as many high-scoring molecules as baselines. Furthermore, the fully autonomous Agent-Agent system outperformed traditional Human-Human tuning, demonstrating the efficacy of agentic tuning in capturing expert intent.",
    "authors": [
      "Yuto Suzuki",
      "Paul Awolade",
      "Daniel V. LaBarbera",
      "Farnoush Banaei-Kashani"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20510v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20510v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20494v1",
    "title": "Adversarial Confusion Attack: Disrupting Multimodal Large Language Models",
    "summary": "We introduce the Adversarial Confusion Attack, a new class of threats against multimodal large language models (MLLMs). Unlike jailbreaks or targeted misclassification, the goal is to induce systematic disruption that makes the model generate incoherent or confidently incorrect outputs. Applications include embedding adversarial images into websites to prevent MLLM-powered agents from operating reliably. The proposed attack maximizes next-token entropy using a small ensemble of open-source MLLMs. In the white-box setting, we show that a single adversarial image can disrupt all models in the ensemble, both in the full-image and adversarial CAPTCHA settings. Despite relying on a basic adversarial technique (PGD), the attack generates perturbations that transfer to both unseen open-source (e.g., Qwen3-VL) and proprietary (e.g., GPT-5.1) models.",
    "authors": [
      "Jakub Hoscilowicz",
      "Artur Janicki"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20494v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20494v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20390v1",
    "title": "FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers",
    "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art generation quality but require long sequential denoising trajectories, leading to high inference latency. Recent speculative inference methods enable lossless parallel sampling in U-Net-based diffusion models via a drafter-verifier scheme, but their acceleration is limited on DiTs due to insufficient draft accuracy during verification. To address this limitation, we analyze the DiTs' feature dynamics and find the features of the final transformer layer (top-block) exhibit strong temporal consistency and rich semantic abstraction. Based on this insight, we propose FREE, a novel framework that employs a lightweight drafter to perform feature-level autoregression with parallel verification, guaranteeing lossless acceleration with theoretical and empirical support. Meanwhile, prediction variance (uncertainty) of DiTs naturally increases in later denoising steps, reducing acceptance rates under speculative sampling. To mitigate this effect, we further introduce an uncertainty-guided relaxation strategy, forming FREE (relax), which dynamically adjusts the acceptance probability in response to uncertainty levels. Experiments on ImageNet-$512^2$ show that FREE achieves up to $1.86 \\times$ acceleration, and FREE (relax) further reaches $2.25 \\times$ speedup while maintaining high perceptual and quantitative fidelity in generation quality.",
    "authors": [
      "Xinwan Wen",
      "Bowen Li",
      "Jiajun Luo",
      "Ye Li",
      "Zhi Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20390v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20390v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20343v1",
    "title": "AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend",
    "summary": "We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.",
    "authors": [
      "Hengyi Wang",
      "Lourdes Agapito"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20343v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20343v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20330v1",
    "title": "ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation",
    "summary": "Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.",
    "authors": [
      "Yuhan Wu",
      "Tiantian Wei",
      "Shuo Wang",
      "ZhiChao Wang",
      "Yanyong Zhang",
      "Daniel Cremers",
      "Yan Xia"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20330v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20330v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20296v1",
    "title": "Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction",
    "summary": "Despite significant advancements in deep learning-based sparse-view computed tomography (SVCT) reconstruction algorithms, these methods still encounter two primary limitations: (i) It is challenging to explicitly prove that the prior networks of deep unfolding algorithms satisfy Lipschitz constraints due to their empirically designed nature. (ii) The substantial storage costs of training a separate model for each setting in the case of multiple views hinder practical clinical applications. To address these issues, we elaborate an explicitly provable Lipschitz-constrained network, dubbed LipNet, and integrate an explicit prompt module to provide discriminative knowledge of different sparse sampling settings, enabling the treatment of multiple sparse view configurations within a single model. Furthermore, we develop a storage-saving deep unfolding framework for multiple-in-one SVCT reconstruction, termed PromptCT, which embeds LipNet as its prior network to ensure the convergence of its corresponding iterative algorithm. In simulated and real data experiments, PromptCT outperforms benchmark reconstruction algorithms in multiple-in-one SVCT reconstruction, achieving higher-quality reconstructions with lower storage costs. On the theoretical side, we explicitly demonstrate that LipNet satisfies boundary property, further proving its Lipschitz continuity and subsequently analyzing the convergence of the proposed iterative algorithms. The data and code are publicly available at https://github.com/shibaoshun/PromptCT.",
    "authors": [
      "Baoshun Shi",
      "Ke Jiang",
      "Qiusheng Lian",
      "Xinran Yu",
      "Huazhu Fu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20296v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20296v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20258v1",
    "title": "Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization",
    "summary": "Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.",
    "authors": [
      "Xiaohan Wang",
      "Zhangtao Cheng",
      "Ting Zhong",
      "Leiting Chen",
      "Fan Zhou"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20258v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20258v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20216v1",
    "title": "CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents",
    "summary": "Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \\emph{CostNav}, a \\textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \\textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\\% SLA compliance but is \\emph{not} commercially viable: yielding a loss of \\$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.",
    "authors": [
      "Haebin Seong",
      "Sungmin Kim",
      "Minchan Kim",
      "Yongjun Cho",
      "Myunchul Joe",
      "Suhwan Choi",
      "Jaeyoon Jung",
      "Jiyong Youn",
      "Yoonshik Kim",
      "Samwoo Seong",
      "Yubeen Park",
      "Youngjae Yu",
      "Yunsung Lee"
    ],
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20216v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20216v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20169v1",
    "title": "ADNet: A Large-Scale and Extensible Multi-Domain Benchmark for Anomaly Detection Across 380 Real-World Categories",
    "summary": "Anomaly detection (AD) aims to identify defects using normal-only training data. Existing anomaly detection benchmarks (e.g., MVTec-AD with 15 categories) cover only a narrow range of categories, limiting the evaluation of cross-context generalization and scalability. We introduce ADNet, a large-scale, multi-domain benchmark comprising 380 categories aggregated from 49 publicly available datasets across Electronics, Industry, Agrifood, Infrastructure, and Medical domains. The benchmark includes a total of 196,294 RGB images, consisting of 116,192 normal samples for training and 80,102 test images, of which 60,311 are anomalous. All images are standardized with MVTec-style pixel-level annotations and structured text descriptions spanning both spatial and visual attributes, enabling multimodal anomaly detection tasks. Extensive experiments reveal a clear scalability challenge: existing state-of-the-art methods achieve 90.6% I-AUROC in one-for-one settings but drop to 78.5% when scaling to all 380 categories in a multi-class setting. To address this, we propose Dinomaly-m, a context-guided Mixture-of-Experts extension of Dinomaly that expands decoder capacity without increasing inference cost. It achieves 83.2% I-AUROC and 93.1% P-AUROC, demonstrating superior performance over existing approaches. ADNet is designed as a standardized and extensible benchmark, supporting the community in expanding anomaly detection datasets across diverse domains and providing a scalable foundation for future anomaly detection foundation models. Dataset: https://grainnet.github.io/ADNet",
    "authors": [
      "Hai Ling",
      "Jia Guo",
      "Zhulin Tao",
      "Yunkang Cao",
      "Donglin Di",
      "Hongyan Xu",
      "Xiu Su",
      "Yang Song",
      "Lei Fan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20169v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20169v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.20644v1",
    "title": "Vision-Language Memory for Spatial Reasoning",
    "summary": "Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.",
    "authors": [
      "Zuntao Liu",
      "Yi Du",
      "Taimeng Fu",
      "Shaoshu Su",
      "Cherie Ho",
      "Chen Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20644v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20644v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20612v1",
    "title": "Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition",
    "summary": "Many consequential real-world systems, like wind fields and ocean currents, are dynamic and hard to model. Learning their governing dynamics remains a central challenge in scientific machine learning. Dynamic Mode Decomposition (DMD) provides a simple, data-driven approximation, but practical use is limited by sparse/noisy observations from continuous fields, reliance on linear approximations, and the lack of principled uncertainty quantification. To address these issues, we introduce Stochastic NODE-DMD, a probabilistic extension of DMD that models continuous-time, nonlinear dynamics while remaining interpretable. Our approach enables continuous spatiotemporal reconstruction at arbitrary coordinates and quantifies predictive uncertainty. Across four benchmarks, a synthetic setting and three physics-based flows, it surpasses a baseline in reconstruction accuracy when trained from only 10% observation density. It further recovers the dynamical structure by aligning learned modes and continuous-time eigenvalues with ground truth. Finally, on datasets with multiple realizations, our method learns a calibrated distribution over latent dynamics that preserves ensemble variability rather than averaging across regimes. Our code is available at: https://github.com/sedan-group/Stochastic-NODE-DMD",
    "authors": [
      "Yujin Kim",
      "Sarah Dean"
    ],
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20612v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20612v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20562v1",
    "title": "PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding",
    "summary": "While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.",
    "authors": [
      "Haoze Zhang",
      "Tianyu Huang",
      "Zichen Wan",
      "Xiaowei Jin",
      "Hongzhi Zhang",
      "Hui Li",
      "Wangmeng Zuo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20562v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20562v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20513v1",
    "title": "DesignPref: Capturing Personal Preferences in Visual Design Generation",
    "summary": "Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.",
    "authors": [
      "Yi-Hao Peng",
      "Jeffrey P. Bigham",
      "Jason Wu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20513v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20513v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20478v1",
    "title": "NVIDIA Nemotron Parse 1.1",
    "summary": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.",
    "authors": [
      "Kateryna Chumachenko",
      "Amala Sanjay Deshmukh",
      "Jarno Seppanen",
      "Ilia Karmanov",
      "Chia-Chih Chen",
      "Lukas Voegtle",
      "Philipp Fischer",
      "Marek Wawrzos",
      "Saeid Motiian",
      "Roman Ageev",
      "Kedi Wu",
      "Alexandre Milesi",
      "Maryam Moosaei",
      "Krzysztof Pawelec",
      "Padmavathy Subramanian",
      "Mehrzad Samadi",
      "Xin Yu",
      "Celina Dear",
      "Sarah Stoddard",
      "Jenna Diamond",
      "Jesse Oliver",
      "Leanna Chraghchian",
      "Patrick Skelly",
      "Tom Balough",
      "Yao Xu",
      "Jane Polak Scowcroft",
      "Daniel Korzekwa",
      "Darragh Hanley",
      "Sandip Bhaskar",
      "Timo Roman",
      "Karan Sapra",
      "Andrew Tao",
      "Bryan Catanzaro"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20478v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20478v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20431v1",
    "title": "BRIC: Bridging Kinematic Plans and Physical Control at Test Time",
    "summary": "We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.",
    "authors": [
      "Dohun Lim",
      "Minji Kim",
      "Jaewoon Lim",
      "Sungchan Kim"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20431v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20431v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20366v1",
    "title": "VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild",
    "summary": "Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, \\emph{i.e.} VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.",
    "authors": [
      "Xin Ming",
      "Yuxuan Han",
      "Tianyu Huang",
      "Feng Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20366v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20366v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20351v1",
    "title": "Thinking in 360°: Humanoid Visual Search in the Wild",
    "summary": "Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360°. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360° panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.",
    "authors": [
      "Heyang Yu",
      "Yinan Han",
      "Xiangyu Zhang",
      "Baiqiao Yin",
      "Bowen Chang",
      "Xiangyu Han",
      "Xinhao Liu",
      "Jing Zhang",
      "Marco Pavone",
      "Chen Feng",
      "Saining Xie",
      "Yiming Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20351v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20351v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20157v1",
    "title": "SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery",
    "summary": "Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.",
    "authors": [
      "Da Li",
      "Ji-Ping Jin",
      "Xuanlong Yu",
      "Wei Liu",
      "Xiaodong Cun",
      "Kai Chen",
      "Rui Fan",
      "Jiangang Kong",
      "Shen Xi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20157v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20157v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20156v1",
    "title": "Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving",
    "summary": "Motion planning for autonomous driving must handle multiple plausible futures while remaining computationally efficient. Recent end-to-end systems and world-model-based planners predict rich multi-modal trajectories, but typically rely on handcrafted anchors or reinforcement learning to select a single best mode for training and control. This selection discards information about alternative futures and complicates optimization. We propose MAP-World, a prior-free multi-modal planning framework that couples masked action planning with a path-weighted world model. The Masked Action Planning (MAP) module treats future ego motion as masked sequence completion: past waypoints are encoded as visible tokens, future waypoints are represented as mask tokens, and a driving-intent path provides a coarse scaffold. A compact latent planning state is expanded into multiple trajectory queries with injected noise, yielding diverse, temporally consistent modes without anchor libraries or teacher policies. A lightweight world model then rolls out future BEV semantics conditioned on each candidate trajectory. During training, semantic losses are computed as an expectation over modes, using trajectory probabilities as discrete path weights, so the planner learns from the full distribution of plausible futures instead of a single selected path. On NAVSIM, our method matches anchor-based approaches and achieves state-of-the-art performance among world-model-based methods, while avoiding reinforcement learning and maintaining real-time inference latency.",
    "authors": [
      "Bin Hu",
      "Zijian Lu",
      "Haicheng Liao",
      "Chengran Yuan",
      "Bin Rao",
      "Yongkang Li",
      "Guofa Li",
      "Zhiyong Cui",
      "Cheng-zhong Xu",
      "Zhenning Li"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20156v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20156v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.20648v1",
    "title": "LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight",
    "summary": "To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.",
    "authors": [
      "Yunze Man",
      "Shihao Wang",
      "Guowen Zhang",
      "Johan Bjorck",
      "Zhiqi Li",
      "Liang-Yan Gui",
      "Jim Fan",
      "Jan Kautz",
      "Yu-Xiong Wang",
      "Zhiding Yu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20648v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20648v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20587v1",
    "title": "Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models",
    "summary": "We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.",
    "authors": [
      "Karim Kadry",
      "Abdallah Abdelwahed",
      "Shoaib Goraya",
      "Ajay Manicka",
      "Naravich Chutisilp",
      "Farhad Nezami",
      "Elazer Edelman"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20587v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20587v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20541v1",
    "title": "Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation",
    "summary": "This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.",
    "authors": [
      "Andrea Ranieri",
      "Giorgio Palmieri",
      "Silvia Biasotti"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20541v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20541v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20315v1",
    "title": "Geometry of Decision Making in Language Models",
    "summary": "Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \\textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.",
    "authors": [
      "Abhinav Joshi",
      "Divyanshu Bhatt",
      "Ashutosh Modi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20315v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20315v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20284v1",
    "title": "Can LLMs Make (Personalized) Access Control Decisions?",
    "summary": "Precise access control decisions are crucial to the security of both traditional applications and emerging agent-based systems. Typically, these decisions are made by users during app installation or at runtime. Due to the increasing complexity and automation of systems, making these access control decisions can add a significant cognitive load on users, often overloading them and leading to suboptimal or even arbitrary access control decisions. To address this problem, we propose to leverage the processing and reasoning capabilities of large language models (LLMs) to make dynamic, context-aware decisions aligned with the user's security preferences. For this purpose, we conducted a user study, which resulted in a dataset of 307 natural-language privacy statements and 14,682 access control decisions made by users. We then compare these decisions against those made by two versions of LLMs: a general and a personalized one, for which we also gathered user feedback on 1,446 of its decisions.   Our results show that in general, LLMs can reflect users' preferences well, achieving up to 86\\% accuracy when compared to the decision made by the majority of users. Our study also reveals a crucial trade-off in personalizing such a system: while providing user-specific privacy preferences to the LLM generally improves agreement with individual user decisions, adhering to those preferences can also violate some security best practices. Based on our findings, we discuss design and risk considerations for implementing a practical natural-language-based access control system that balances personalization, security, and utility.",
    "authors": [
      "Friederike Groschupp",
      "Daniele Lain",
      "Aritra Dhar",
      "Lara Magdalena Lazier",
      "Srdjan Čapkun"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20284v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20284v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20202v1",
    "title": "Robust 3D Brain MRI Inpainting with Random Masking Augmentation",
    "summary": "The ASNR-MICCAI BraTS-Inpainting Challenge was established to mitigate dataset biases that limit deep learning models in the quantitative analysis of brain tumor MRI. This paper details our submission to the 2025 challenge, a novel deep learning framework for synthesizing healthy tissue in 3D scans. The core of our method is a U-Net architecture trained to inpaint synthetically corrupted regions, enhanced with a random masking augmentation strategy to improve generalization. Quantitative evaluation confirmed the efficacy of our approach, yielding an SSIM of 0.873$\\pm$0.004, a PSNR of 24.996$\\pm$4.694, and an MSE of 0.005$\\pm$0.087 on the validation set. On the final online test set, our method achieved an SSIM of 0.919$\\pm$0.088, a PSNR of 26.932$\\pm$5.057, and an RMSE of 0.052$\\pm$0.026. This performance secured first place in the BraTS-Inpainting 2025 challenge and surpassed the winning solutions from the 2023 and 2024 competitions on the official leaderboard.",
    "authors": [
      "Juexin Zhang",
      "Ying Weng",
      "Ke Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20202v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20202v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20162v1",
    "title": "While recognizing actions, LMMs struggle to detect core interaction events",
    "summary": "Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.",
    "authors": [
      "Daniel Harari",
      "Michael Sidorov",
      "Liel David",
      "Chen Shterental",
      "Abrham Kahsay Gebreselasie",
      "Muhammad Haris Khan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20162v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20162v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20143v1",
    "title": "SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models",
    "summary": "Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.",
    "authors": [
      "Wen-Fang Su",
      "Hsiao-Wei Chou",
      "Wen-Yang Lin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20143v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20143v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.20624v1",
    "title": "ShapeGen: Towards High-Quality 3D Shape Synthesis",
    "summary": "Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.",
    "authors": [
      "Yangguang Li",
      "Xianglong He",
      "Zi-Xin Zou",
      "Zexiang Liu",
      "Wanli Ouyang",
      "Ding Liang",
      "Yan-Pei Cao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20624v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20624v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20621v1",
    "title": "DiFR: Inference Verification Despite Nondeterminism",
    "summary": "As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.",
    "authors": [
      "Adam Karvonen",
      "Daniel Reuter",
      "Roy Rinberg",
      "Luke Marks",
      "Adrià Garriga-Alonso",
      "Keri Warr"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20621v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20621v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20601v1",
    "title": "The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting",
    "summary": "Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.",
    "authors": [
      "Heman Shakeri"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20601v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20601v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20532v1",
    "title": "MIMIC-MJX: Neuromechanical Emulation of Animal Behavior",
    "summary": "The primary output of the nervous system is movement and behavior. While recent advances have democratized pose tracking during complex behavior, kinematic trajectories alone provide only indirect access to the underlying control processes. Here we present MIMIC-MJX, a framework for learning biologically-plausible neural control policies from kinematics. MIMIC-MJX models the generative process of motor control by training neural controllers that learn to actuate biomechanically-realistic body models in physics simulation to reproduce real kinematic trajectories. We demonstrate that our implementation is accurate, fast, data-efficient, and generalizable to diverse animal body models. Policies trained with MIMIC-MJX can be utilized to both analyze neural control strategies and simulate behavioral experiments, illustrating its potential as an integrative modeling framework for neuroscience.",
    "authors": [
      "Charles Y. Zhang",
      "Yuanjia Yang",
      "Aidan Sirbu",
      "Elliott T. T. Abe",
      "Emil Wärnberg",
      "Eric J. Leonardis",
      "Diego E. Aldarondo",
      "Adam Lee",
      "Aaditya Prasad",
      "Jason Foat",
      "Kaiwen Bian",
      "Joshua Park",
      "Rusham Bhatt",
      "Hutton Saunders",
      "Akira Nagamori",
      "Ayesha R. Thanawalla",
      "Kee Wui Huang",
      "Fabian Plum",
      "Hendrik K. Beck",
      "Steven W. Flavell",
      "David Labonte",
      "Blake A. Richards",
      "Bingni W. Brunton",
      "Eiman Azim",
      "Bence P. Ölveczky",
      "Talmo D. Pereira"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20532v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20532v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20480v1",
    "title": "Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders",
    "summary": "Advanced Persistent Threats (APTs) pose a significant challenge in cybersecurity due to their stealthy and long-term nature. Modern supervised learning methods require extensive labeled data, which is often scarce in real-world cybersecurity environments. In this paper, we propose an innovative approach that leverages AutoEncoders for unsupervised anomaly detection, augmented by active learning to iteratively improve the detection of APT anomalies. By selectively querying an oracle for labels on uncertain or ambiguous samples, we minimize labeling costs while improving detection rates, enabling the model to improve its detection accuracy with minimal data while reducing the need for extensive manual labeling. We provide a detailed formulation of the proposed Attention Adversarial Dual AutoEncoder-based anomaly detection framework and show how the active learning loop iteratively enhances the model. The framework is evaluated on real-world imbalanced provenance trace databases produced by the DARPA Transparent Computing program, where APT-like attacks constitute as little as 0.004\\% of the data. The datasets span multiple operating systems, including Android, Linux, BSD, and Windows, and cover two attack scenarios. The results have shown significant improvements in detection rates during active learning and better performance compared to other existing approaches.",
    "authors": [
      "Sidahmed Benabderrahmane",
      "James Cheney",
      "Talal Rahwan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.NE"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20480v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20480v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20470v1",
    "title": "Efficient and Fast Generative-Based Singing Voice Separation using a Latent Diffusion Model",
    "summary": "Extracting individual elements from music mixtures is a valuable tool for music production and practice. While neural networks optimized to mask or transform mixture spectrograms into the individual source(s) have been the leading approach, the source overlap and correlation in music signals poses an inherent challenge. Also, accessing all sources in the mixture is crucial to train these systems, while complicated. Attempts to address these challenges in a generative fashion exist, however, the separation performance and inference efficiency remain limited. In this work, we study the potential of diffusion models to advance toward bridging this gap, focusing on generative singing voice separation relying only on corresponding pairs of isolated vocals and mixtures for training. To align with creative workflows, we leverage latent diffusion: the system generates samples encoded in a compact latent space, and subsequently decodes these into audio. This enables efficient optimization and faster inference. Our system is trained using only open data. We outperform existing generative separation systems, and level the compared non-generative systems on a list of signal quality measures and on interference removal. We provide a noise robustness study on the latent encoder, providing insights on its potential for the task. We release a modular toolkit for further research on the topic.",
    "authors": [
      "Genís Plaja-Roglans",
      "Yun-Ning Hung",
      "Xavier Serra",
      "Igor Pereira"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20470v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20470v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20457v1",
    "title": "A Fully Probabilistic Tensor Network for Regularized Volterra System Identification",
    "summary": "Modeling nonlinear systems with Volterra series is challenging because the number of kernel coefficients grows exponentially with the model order. This work introduces Bayesian Tensor Network Volterra kernel machines (BTN-V), extending the Bayesian Tensor Network framework to Volterra system identification. BTN-V represents Volterra kernels using canonical polyadic decomposition, reducing model complexity from O(I^D) to O(DIR). By treating all tensor components and hyperparameters as random variables, BTN-V provides predictive uncertainty estimation at no additional computational cost. Sparsity-inducing hierarchical priors enable automatic rank determination and the learning of fading-memory behavior directly from data, improving interpretability and preventing overfitting. Empirical results demonstrate competitive accuracy, enhanced uncertainty quantification, and reduced computational cost.",
    "authors": [
      "Afra Kilic",
      "Kim Batselier"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20457v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20457v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20422v1",
    "title": "VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning",
    "summary": "Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.",
    "authors": [
      "Bo Pang",
      "Chenxi Xu",
      "Jierui Ren",
      "Guoping Wang",
      "Sheng Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20422v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20422v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20321v1",
    "title": "Active Inference in Discrete State Spaces from First Principles",
    "summary": "We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.",
    "authors": [
      "Patrick Kenny"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20321v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20321v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20307v1",
    "title": "TReFT: Taming Rectified Flow Models For One-Step Image Translation",
    "summary": "Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.",
    "authors": [
      "Shengqian Li",
      "Ming Gao",
      "Yi Liu",
      "Zuzeng Lin",
      "Feng Wang",
      "Feng Dai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20307v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20307v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20234v1",
    "title": "Leveraging weights signals -- Predicting and improving generalizability in reinforcement learning",
    "summary": "Generalizability of Reinforcement Learning (RL) agents (ability to perform on environments different from the ones they have been trained on) is a key problem as agents have the tendency to overfit to their training environments. In order to address this problem and offer a solution to increase the generalizability of RL agents, we introduce a new methodology to predict the generalizability score of RL agents based on the internal weights of the agent's neural networks. Using this prediction capability, we propose some changes in the Proximal Policy Optimization (PPO) loss function to boost the generalization score of the agents trained with this upgraded version. Experimental results demonstrate that our improved PPO algorithm yields agents with stronger generalizability compared to the original version.",
    "authors": [
      "Olivier Moulin",
      "Vincent Francois-lavet",
      "Paul Elbers",
      "Mark Hoogendoorn"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20234v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20234v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20186v1",
    "title": "Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis",
    "summary": "Foundation video generation models such as WAN 2.2 exhibit strong text- and image-conditioned synthesis abilities but remain constrained to the same-view generation setting. In this work, we introduce Exo2EgoSyn, an adaptation of WAN 2.2 that unlocks Exocentric-to-Egocentric(Exo2Ego) cross-view video synthesis. Our framework consists of three key modules. Ego-Exo View Alignment(EgoExo-Align) enforces latent-space alignment between exocentric and egocentric first-frame representations, reorienting the generative space from the given exo view toward the ego view. Multi-view Exocentric Video Conditioning (MultiExoCon) aggregates multi-view exocentric videos into a unified conditioning signal, extending WAN2.2 beyond its vanilla single-image or text conditioning. Furthermore, Pose-Aware Latent Injection (PoseInj) injects relative exo-to-ego camera pose information into the latent state, guiding geometry-aware synthesis across viewpoints. Together, these modules enable high-fidelity ego view video generation from third-person observations without retraining from scratch. Experiments on ExoEgo4D validate that Exo2EgoSyn significantly improves Ego2Exo synthesis, paving the way for scalable cross-view video generation with foundation models. Source code and models will be released publicly.",
    "authors": [
      "Mohammad Mahdi",
      "Yuqian Fu",
      "Nedko Savov",
      "Jiancheng Pan",
      "Danda Pani Paudel",
      "Luc Van Gool"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20186v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20186v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20145v1",
    "title": "Vision-Language Models for Automated 3D PET/CT Report Generation",
    "summary": "Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.",
    "authors": [
      "Wenpei Jiao",
      "Kun Shang",
      "Hui Li",
      "Ke Yan",
      "Jiajin Zhang",
      "Guangjie Yang",
      "Lijuan Guo",
      "Yan Wan",
      "Xing Yang",
      "Dakai Jin",
      "Zhaoheng Xie"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20145v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20145v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.20651v1",
    "title": "RubricRL: Simple Generalizable Rewards for Text-to-Image Generation",
    "summary": "Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.",
    "authors": [
      "Xuelu Feng",
      "Yunsheng Li",
      "Ziyu Wan",
      "Zixuan Gao",
      "Junsong Yuan",
      "Dongdong Chen",
      "Chunming Qiao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20651v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20651v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20605v1",
    "title": "How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets",
    "summary": "We introduce and analyse active learning markets as a way to purchase labels, in situations where analysts aim to acquire additional data to improve model fitting, or to better train models for predictive analytics applications. This comes in contrast to the many proposals that already exist to purchase features and examples. By originally formalising the market clearing as an optimisation problem, we integrate budget constraints and improvement thresholds into the label acquisition process. We focus on a single-buyer-multiple-seller setup and propose the use of two active learning strategies (variance based and query-by-committee based), paired with distinct pricing mechanisms. They are compared to a benchmark random sampling approach. The proposed strategies are validated on real-world datasets from two critical application domains: real estate pricing and energy forecasting. Results demonstrate the robustness of our approach, consistently achieving superior performance with fewer labels acquired compared to conventional methods. Our proposal comprises an easy-to-implement practical solution for optimising data acquisition in resource-constrained environments.",
    "authors": [
      "Xiwen Huang",
      "Pierre Pinson"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20605v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20605v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20540v1",
    "title": "Proceedings Twentieth Conference on Theoretical Aspects of Rationality and Knowledge",
    "summary": "The TARK conference (Theoretical Aspects of Rationality and Knowledge) is a conference that aims to bring together researchers from a wide variety of fields, including computer science, artificial intelligence, game theory, decision theory, philosophy, logic, linguistics, and cognitive science. Its goal is to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge.   Previous conferences have been held biennially around the world since 1986, on the initiative of Joe Halpern (Cornell University). Topics of interest include, but are not limited to, semantic models for knowledge, belief, uncertainty, awareness, bounded rationality, common sense epistemic reasoning, epistemic logic, epistemic game theory, knowledge and action, applications of reasoning about knowledge and other mental states, belief revision, computational social choice, algorithmic game theory, and foundations of multi-agent systems.   Information about TARK is available at http://www.tark.org/.   These proceedings contain the papers that have been accepted for presentation at the Twentieth Conference on Theoretical Aspects of Rationality and Knowledge (TARK 2025), held July 14--16, 2025, at Heinrich-Heine-Universität, Düsseldorf, Germany. The conference website can be found at https://ccc.cs.uni-duesseldorf.de/tark-2025/.",
    "authors": [
      "Adam Bjorndahl"
    ],
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20540v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20540v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20520v1",
    "title": "HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation",
    "summary": "Recent unified models integrate understanding experts (e.g., LLMs) with generative experts (e.g., diffusion models), achieving strong multimodal performance. However, recent advanced methods such as BAGEL and LMFusion follow the Mixture-of-Transformers (MoT) paradigm, adopting a symmetric design that mirrors one expert to another for convenient initialization and fusion, which remains suboptimal due to inherent modality discrepancies. In this work, we propose HBridge, an asymmetric H-shaped architecture that enables heterogeneous experts to optimally leverage pretrained priors from their respective modality domains. Unlike prior dense fusion strategies that straightforwardly connect all layers between experts via shared attention, HBridge selectively bridges intermediate layers, reducing over 40% attention sharing, which improves efficiency and enhances generation quality. Shallow and deep layers, which capture modality-specific representations, are decoupled, while mid-layer bridging promotes semantic alignment. To further strengthen cross-modal coherence, we introduce semantic reconstruction tokens that explicitly guide the generative expert to reconstruct visual semantic tokens of the target image. Extensive experiments across multiple benchmarks demonstrate the effectiveness and superior performance of HBridge, establishing a new paradigm for unified multimodal generation.",
    "authors": [
      "Xiang Wang",
      "Zhifei Zhang",
      "He Zhang",
      "Zhe Lin",
      "Yuqian Zhou",
      "Qing Liu",
      "Shiwei Zhang",
      "Yijun Li",
      "Shaoteng Liu",
      "Haitian Zheng",
      "Jason Kuen",
      "Yuehuan Wang",
      "Changxin Gao",
      "Nong Sang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20520v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20520v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20490v1",
    "title": "MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology",
    "summary": "Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.",
    "authors": [
      "Kiril Vasilev",
      "Alexandre Misrahi",
      "Eeshaan Jain",
      "Phil F Cheng",
      "Petros Liakopoulos",
      "Olivier Michielin",
      "Michael Moor",
      "Charlotte Bunne"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20490v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20490v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20489v1",
    "title": "InferF: Declarative Factorization of AI/ML Inferences over Joins",
    "summary": "Real-world AI/ML workflows often apply inference computations to feature vectors joined from multiple datasets. To avoid the redundant AI/ML computations caused by repeated data records in the join's output, factorized ML has been proposed to decompose ML computations into sub-computations to be executed on each normalized dataset. However, there is insufficient discussion on how factorized ML could impact AI/ML inference over multi-way joins. To address the limitations, we propose a novel declarative InferF system, focusing on the factorization of arbitrary inference workflows represented as analyzable expressions over the multi-way joins. We formalize our problem to flexibly push down partial factorized computations to qualified nodes in the join tree to minimize the overall inference computation and join costs and propose two algorithms to resolve the problem: (1) a greedy algorithm based on a per-node cost function that estimates the influence on overall latency if a subset of factorized computations is pushed to a node, and (2) a genetic algorithm for iteratively enumerating and evaluating promising factorization plans. We implement InferF on Velox, an open-sourced database engine from Meta, evaluate it on real-world datasets, observed up to 11.3x speedups, and systematically summarized the factors that determine when factorized ML can benefit AI/ML inference workflows.",
    "authors": [
      "Kanchan Chowdhury",
      "Lixi Zhou",
      "Lulu Xie",
      "Xinwei Fu",
      "Jia Zou"
    ],
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20489v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20489v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20474v1",
    "title": "Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification",
    "summary": "Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.",
    "authors": [
      "Akshit Pramod Anchan",
      "Jewelith Thomas",
      "Sritama Roy"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20474v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20474v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20446v1",
    "title": "Learning to Generate Human-Human-Object Interactions from Textual Descriptions",
    "summary": "The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.",
    "authors": [
      "Jeonghyeon Na",
      "Sangwon Baik",
      "Inhee Lee",
      "Junyoung Lee",
      "Hanbyul Joo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20446v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20446v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20439v1",
    "title": "Object-Centric Vision Token Pruning for Vision Language Models",
    "summary": "In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.",
    "authors": [
      "Guangyuan Li",
      "Rongzhen Zhao",
      "Jinhong Deng",
      "Yanbo Wang",
      "Joni Pajarinen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20439v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20439v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20410v1",
    "title": "Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs",
    "summary": "Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.",
    "authors": [
      "Bao Tang",
      "Shuai Zhang",
      "Yueting Zhu",
      "Jijun Xiang",
      "Xin Yang",
      "Li Yu",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20410v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20410v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20361v1",
    "title": "Extension and neural operator approximation of the electrical impedance tomography inverse map",
    "summary": "This paper considers the problem of noise-robust neural operator approximation for the solution map of Calderón's inverse conductivity problem. In this continuum model of electrical impedance tomography (EIT), the boundary measurements are realized as a noisy perturbation of the Neumann-to-Dirichlet map's integral kernel. The theoretical analysis proceeds by extending the domain of the inversion operator to a Hilbert space of kernel functions. The resulting extension shares the same stability properties as the original inverse map from kernels to conductivities, but is now amenable to neural operator approximation. Numerical experiments demonstrate that Fourier neural operators excel at reconstructing infinite-dimensional piecewise constant and lognormal conductivities in noisy setups both within and beyond the theory's assumptions. The methodology developed in this paper for EIT exemplifies a broader strategy for addressing nonlinear inverse problems with a noise-aware operator learning framework.",
    "authors": [
      "Maarten V. de Hoop",
      "Nikola B. Kovachki",
      "Matti Lassas",
      "Nicholas H. Nelsen"
    ],
    "categories": [
      "math.NA",
      "cs.LG",
      "math.AP"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20361v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20361v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20327v1",
    "title": "MXtalTools: A Toolkit for Machine Learning on Molecular Crystals",
    "summary": "We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.",
    "authors": [
      "Michael Kilgour",
      "Mark E. Tuckerman",
      "Jutta Rogal"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20327v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20327v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20325v1",
    "title": "AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models",
    "summary": "End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream\" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.",
    "authors": [
      "Tianyi Yan",
      "Tao Tang",
      "Xingtai Gui",
      "Yongkang Li",
      "Jiasen Zhesng",
      "Weiyao Huang",
      "Lingdong Kong",
      "Wencheng Han",
      "Xia Zhou",
      "Xueyang Zhang",
      "Yifei Zhan",
      "Kun Zhan",
      "Cheng-zhong Xu",
      "Jianbing Shen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20325v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20325v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20236v1",
    "title": "Actionable and diverse counterfactual explanations incorporating domain knowledge and causal constraints",
    "summary": "Counterfactual explanations enhance the actionable interpretability of machine learning models by identifying the minimal changes required to achieve a desired outcome of the model. However, existing methods often ignore the complex dependencies in real-world datasets, leading to unrealistic or impractical modifications. Motivated by cybersecurity applications in the email marketing domain, we propose a method for generating Diverse, Actionable, and kNowledge-Constrained Explanations (DANCE), which incorporates feature dependencies and causal constraints to ensure plausibility and real-world feasibility of counterfactuals. Our method learns linear and nonlinear constraints from data or integrates expert-provided dependency graphs, ensuring counterfactuals are plausible and actionable. By maintaining consistency with feature relationships, the method produces explanations that align with real-world constraints. Additionally, it balances plausibility, diversity, and sparsity, effectively addressing key limitations in existing algorithms. The work is developed based on a real-life case study with Freshmail, the largest email marketing company in Poland and supported by a joint R&D project Sendguard. Furthermore, we provide an extensive evaluation using 140 public datasets, which highlights its ability to generate meaningful, domain-relevant counterfactuals that outperform other existing approaches based on widely used metrics. The source code for reproduction of the results can be found in a GitHub repository we provide.",
    "authors": [
      "Szymon Bobek",
      "Łukasz Bałec",
      "Grzegorz J. Nalepa"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20236v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20236v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20170v1",
    "title": "AdaCap: An Adaptive Contrastive Approach for Small-Data Neural Networks",
    "summary": "Neural networks struggle on small tabular datasets, where tree-based models remain dominant. We introduce Adaptive Contrastive Approach (AdaCap), a training scheme that combines a permutation-based contrastive loss with a Tikhonov-based closed-form output mapping. Across 85 real-world regression datasets and multiple architectures, AdaCap yields consistent and statistically significant improvements in the small-sample regime, particularly for residual models. A meta-predictor trained on dataset characteristics (size, skewness, noise) accurately anticipates when AdaCap is beneficial. These results show that AdaCap acts as a targeted regularization mechanism, strengthening neural networks precisely where they are most fragile. All results and code are publicly available at https://github.com/BrunoBelucci/adacap.",
    "authors": [
      "Bruno Belucci",
      "Karim Lounici",
      "Katia Meziani"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20170v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20170v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.20627v1",
    "title": "Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems",
    "summary": "The integration of AI components, particularly Deep Neural Networks (DNNs), into safety-critical systems such as aerospace and autonomous vehicles presents fundamental challenges for assurance. The opacity of AI systems, combined with the semantic gap between high-level requirements and low-level network representations, creates barriers to traditional verification approaches. These AI-specific challenges are amplified by longstanding issues in Requirements Engineering, including ambiguity in natural language specifications and scalability bottlenecks in formalization. We propose an approach that leverages AI itself to address these challenges through two complementary components. REACT (Requirements Engineering with AI for Consistency and Testing) employs Large Language Models (LLMs) to bridge the gap between informal natural language requirements and formal specifications, enabling early verification and validation. SemaLens (Semantic Analysis of Visual Perception using large Multi-modal models) utilizes Vision Language Models (VLMs) to reason about, test, and monitor DNN-based perception systems using human-understandable concepts. Together, these components provide a comprehensive pipeline from informal requirements to validated implementations.",
    "authors": [
      "Anastasia Mavridou",
      "Divya Gopinath",
      "Corina S. Păsăreanu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20627v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20627v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.20544v1",
    "title": "New York Smells: A Large Multimodal Dataset for Olfaction",
    "summary": "While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.",
    "authors": [
      "Ege Ozguroglu",
      "Junbang Liang",
      "Ruoshi Liu",
      "Mia Chiquier",
      "Michael DeTienne",
      "Wesley Wei Qian",
      "Alexandra Horowitz",
      "Andrew Owens",
      "Carl Vondrick"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20544v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20544v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.20497v1",
    "title": "Quantifying the Privacy Implications of High-Fidelity Synthetic Network Traffic",
    "summary": "To address the scarcity and privacy concerns of network traffic data, various generative models have been developed to produce synthetic traffic. However, synthetic traffic is not inherently privacy-preserving, and the extent to which it leaks sensitive information, and how to measure such leakage, remain largely unexplored. This challenge is further compounded by the diversity of model architectures, which shape how traffic is represented and synthesized. We introduce a comprehensive set of privacy metrics for synthetic network traffic, combining standard approaches like membership inference attacks (MIA) and data extraction attacks with network-specific identifiers and attributes. Using these metrics, we systematically evaluate the vulnerability of different representative generative models and examine the factors that influence attack success. Our results reveal substantial variability in privacy risks across models and datasets. MIA success ranges from 0% to 88%, and up to 100% of network identifiers can be recovered from generated traffic, highlighting serious privacy vulnerabilities. We further identify key factors that significantly affect attack outcomes, including training data diversity and how well the generative model fits the training data. These findings provide actionable guidance for designing and deploying generative models that minimize privacy leakage, establishing a foundation for safer synthetic network traffic generation.",
    "authors": [
      "Van Tran",
      "Shinan Liu",
      "Tian Li",
      "Nick Feamster"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20497v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20497v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.20283v1",
    "title": "Solving Heterogeneous Agent Models with Physics-informed Neural Networks",
    "summary": "Understanding household behaviour is essential for modelling macroeconomic dynamics and designing effective policy. While heterogeneous agent models offer a more realistic alternative to representative agent frameworks, their implementation poses significant computational challenges, particularly in continuous time. The Aiyagari-Bewley-Huggett (ABH) framework, recast as a system of partial differential equations, typically relies on grid-based solvers that suffer from the curse of dimensionality, high computational cost, and numerical inaccuracies. This paper introduces the ABH-PINN solver, an approach based on Physics-Informed Neural Networks (PINNs), which embeds the Hamilton-Jacobi-Bellman and Kolmogorov Forward equations directly into the neural network training objective. By replacing grid-based approximation with mesh-free, differentiable function learning, the ABH-PINN solver benefits from the advantages of PINNs of improved scalability, smoother solutions, and computational efficiency. Preliminary results show that the PINN-based approach is able to obtain economically valid results matching the established finite-difference solvers.",
    "authors": [
      "Marta Grzeskiewicz"
    ],
    "categories": [
      "econ.GN",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20283v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20283v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.20272v1",
    "title": "VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs",
    "summary": "While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This high-level vision-grounded semantics, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored area in current MLLMs. To systematically evaluate this capability, we present VKnowU, a comprehensive benchmark featuring 1,680 questions in 1,249 videos, covering 8 core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Evaluation of 23 SOTA MLLMs reveals that leading models still fall short of human performance, with particularly notable gaps in the world-centric. To bridge this gap, we introduce a new dataset, VKnowQA, and VideoKnow+, a baseline model that explicitly incorporates visual knowledge into MLLMs. VideoKnow+ follows a structured See-Think-Answer paradigm and adopts reinforcement learning with visual knowledge reward, achieving a +3.7% improvement on VKnowU and consistent gains on MVBench, Video-MME, and MMVU. Our work highlights visual knowledge as a missing cornerstone for developing more generalizable MLLMs that can not only see but also truly understand our physical and social worlds.",
    "authors": [
      "Tianxiang Jiang",
      "Sheng Xia",
      "Yicheng Xu",
      "Linquan Wu",
      "Xiangyu Zeng",
      "Limin Wang",
      "Yu Qiao",
      "Yi Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20272v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20272v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.20645v1",
    "title": "PixelDiT: Pixel Diffusion Transformers for Image Generation",
    "summary": "Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.",
    "authors": [
      "Yongsheng Yu",
      "Wei Xiong",
      "Weili Nie",
      "Yichen Sheng",
      "Shiqiu Liu",
      "Jiebo Luo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20645v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20645v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20610v1",
    "title": "Building a Foundation Model for Trajectory from Scratch",
    "summary": "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.",
    "authors": [
      "Gaspard Merten",
      "Mahmoud Sakr",
      "Gilles Dejaegere"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20610v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20610v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20565v1",
    "title": "DINO-Tok: Adapting DINO for Visual Tokenizers",
    "summary": "Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.",
    "authors": [
      "Mingkai Jia",
      "Mingxiao Li",
      "Liaoyuan Fan",
      "Tianxing Shi",
      "Jiaxin Guo",
      "Zeming Li",
      "Xiaoyang Guo",
      "Xiao-Xiao Long",
      "Qian Zhang",
      "Ping Tan",
      "Wei Yin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20565v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20565v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20526v1",
    "title": "Assessing LLMs' Performance: Insights from the Chinese Pharmacist Exam",
    "summary": "Background: As large language models (LLMs) become increasingly integrated into digital health education and assessment workflows, their capabilities in supporting high-stakes, domain-specific certification tasks remain underexplored.In China, the national pharmacist licensure exam serves as a standardized benchmark for evaluating pharmacists' clinical and theoretical competencies. Objective: This study aimed to compare the performance of two LLMs: ChatGPT-4o and DeepSeek-R1 on real questions from the Chinese Pharmacist Licensing Examination (2017-2021), and to discuss the implications of these performance differences for AI-enabled formative evaluation. Methods: A total of 2,306 multiple-choice (text-only) questions were compiled from official exams, training materials, and public databases. Questions containing tables or images were excluded. Each item was input in its original Chinese format, and model responses were evaluated for exact accuracy. Pearson's Chi-squared test was used to compare overall performance, and Fisher's exact test was applied to year-wise multiple-choice accuracy. Results: DeepSeek-R1 outperformed ChatGPT-4o with a significantly higher overall accuracy (90.0% vs. 76.1%, p < 0.001). Unit-level analyses revealed consistent advantages for DeepSeek-R1, particularly in foundational and clinical synthesis modules. While year-by-year multiple-choice performance also favored DeepSeek-R1, this performance gap did not reach statistical significance in any specific unit-year (all p > 0.05). Conclusion: DeepSeek-R1 demonstrated robust alignment with the structural and semantic demands of the pharmacist licensure exam. These findings suggest that domain-specific models warrant further investigation for this context, while also reinforcing the necessity of human oversight in legally and ethically sensitive contexts.",
    "authors": [
      "Xinran Wang",
      "Boran Zhu",
      "Shujuan Zhou",
      "Ziwen Long",
      "Dehua Zhou",
      "Shu Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20526v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20526v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20525v1",
    "title": "Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos",
    "summary": "We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.",
    "authors": [
      "Yayuan Li",
      "Aadit Jain",
      "Filippos Bellos",
      "Jason J. Corso"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20525v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20525v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20445v1",
    "title": "Diffusion for Fusion: Designing Stellarators with Generative AI",
    "summary": "Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.",
    "authors": [
      "Misha Padidar",
      "Teresa Huang",
      "Andrew Giuliani",
      "Marina Spivak"
    ],
    "categories": [
      "cs.LG",
      "physics.plasm-ph"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20445v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20445v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20407v1",
    "title": "Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets",
    "summary": "We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.",
    "authors": [
      "Kasper Green Larsen",
      "Natascha Schalburg"
    ],
    "categories": [
      "cs.LG",
      "math.ST"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20407v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20407v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20406v1",
    "title": "Short-Range Oversquashing",
    "summary": "Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.   In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.   We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.",
    "authors": [
      "Yaaqov Mishayev",
      "Yonatan Sverdlov",
      "Tal Amir",
      "Nadav Dym"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20406v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20406v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20332v1",
    "title": "3D Motion Perception of Binocular Vision Target with PID-CNN",
    "summary": "This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.",
    "authors": [
      "Shi Jiazhao",
      "Pan Pan",
      "Shi Haotian"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20332v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20332v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20319v1",
    "title": "IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection",
    "summary": "Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.",
    "authors": [
      "Xuelin Qian",
      "Jiaming Lu",
      "Zixuan Wang",
      "Wenxuan Wang",
      "Zhongling Huang",
      "Dingwen Zhang",
      "Junwei Han"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20319v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20319v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20306v1",
    "title": "TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection",
    "summary": "Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance.",
    "authors": [
      "Han Guo",
      "Chenyang Liu",
      "Haotian Zhang",
      "Bowen Chen",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20306v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20306v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20270v1",
    "title": "DRL-Guided Neural Batch Sampling for Semi-Supervised Pixel-Level Anomaly Detection",
    "summary": "Anomaly detection in industrial visual inspection is challenging due to the scarcity of defective samples. Most existing methods rely on unsupervised reconstruction using only normal data, often resulting in overfitting and poor detection of subtle defects. We propose a semi-supervised deep reinforcement learning framework that integrates a neural batch sampler, an autoencoder, and a predictor. The RL-based sampler adaptively selects informative patches by balancing exploration and exploitation through a composite reward. The autoencoder generates loss profiles highlighting abnormal regions, while the predictor performs segmentation in the loss-profile space. This interaction enables the system to effectively learn both normal and defective patterns with limited labeled data. Experiments on the MVTec AD dataset demonstrate that our method achieves higher accuracy and better localization of subtle anomalies than recent state-of-the-art approaches while maintaining low complexity, yielding an average improvement of 0.15 in F1_max and 0.06 in AUC, with a maximum gain of 0.37 in F1_max in the best case.",
    "authors": [
      "Amirhossein Khadivi Noghredeh",
      "Abdollah Safari",
      "Fatemeh Ziaeetabar",
      "Firoozeh Haghighi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20270v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20270v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20254v1",
    "title": "XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface",
    "summary": "Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation.   Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames.   Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections.   Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.",
    "authors": [
      "Alexander C. Jenke",
      "Gregor Just",
      "Claas de Boer",
      "Martin Wagner",
      "Sebastian Bodenstedt",
      "Stefanie Speidel"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20254v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20254v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20251v1",
    "title": "PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling",
    "summary": "Recent advances in text-to-image (T2I) generation have achieved remarkable visual outcomes through large-scale rectified flow models. However, how these models behave under long prompts remains underexplored. Long prompts encode rich content, spatial, and stylistic information that enhances fidelity but often suppresses diversity, leading to repetitive and less creative outputs. In this work, we systematically study this fidelity-diversity dilemma and reveal that state-of-the-art models exhibit a clear drop in diversity as prompt length increases. To enable consistent evaluation, we introduce LPD-Bench, a benchmark designed for assessing both fidelity and diversity in long-prompt generation. Building on our analysis, we develop a theoretical framework that increases sampling entropy through prompt reformulation and propose a training-free method, PromptMoG, which samples prompt embeddings from a Mixture-of-Gaussians in the embedding space to enhance diversity while preserving semantics. Extensive experiments on four state-of-the-art models, SD3.5-Large, Flux.1-Krea-Dev, CogView4, and Qwen-Image, demonstrate that PromptMoG consistently improves long-prompt generation diversity without semantic drifting.",
    "authors": [
      "Bo-Kai Ruan",
      "Teng-Fang Hsiao",
      "Ling Lo",
      "Yi-Lun Wu",
      "Hong-Han Shuai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20251v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20251v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.20649v1",
    "title": "Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout",
    "summary": "Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.",
    "authors": [
      "Hidir Yesiltepe",
      "Tuna Han Salih Meral",
      "Adil Kaan Akan",
      "Kaan Oktay",
      "Pinar Yanardag"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20649v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20649v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20620v1",
    "title": "Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI",
    "summary": "Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.",
    "authors": [
      "Xinhao Liu",
      "Jiaqi Li",
      "Youming Deng",
      "Ruxin Chen",
      "Yingjia Zhang",
      "Yifei Ma",
      "Li Guo",
      "Yiming Li",
      "Jing Zhang",
      "Chen Feng"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20620v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20620v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20614v1",
    "title": "The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment",
    "summary": "Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.",
    "authors": [
      "Ziheng Ouyang",
      "Yiren Song",
      "Yaoli Liu",
      "Shihao Zhu",
      "Qibin Hou",
      "Ming-Ming Cheng",
      "Mike Zheng Shou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20614v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20614v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20590v1",
    "title": "EnergyTwin: A Multi-Agent System for Simulating and Coordinating Energy Microgrids",
    "summary": "Microgrids are deployed to reduce purchased grid energy, limit exposure to volatile tariffs, and ensure service continuity during disturbances. This requires coordinating heterogeneous distributed energy resources across multiple time scales and under variable conditions. Among existing tools, typically, power-system simulators capture physical behaviour but assume centralized control, while multi-agent frameworks model decentralized decision-making but represent energy with no physical grounding. In this context, the EnergyTwin is introduced, an agent-based microgrid simulation environment that couples physically grounded models with forecast-informed, rolling-horizon planning, and negotiations. Each asset is modeled as an agent, interacting with a central agent that obtains forecasts, formulates predictions, and allocates energy through contract-based interactions. EnergyTwin targets tertiary-layer decision making and is extensible for digital-twin use. Its feasibility was evaluated in a university campus microgrid scenario where multiple planning strategies were compared. Achieved results show that forecast-driven rolling-horizon planning increases local energy self-sufficiency, maintains higher battery reserves, and reduces exposure to low-resilience operating states. They demonstrate also potential of EnergyTwin as platform supporting research on resilient, negotiation-driven microgrids.",
    "authors": [
      "Jakub Muszyński",
      "Ignacy Walużenicz",
      "Patryk Zan",
      "Zofia Wrona",
      "Maria Ganzha",
      "Marcin Paprzycki",
      "Costin Bădică"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20590v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20590v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20570v1",
    "title": "Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics",
    "summary": "Safety-critical assistive systems that directly decode user intent from neural signals require rigorous guarantees of reliability and trust. We present GUARDIAN (Gated Uncertainty-Aware Runtime Dual Invariants), a framework for real-time neuro-symbolic verification for neural signal-controlled robotics. GUARDIAN enforces both logical safety and physiological trust by coupling confidence-calibrated brain signal decoding with symbolic goal grounding and dual-layer runtime monitoring. On the BNCI2014 motor imagery electroencephalogram (EEG) dataset with 9 subjects and 5,184 trials, the system performs at a high safety rate of 94-97% even with lightweight decoder architectures with low test accuracies (27-46%) and high ECE confidence miscalibration (0.22-0.41). We demonstrate 1.7x correct interventions in simulated noise testing versus at baseline. The monitor operates at 100Hz and sub-millisecond decision latency, making it practically viable for closed-loop neural signal-based systems. Across 21 ablation results, GUARDIAN exhibits a graduated response to signal degradation, and produces auditable traces from intent, plan to action, helping to link neural evidence to verifiable robot action.",
    "authors": [
      "Tasha Kim",
      "Oiwi Parker Jones"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20570v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20570v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20516v1",
    "title": "Adam Simplified: Bias Correction Simplified",
    "summary": "The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \\in [0,1)$. Our findings challenge the universal inclusion of this component.",
    "authors": [
      "Sam Laing",
      "Antonio Orvieto"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20516v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20516v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20503v1",
    "title": "Generative Modeling with Manifold Percolation",
    "summary": "Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task manifests as disentangling the geometric support from the probability distribution. We propose that Continuum Percolation is uniquely suited for this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous isomorphism between the topological phase transitions of Random Geometric Graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we demonstrate that our metric captures structural pathologies (such as implicit mode collapse) where statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function to guide training. Experimental results confirm that this approach not only prevents manifold shrinkage but drives the model toward a state of \"Hyper-Generalization,\" achieving good fidelity and verified topological expansion.",
    "authors": [
      "Rui Tong"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20503v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20503v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20399v1",
    "title": "BengaliFig: A Low-Resource Challenge for Figurative and Culturally Grounded Reasoning in Bengali",
    "summary": "Large language models excel on broad multilingual benchmarks but remain to be evaluated extensively in figurative and culturally grounded reasoning, especially in low-resource contexts. We present BengaliFig, a compact yet richly annotated challenge set that targets this gap in Bengali, a widely spoken low-resourced language. The dataset contains 435 unique riddles drawn from Bengali oral and literary traditions. Each item is annotated along five orthogonal dimensions capturing reasoning type, trap type, cultural depth, answer category, and difficulty, and is automatically converted to multiple-choice format through a constraint-aware, AI-assisted pipeline. We evaluate eight frontier LLMs from major providers under zero-shot and few-shot chain-of-thought prompting, revealing consistent weaknesses in metaphorical and culturally specific reasoning. BengaliFig thus contributes both a diagnostic probe for evaluating LLM robustness in low-resource cultural contexts and a step toward inclusive and heritage-aware NLP evaluation.",
    "authors": [
      "Abdullah Al Sefat"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20399v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20399v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20395v1",
    "title": "Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI",
    "summary": "Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.   We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.   Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.   To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.",
    "authors": [
      "M. C. Schoppema",
      "B. H. M. van der Velden",
      "A. Hürriyetoğlu",
      "M. D. Klijnstra",
      "E. J. Faassen",
      "A. Gerssen",
      "H. J. van der Fels-Klerx"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20395v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20395v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20354v1",
    "title": "GS-Checker: Tampering Localization for 3D Gaussian Splatting",
    "summary": "Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.",
    "authors": [
      "Haoliang Han",
      "Ziyuan Luo",
      "Jun Qi",
      "Anderson Rocha",
      "Renjie Wan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20354v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20354v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20280v1",
    "title": "Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement",
    "summary": "Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.",
    "authors": [
      "Yang Liu",
      "Xilin Zhao",
      "Peisong Wen",
      "Siran Dai",
      "Qingming Huang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20280v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20280v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20151v1",
    "title": "Hybrid Convolution and Frequency State Space Network for Image Compression",
    "summary": "Learned image compression (LIC) has recently benefited from Transformer based and state space model (SSM) based architectures. Convolutional neural networks (CNNs) effectively capture local high frequency details, whereas Transformers and SSMs provide strong long range modeling capabilities but may cause structural information loss or ignore frequency characteristics that are crucial for compression. In this work we propose HCFSSNet, a Hybrid Convolution and Frequency State Space Network for LIC. HCFSSNet uses CNNs to extract local high frequency structures and introduces a Vision Frequency State Space (VFSS) block that models long range low frequency information. The VFSS block combines an Omni directional Neighborhood State Space (VONSS) module, which scans features horizontally, vertically and diagonally, with an Adaptive Frequency Modulation Module (AFMM) that applies content adaptive weighting of discrete cosine transform frequency components for more efficient bit allocation. To further reduce redundancy in the entropy model, we integrate AFMM with a Swin Transformer to form a Frequency Swin Transformer Attention Module (FSTAM) for frequency aware side information modeling. Experiments on the Kodak, Tecnick and CLIC Professional Validation datasets show that HCFSSNet achieves competitive rate distortion performance compared with recent SSM based codecs such as MambaIC, while using significantly fewer parameters. On Kodak, Tecnick and CLIC, HCFSSNet reduces BD rate over the VTM anchor by 18.06, 24.56 and 22.44 percent, respectively, providing an efficient and interpretable hybrid architecture for future learned image compression systems.",
    "authors": [
      "Haodong Pan",
      "Hao Wei",
      "Yusong Wang",
      "Nanning Zheng",
      "Caigui Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20151v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20151v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20138v1",
    "title": "From data to concepts via wiring diagrams",
    "summary": "A wiring diagram is a labeled directed graph that represents an abstract concept such as a temporal process. In this article, we introduce the notion of a quasi-skeleton wiring diagram graph, and prove that quasi-skeleton wiring diagram graphs correspond to Hasse diagrams. Using this result, we designed algorithms that extract wiring diagrams from sequential data. We used our algorithms in analyzing the behavior of an autonomous agent playing a computer game, and the algorithms correctly identified the winning strategies. We compared the performance of our main algorithm with two other algorithms based on standard clustering techniques (DBSCAN and agglomerative hierarchical), including when some of the data was perturbed. Overall, this article brings together techniques in category theory, graph theory, clustering, reinforcement learning, and data engineering.",
    "authors": [
      "Jason Lo",
      "Mohammadnima Jafari"
    ],
    "categories": [
      "cs.AI",
      "cs.DM",
      "cs.LG",
      "math.CO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20138v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20138v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.20591v1",
    "title": "Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning",
    "summary": "The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.",
    "authors": [
      "Charlotte Beylier",
      "Hannah Selder",
      "Arthur Fleig",
      "Simon M. Hofmann",
      "Nico Scherf"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20591v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20591v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20584v1",
    "title": "A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent",
    "summary": "Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.",
    "authors": [
      "Shuo Xie",
      "Tianhao Wang",
      "Beining Wu",
      "Zhiyuan Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20584v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20584v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20460v1",
    "title": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search",
    "summary": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.",
    "authors": [
      "Yunqi Zhou",
      "Chengjie Jiang",
      "Chun Yuan",
      "Jing Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20460v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20460v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20426v1",
    "title": "Block Cascading: Training Free Acceleration of Block-Causal Video Models",
    "summary": "Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/",
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Nikhil Pinnaparaju",
      "Rahim Entezari",
      "Jim Scott",
      "Yi-Zhe Song",
      "Varun Jampani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20426v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20426v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20413v1",
    "title": "PAC-Bayes Meets Online Contextual Optimization",
    "summary": "The predict-then-optimize paradigm bridges online learning and contextual optimization in dynamic environments. Previous works have investigated the sequential updating of predictors using feedback from downstream decisions to minimize regret in the full-information settings. However, existing approaches are predominantly frequentist, rely heavily on gradient-based strategies, and employ deterministic predictors that could yield high variance in practice despite their asymptotic guarantees. This work introduces, to the best of our knowledge, the first Bayesian online contextual optimization framework. Grounded in PAC-Bayes theory and general Bayesian updating principles, our framework achieves $\\mathcal{O}(\\sqrt{T})$ regret for bounded and mixable losses via a Gibbs posterior, eliminates the dependence on gradients through sequential Monte Carlo samplers, and thereby accommodates nondifferentiable problems. Theoretical developments and numerical experiments substantiate our claims.",
    "authors": [
      "Zhuojun Xie",
      "Adam Abdin",
      "Yiping Fang"
    ],
    "categories": [
      "math.OC",
      "stat.ML"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20413v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20413v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20401v1",
    "title": "A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control",
    "summary": "Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.",
    "authors": [
      "Jiawei Lin",
      "Guanlong Jiao",
      "Jianjin Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20401v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20401v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20256v1",
    "title": "The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation",
    "summary": "A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.",
    "authors": [
      "Weijia Mao",
      "Hao Chen",
      "Zhenheng Yang",
      "Mike Zheng Shou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20256v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20256v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20245v1",
    "title": "HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers",
    "summary": "Existing deep learning methods in multimode fiber (MMF) imaging often focus on simpler datasets, limiting their applicability to complex, real-world imaging tasks. These models are typically data-intensive, a challenge that becomes more pronounced when dealing with diverse and complex images. In this work, we propose HistoSpeckle-Net, a deep learning architecture designed to reconstruct structurally rich medical images from MMF speckles. To build a clinically relevant dataset, we develop an optical setup that couples laser light through a spatial light modulator (SLM) into an MMF, capturing output speckle patterns corresponding to input OrganAMNIST images. Unlike previous MMF imaging approaches, which have not considered the underlying statistics of speckles and reconstructed images, we introduce a distribution-aware learning strategy. We employ a histogram-based mutual information loss to enhance model robustness and reduce reliance on large datasets. Our model includes a histogram computation unit that estimates smooth marginal and joint histograms for calculating mutual information loss. It also incorporates a unique Three-Scale Feature Refinement Module, which leads to multiscale Structural Similarity Index Measure (SSIM) loss computation. Together, these two loss functions enhance both the structural fidelity and statistical alignment of the reconstructed images. Our experiments on the complex OrganAMNIST dataset demonstrate that HistoSpeckle-Net achieves higher fidelity than baseline models such as U-Net and Pix2Pix. It gives superior performance even with limited training samples and across varying fiber bending conditions. By effectively reconstructing complex anatomical features with reduced data and under fiber perturbations, HistoSpeckle-Net brings MMF imaging closer to practical deployment in real-world clinical environments.",
    "authors": [
      "Jawaria Maqbool",
      "M. Imran Cheema"
    ],
    "categories": [
      "cs.CV",
      "physics.optics"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20245v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20245v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.20594v1",
    "title": "Variational bagging: a robust approach for Bayesian uncertainty quantification",
    "summary": "Variational Bayes methods are popular due to their computational efficiency and adaptability to diverse applications. In specifying the variational family, mean-field classes are commonly used, which enables efficient algorithms such as coordinate ascent variational inference (CAVI) but fails to capture parameter dependence and typically underestimates uncertainty. In this work, we introduce a variational bagging approach that integrates a bagging procedure with variational Bayes, resulting in a bagged variational posterior for improved inference. We establish strong theoretical guarantees, including posterior contraction rates for general models and a Bernstein-von Mises (BVM) type theorem that ensures valid uncertainty quantification. Notably, our results show that even when using a mean-field variational family, our approach can recover off-diagonal elements of the limiting covariance structure and provide proper uncertainty quantification. In addition, variational bagging is robust to model misspecification, with covariance structures matching those of the target covariance. We illustrate our variational bagging method in numerical studies through applications to parametric models, finite mixture models, deep neural networks, and variational autoencoders (VAEs).",
    "authors": [
      "Shitao Fan",
      "Ilsang Ohn",
      "David Dunson",
      "Lizhen Lin"
    ],
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20594v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20594v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.20263v1",
    "title": "Advancing Image Classification with Discrete Diffusion Classification Modeling",
    "summary": "Image classification is a well-studied task in computer vision, and yet it remains challenging under high-uncertainty conditions, such as when input images are corrupted or training data are limited. Conventional classification approaches typically train models to directly predict class labels from input images, but this might lead to suboptimal performance in such scenarios. To address this issue, we propose Discrete Diffusion Classification Modeling (DiDiCM), a novel framework that leverages a diffusion-based procedure to model the posterior distribution of class labels conditioned on the input image. DiDiCM supports diffusion-based predictions either on class probabilities or on discrete class labels, providing flexibility in computation and memory trade-offs. We conduct a comprehensive empirical study demonstrating the superior performance of DiDiCM over standard classifiers, showing that a few diffusion iterations achieve higher classification accuracy on the ImageNet dataset compared to baselines, with accuracy gains increasing as the task becomes more challenging. We release our code at https://github.com/omerb01/didicm .",
    "authors": [
      "Omer Belhasin",
      "Shelly Golan",
      "Ran El-Yaniv",
      "Michael Elad"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20263v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20263v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.20152v1",
    "title": "Restora-Flow: Mask-Guided Image Restoration with Flow Matching",
    "summary": "Flow matching has emerged as a promising generative approach that addresses the lengthy sampling times associated with state-of-the-art diffusion models and enables a more flexible trajectory design, while maintaining high-quality image generation. This capability makes it suitable as a generative prior for image restoration tasks. Although current methods leveraging flow models have shown promising results in restoration, some still suffer from long processing times or produce over-smoothed results. To address these challenges, we introduce Restora-Flow, a training-free method that guides flow matching sampling by a degradation mask and incorporates a trajectory correction mechanism to enforce consistency with degraded inputs. We evaluate our approach on both natural and medical datasets across several image restoration tasks involving a mask-based degradation, i.e., inpainting, super-resolution and denoising. We show superior perceptual quality and processing time compared to diffusion and flow matching-based reference methods.",
    "authors": [
      "Arnela Hadzic",
      "Franz Thaler",
      "Lea Bogensperger",
      "Simon Johannes Joham",
      "Martin Urschler"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20152v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20152v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.20274v1",
    "title": "ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis",
    "summary": "Until recently, the general corpus of CLIP-type fundamental models has widely explored either the retrieval of short descriptions or the classification of objects in the scene as SINGLE-object image classification task. The same holds for retrieving the image embedding (image retrieval task) given a text prompt. However, real-world scene images exhibit rich compositional structure involving multiple objects and actions. The latest methods in the CLIP-based literature improve class-level discrimination by mining harder negative image-text pairs and by refining permanent text prompts, often using LLMs. However, these improvements remain confined to predefined class lists and do not explicitly model relational or compositional structure. PyramidCLIP partially addresses this gap by aligning global and local visual features, yet it still lacks explicit modeling of inter-object relations. Hence, to further leverage this aspect for scene analysis, the proposed ScenarioCLIP model accepts input texts, grounded relations, and input images, along with focused regions highlighting relations. The proposed model is pretrained on curated scenario data, and finetuned for specialized downstream tasks, such as cross-modal retrieval and fine-grained visual understanding tasks. To address the lack of domain-specific datasets, we generate a novel dataset by extending image-text pairs from existing diverse indoor and outdoor scenario datasets that are publicly available. We used a pipeline of existing language models to ground action, object, and relations, filled by manual and automatic curation. We established a comprehensive benchmark for several scenario-based tasks and compared it with many baseline methods. ScenarioCLIP demonstrates robust zero-shot and finetune performance on various domain-specific tasks. Our code and dataset are available at https://github.com/scenario-clip/ScenarioCLIP",
    "authors": [
      "Advik Sinha",
      "Saurabh Atreya",
      "Aashutosh A",
      "Sk Aziz Ali",
      "Abhijit Das"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20274v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20274v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.20563v1",
    "title": "A Reason-then-Describe Instruction Interpreter for Controllable Video Generation",
    "summary": "Diffusion Transformers have significantly improved video fidelity and temporal coherence, however, practical controllability remains limited. Concise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent-output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions, and (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent. Project Page: https://sqwu.top/ReaDe/.",
    "authors": [
      "Shengqiong Wu",
      "Weicai Ye",
      "Yuanxing Zhang",
      "Jiahao Wang",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Kun Gai",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20563v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20563v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.20221v1",
    "title": "Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder",
    "summary": "The significant molecular and pathological heterogeneity of glioblastoma, an aggressive brain tumor, complicates diagnosis and patient stratification. While traditional histopathological assessment remains the standard, deep learning offers a promising path toward objective and automated analysis of whole slide images. For the BraTS-Path 2025 Challenge, we developed a method that fine-tunes a pre-trained Vision Transformer (ViT) encoder with a dedicated classification head on the official training dataset. Our model's performance on the online validation set, evaluated via the Synapse platform, yielded a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676. On the final test set, the model achieved an MCC of 0.6509 and an F1-score of 0.5330, which secured our team second place in the BraTS-Pathology 2025 Challenge. Our results establish a solid baseline for ViT-based histopathological analysis, and future efforts will focus on bridging the performance gap observed on the unseen validation data.",
    "authors": [
      "Juexin Zhang",
      "Qifeng Zhong",
      "Ying Weng",
      "Ke Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20221v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20221v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.20515v1",
    "title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "summary": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.",
    "authors": [
      "Kuniaki Saito",
      "Risa Shinoda",
      "Shohei Tanaka",
      "Tosho Hirasawa",
      "Fumio Okura",
      "Yoshitaka Ushiku"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20515v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20515v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.20646v1",
    "title": "3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding",
    "summary": "This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.",
    "authors": [
      "Xiaoye Wang",
      "Chen Tang",
      "Xiangyu Yue",
      "Wei-Hong Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20646v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20646v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.20635v1",
    "title": "iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation",
    "summary": "Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.",
    "authors": [
      "Zhoujie Fu",
      "Xianfang Zeng",
      "Jinghong Lan",
      "Xinyao Liao",
      "Cheng Chen",
      "Junyi Chen",
      "Jiacheng Wei",
      "Wei Cheng",
      "Shiyu Liu",
      "Yunuo Chen",
      "Gang Yu",
      "Guosheng Lin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20635v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20635v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.20322v1",
    "title": "Modified Equations for Stochastic Optimization",
    "summary": "In this thesis, we extend the recently introduced theory of stochastic modified equations (SMEs) for stochastic gradient optimization algorithms.   In Ch. 3 we study time-inhomogeneous SDEs driven by Brownian motion. For certain SDEs we prove a 1st and 2nd-order weak approximation properties, and we compute their linear error terms explicitly, under certain regularity conditions. In Ch. 4 we instantiate our results for SGD, working out the example of linear regression explicitly. We use this example to compare the linear error terms of gradient flow and two commonly used 1st-order SMEs for SGD in Ch. 5.   In the second part of the thesis we introduce and study a novel diffusion approximation for SGD without replacement (SGDo) in the finite-data setting. In Ch. 6 we motivate and define the notion of an epoched Brownian motion (EBM). We argue that Young differential equations (YDEs) driven by EBMs serve as continuous-time models for SGDo for any shuffling scheme whose induced permutations converge to a det. permuton. Further, we prove a.s. convergence for these YDEs in the strongly convex setting. Moreover, we compute an upper asymptotic bound on the convergence rate which is as sharp as, or better than previous results for SGDo. In Ch. 7 we study scaling limits of families of random walks (RW) that share the same increments up to a random permutation. We show weak convergence under the assumption that the sequence of permutations converges to a det. (higher-dimensional) permuton. This permuton determines the covariance function of the limiting Gaussian process. Conversely, we show that every Gaussian process with a covariance function determined by a permuton in this way arises as a weak scaling limit of families of RW with shared increments. Finally, we apply our weak convergence theory to show that EBMs arise as scaling limits of RW with finitely many distinct increments.",
    "authors": [
      "Stefan Perko"
    ],
    "categories": [
      "math.PR",
      "math.NA",
      "stat.ML"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20322v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20322v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.20123v1",
    "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers",
    "summary": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.",
    "authors": [
      "Min Zhao",
      "Hongzhou Zhu",
      "Yingze Wang",
      "Bokai Yan",
      "Jintao Zhang",
      "Guande He",
      "Ling Yang",
      "Chongxuan Li",
      "Jun Zhu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20123v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20123v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.20218v1",
    "title": "Text-guided Controllable Diffusion for Realistic Camouflage Images Generation",
    "summary": "Camouflage Images Generation (CIG) is an emerging research area that focuses on synthesizing images in which objects are harmoniously blended and exhibit high visual consistency with their surroundings. Existing methods perform CIG by either fusing objects into specific backgrounds or outpainting the surroundings via foreground object-guided diffusion. However, they often fail to obtain natural results because they overlook the logical relationship between camouflaged objects and background environments. To address this issue, we propose CT-CIG, a Controllable Text-guided Camouflage Images Generation method that produces realistic and logically plausible camouflage images. Leveraging Large Visual Language Models (VLM), we design a Camouflage-Revealing Dialogue Mechanism (CRDM) to annotate existing camouflage datasets with high-quality text prompts. Subsequently, the constructed image-prompt pairs are utilized to finetune Stable Diffusion, incorporating a lightweight controller to guide the location and shape of camouflaged objects for enhanced camouflage scene fitness. Moreover, we design a Frequency Interaction Refinement Module (FIRM) to capture high-frequency texture features, facilitating the learning of complex camouflage patterns. Extensive experiments, including CLIPScore evaluation and camouflage effectiveness assessment, demonstrate the semantic alignment of our generated text prompts and CT-CIG's ability to produce photorealistic camouflage images.",
    "authors": [
      "Yuhang Qian",
      "Haiyan Chen",
      "Wentong Li",
      "Ningzhong Liu",
      "Jie Qin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20218v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20218v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2511.20158v1",
    "title": "Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs",
    "summary": "While continual visual instruction tuning (CVIT) has shown promise in adapting multimodal large language models (MLLMs), existing studies predominantly focus on models without safety alignment. This critical oversight ignores the fact that real-world MLLMs inherently require such mechanisms to mitigate potential risks. In this work, we shift our focus to CVIT for safety-aligned MLLMs and observe that during continual adaptation, the model not only suffers from task forgetting but also exhibits degradation in its safety. Achieving a harmonious balance between safety and task performance remains a crucial challenge. To address this, we propose Harmonious Parameter Adaptation (HPA), a post-training framework composed of focusing-based parameter partition, harmoniously balanced parameter selection, and orthogonal parameter adjustment. Specifically, HPA partitions parameters into two types based on their focus on safety or task performance, and selects the focused ones to preserve from a balanced perspective. In addition, HPA imposes orthogonality constraints on parameter updates to further alleviate catastrophic forgetting. Extensive experiments on the CVIT benchmark and safety evaluation datasets demonstrate that HPA better maintains high safety and mitigates forgetting than existing baselines.",
    "authors": [
      "Ziqi Wang",
      "Chang Che",
      "Qi Wang",
      "Hui Ma",
      "Zenglin Shi",
      "Cees G. M. Snoek",
      "Meng Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20158v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20158v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2511.20348v1",
    "title": "Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin",
    "summary": "3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.",
    "authors": [
      "João Malheiro Silva",
      "Andy Huynh",
      "Tong Duy Son",
      "Holger Caesar"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20348v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20348v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.20607v1",
    "title": "Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains",
    "summary": "We study the optimization of functions with $n>2$ arguments that have a representation as a sum of several functions that have only $2$ of the $n$ arguments each, termed sums of bivariates, on finite domains. The complexity of optimizing sums of bivariates is shown to be NP-equivalent and it is shown that there exists free lunch in the optimization of sums of bivariates. Based on measure-valued extensions of the objective function, so-called relaxations, $\\ell^2$-approximation, and entropy-regularization, we derive several tractable problem formulations solvable with linear programming, coordinate ascent as well as with closed-form solutions. The limits of applying tractable versions of such relaxations to sums of bivariates are investigated using general results for reconstructing measures from their bivariate marginals. Experiments in which the derived algorithms are applied to random functions, vertex coloring, and signal reconstruction problems provide insights into qualitatively different function classes that can be modeled as sums of bivariates.",
    "authors": [
      "Nils Müller"
    ],
    "categories": [
      "math.OC",
      "cs.CV",
      "stat.ML"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20607v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20607v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.20573v1",
    "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering",
    "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.",
    "authors": [
      "Chenhui Gou",
      "Zilong Chen",
      "Zeyu Wang",
      "Feng Li",
      "Deyao Zhu",
      "Zicheng Duan",
      "Kunchang Li",
      "Chaorui Deng",
      "Hongyi Yuan",
      "Haoqi Fan",
      "Cihang Xie",
      "Jianfei Cai",
      "Hamid Rezatofighi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20573v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20573v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.20207v1",
    "title": "Adaptive SGD with Line-Search and Polyak Stepsizes: Nonconvex Convergence and Accelerated Rates",
    "summary": "We extend the convergence analysis of AdaSLS and AdaSPS in [Jiang and Stich, 2024] to the nonconvex setting, presenting a unified convergence analysis of stochastic gradient descent with adaptive Armijo line-search (AdaSLS) and Polyak stepsize (AdaSPS) for nonconvex optimization. Our contributions include: (1) an $\\mathcal{O}(1/\\sqrt{T})$ convergence rate for general nonconvex smooth functions, (2) an $\\mathcal{O}(1/T)$ rate under quasar-convexity and interpolation, and (3) an $\\mathcal{O}(1/T)$ rate under the strong growth condition for general nonconvex functions.",
    "authors": [
      "Haotian Wu"
    ],
    "categories": [
      "math.OC",
      "stat.ML"
    ],
    "published": "2025-11-25",
    "url": "https://arxiv.org/abs/2511.20207v1",
    "pdf_url": "https://arxiv.org/pdf/2511.20207v1.pdf",
    "date": "2025-11-26",
    "source": "arxiv",
    "research_score": 0.4
  }
]