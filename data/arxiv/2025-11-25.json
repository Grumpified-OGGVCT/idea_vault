[
  {
    "arxiv_id": "2511.19344v1",
    "title": "Annotation-Free Class-Incremental Learning",
    "summary": "Despite significant progress in continual learning ranging from architectural novelty to clever strategies for mitigating catastrophic forgetting most existing methods rest on a strong but unrealistic assumption the availability of labeled data throughout the learning process. In real-world scenarios, however, data often arrives sequentially and without annotations, rendering conventional approaches impractical. In this work, we revisit the fundamental assumptions of continual learning and ask: Can current systems adapt when labels are absent and tasks emerge incrementally over time? To this end, we introduce Annotation-Free Class-Incremental Learning (AFCIL), a more realistic and challenging paradigm where unlabeled data arrives continuously, and the learner must incrementally acquire new classes without any supervision. To enable effective learning under AFCIL, we propose CrossWorld CL, a Cross Domain World Guided Continual Learning framework that incorporates external world knowledge as a stable auxiliary source. The method retrieves semantically related ImageNet classes for each downstream category, maps downstream and ImageNet features through a cross domain alignment strategy and finally introduce a novel replay strategy. This design lets the model uncover semantic structure without annotations while keeping earlier knowledge intact. Across four datasets, CrossWorld-CL surpasses CLIP baselines and existing continual and unlabeled learning methods, underscoring the benefit of world knowledge for annotation free continual learning.",
    "authors": [
      "Hari Chandana Kuchibhotla",
      "K S Ananth",
      "Vineeth N Balasubramanian"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19344v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19344v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.96
  },
  {
    "arxiv_id": "2511.19367v1",
    "title": "An Anatomy Aware Hybrid Deep Learning Framework for Lung Cancer Tumor Stage Classification",
    "summary": "Accurate lung cancer tumor staging is crucial for prognosis and treatment planning. However, it remains challenging for end-to-end deep learning approaches, as such approaches often overlook spatial and anatomical information that are central to the tumor-node-metastasis system. The tumor stage depends on multiple quantitative criteria, including the tumor size and its proximity to the nearest anatomical structures, and small variations can alter the staging outcome. We propose a medically grounded hybrid pipeline that performs staging by explicitly measuring the tumor's size and distance properties rather than treating it as a pure image classification task. Our method employs specialized encoder-decoder networks to precisely segment the lung and adjacent anatomy, including the lobes, tumor, mediastinum, and diaphragm. Subsequently, we extract the necessary tumor properties, i.e. measure the largest tumor dimension and calculate the distance between the tumor and neighboring anatomical structures by a quantitative analysis of the segmentation masks. Finally, we apply rule-based tumor staging aligned with the medical guidelines. This novel framework has been evaluated on the Lung-PET-CT-Dx dataset, demonstrating superior performance compared to traditional deep learning models, achieving an overall classification accuracy of 91.36%. We report the per-stage F1-scores of 0.93 (T1), 0.89 (T2), 0.96 (T3), and 0.90 (T4), a critical evaluation aspect often omitted in prior literature. To our knowledge, this is the first study that embeds explicit clinical context into tumor stage classification. Unlike standard convolutional neural networks that operate in an uninterpretable \"black box\" manner, our method offers both state-of-the-art performance and transparent decision support.",
    "authors": [
      "Saniah Kayenat Chowdhury",
      "Rusab Sarmun",
      "Muhammad E. H. Chowdhury",
      "Sohaib Bassam Zoghoul",
      "Israa Al-Hashimi",
      "Adam Mushtak",
      "Amith Khandakar"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19367v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19367v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.94
  },
  {
    "arxiv_id": "2511.19150v1",
    "title": "Feature Ranking in Credit-Risk with Qudit-Based Networks",
    "summary": "In finance, predictive models must balance accuracy and interpretability, particularly in credit risk assessment, where model decisions carry material consequences. We present a quantum neural network (QNN) based on a single qudit, in which both data features and trainable parameters are co-encoded within a unified unitary evolution generated by the full Lie algebra. This design explores the entire Hilbert space while enabling interpretability through the magnitudes of the learned coefficients. We benchmark our model on a real-world, imbalanced credit-risk dataset from Taiwan. The proposed QNN consistently outperforms LR and reaches the results of random forest models in macro-F1 score while preserving a transparent correspondence between learned parameters and input feature importance. To quantify the interpretability of the proposed model, we introduce two complementary metrics: (i) the edit distance between the model's feature ranking and that of LR, and (ii) a feature-poisoning test where selected features are replaced with noise. Results indicate that the proposed quantum model achieves competitive performance while offering a tractable path toward interpretable quantum learning.",
    "authors": [
      "Georgios Maragkopoulos",
      "Lazaros Chavatzoglou",
      "Aikaterini Mandilara",
      "Dimitris Syvridis"
    ],
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19150v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19150v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.89
  },
  {
    "arxiv_id": "2511.19418v1",
    "title": "Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens",
    "summary": "Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.",
    "authors": [
      "Yiming Qin",
      "Bomin Wei",
      "Jiaxin Ge",
      "Konstantinos Kallidromitis",
      "Stephanie Fu",
      "Trevor Darrell",
      "Xudong Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19418v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19418v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.88
  },
  {
    "arxiv_id": "2511.19404v1",
    "title": "Nonparametric Instrumental Variable Regression with Observed Covariates",
    "summary": "We study the problem of nonparametric instrumental variable regression with observed covariates, which we refer to as NPIV-O. Compared with standard nonparametric instrumental variable regression (NPIV), the additional observed covariates facilitate causal identification and enables heterogeneous causal effect estimation. However, the presence of observed covariates introduces two challenges for its theoretical analysis. First, it induces a partial identity structure, which renders previous NPIV analyses - based on measures of ill-posedness, stability conditions, or link conditions - inapplicable. Second, it imposes anisotropic smoothness on the structural function. To address the first challenge, we introduce a novel Fourier measure of partial smoothing; for the second challenge, we extend the existing kernel 2SLS instrumental variable algorithm with observed covariates, termed KIV-O, to incorporate Gaussian kernel lengthscales adaptive to the anisotropic smoothness. We prove upper $L^2$-learning rates for KIV-O and the first $L^2$-minimax lower learning rates for NPIV-O. Both rates interpolate between known optimal rates of NPIV and nonparametric regression (NPR). Interestingly, we identify a gap between our upper and lower bounds, which arises from the choice of kernel lengthscales tuned to minimize a projected risk. Our theoretical analysis also applies to proximal causal inference, an emerging framework for causal effect estimation that shares the same conditional moment restriction as NPIV-O.",
    "authors": [
      "Zikai Shen",
      "Zonghao Chen",
      "Dimitri Meunier",
      "Ingo Steinwart",
      "Arthur Gretton",
      "Zhu Li"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19404v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19404v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.86
  },
  {
    "arxiv_id": "2511.19277v1",
    "title": "Closing Gaps in Emissions Monitoring with Climate TRACE",
    "summary": "Global greenhouse gas emissions estimates are essential for monitoring and mitigation planning. Yet most datasets lack one or more characteristics that enhance their actionability, such as accuracy, global coverage, high spatial and temporal resolution, and frequent updates. To address these gaps, we present Climate TRACE (climatetrace.org), an open-access platform delivering global emissions estimates with enhanced detail, coverage, and timeliness. Climate TRACE synthesizes existing emissions data, prioritizing accuracy, coverage, and resolution, and fills gaps using sector-specific estimation approaches. The dataset is the first to provide globally comprehensive emissions estimates for individual sources (e.g., individual power plants) for all anthropogenic emitting sectors. The dataset spans January 1, 2021, to the present, with a two-month reporting lag and monthly updates. The open-access platform enables non-technical audiences to engage with detailed emissions datasets for most subnational governments worldwide. Climate TRACE supports data-driven climate action at scales where decisions are made, representing a major breakthrough for emissions accounting and mitigation.",
    "authors": [
      "Brittany V. Lancellotti",
      "Jordan M. Malof",
      "Aaron Davitt",
      "Gavin McCormick",
      "Shelby Anderson",
      "Pol Carbó-Mestre",
      "Gary Collins",
      "Verity Crane",
      "Zoheyr Doctor",
      "George Ebri",
      "Kevin Foster",
      "Trey M. Gowdy",
      "Michael Guzzardi",
      "John Heal",
      "Heather Hunter",
      "David Kroodsma",
      "Khandekar Mahammad Galib",
      "Paul J. Markakis",
      "Gavin McDonald",
      "Daniel P. Moore",
      "Eric D. Nguyen",
      "Sabina Parvu",
      "Michael Pekala",
      "Christine D. Piatko",
      "Amy Piscopo",
      "Mark Powell",
      "Krsna Raniga",
      "Elizabeth P. Reilly",
      "Michael Robinette",
      "Ishan Saraswat",
      "Patrick Sicurello",
      "Isabella Söldner-Rembold",
      "Raymond Song",
      "Charlotte Underwood",
      "Kyle Bradbury"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19277v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19277v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.84
  },
  {
    "arxiv_id": "2511.19405v1",
    "title": "Learning Robust Social Strategies with Large Language Models",
    "summary": "As agentic AI becomes more widespread, agents with distinct and possibly conflicting goals will interact in complex ways. These multi-agent interactions pose a fundamental challenge, particularly in social dilemmas, where agents' individual incentives can undermine collective welfare. While reinforcement learning (RL) has been effective for aligning large language models (LLMs) in the single-agent regime, prior small-network results suggest that standard RL in multi-agent settings often converges to defecting, self-interested policies. We show the same effect in LLMs: despite cooperative priors, RL-trained LLM agents develop opportunistic behavior that can exploit even advanced closed-source models. To address this tendency of RL to converge to poor equilibria, we adapt a recent opponent-learning awareness algorithm, Advantage Alignment, to fine-tune LLMs toward multi-agent cooperation and non-exploitability. We then introduce a group-relative baseline that simplifies advantage computation in iterated games, enabling multi-agent training at LLM scale. We also contribute a novel social dilemma environment, Trust and Split, which requires natural language communication to achieve high collective welfare. Across a wide range of social dilemmas, policies learned with Advantage Alignment achieve higher collective payoffs while remaining robust against exploitation by greedy agents.",
    "authors": [
      "Dereck Piche",
      "Mohammed Muqeeth",
      "Milad Aghajohari",
      "Juan Duque",
      "Michael Noukhovitch",
      "Aaron Courville"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19405v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19405v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.82
  },
  {
    "arxiv_id": "2511.19272v1",
    "title": "Tiny-TSM: Efficiently Training a Lightweight SOTA Time Series Foundation Model",
    "summary": "We present Tiny-TSM, a time series foundation model characterized by small scale, economical training, and state-of-the-art performance. It comprises 23M total parameters, trained on a single A100 GPU in less than a week using a new synthetic data generation and data augmentation pipeline (SynthTS). Without any neural architecture search, hyperparameter tuning, or scaling up model size, Tiny-TSM achieves state-of-the-art performance on a wide range of time series benchmark datasets, often outperforming much larger models and even matching the performance of much larger, industrial-scale, likely highly tuned foundation models. Specifically, Tiny-TSM outperforms all other time series foundation models we evaluated on medium- and long-term forecasting tasks under MSE loss, while short-term accuracy is still competitive with state-of-the-art models.   We also introduce a causal input normalization scheme that enables time series models to be trained with dense next-token prediction loss, significantly accelerating convergence speed and reducing training time.   All experiments were conducted on a single A100 GPU, illustrating the practicality of the proposed approach in a resource-constrained setting.",
    "authors": [
      "Felix Birkel"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19272v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19272v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.19066v1",
    "title": "Mitigating Participation Imbalance Bias in Asynchronous Federated Learning",
    "summary": "In Asynchronous Federated Learning (AFL), the central server immediately updates the global model with each arriving client's contribution. As a result, clients perform their local training on different model versions, causing information staleness (delay). In federated environments with non-IID local data distributions, this asynchronous pattern amplifies the adverse effect of client heterogeneity (due to different data distribution, local objectives, etc.), as faster clients contribute more frequent updates, biasing the global model. We term this phenomenon heterogeneity amplification. Our work provides a theoretical analysis that maps AFL design choices to their resulting error sources when heterogeneity amplification occurs. Guided by our analysis, we propose ACE (All-Client Engagement AFL), which mitigates participation imbalance through immediate, non-buffered updates that use the latest information available from all clients. We also introduce a delay-aware variant, ACED, to balance client diversity against update staleness. Experiments on different models for different tasks across diverse heterogeneity and delay settings validate our analysis and demonstrate the robust performance of our approaches.",
    "authors": [
      "Xiangyu Chang",
      "Manyi Yao",
      "Srikanth V. Krishnamurthy",
      "Christian R. Shelton",
      "Anirban Chakraborty",
      "Ananthram Swami",
      "Samet Oymak",
      "Amit Roy-Chowdhury"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19066v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19066v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2511.19200v1",
    "title": "Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?",
    "summary": "Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired \"real\"/\"lookalike\" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.",
    "authors": [
      "Itay Cohen",
      "Ethan Fetaya",
      "Amir Rosenfeld"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19200v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19200v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.19124v1",
    "title": "Uncertainty-Aware Deep Learning Framework for Remaining Useful Life Prediction in Turbofan Engines with Learned Aleatoric Uncertainty",
    "summary": "Accurate Remaining Useful Life (RUL) prediction coupled with uncertainty quantification remains a critical challenge in aerospace prognostics. This research introduces a novel uncertainty-aware deep learning framework that learns aleatoric uncertainty directly through probabilistic modeling, an approach unexplored in existing CMAPSS-based literature. Our hierarchical architecture integrates multi-scale Inception blocks for temporal pattern extraction, bidirectional Long Short-Term Memory networks for sequential modeling, and a dual-level attention mechanism operating simultaneously on sensor and temporal dimensions. The innovation lies in the Bayesian output layer that predicts both mean RUL and variance, enabling the model to learn data-inherent uncertainty. Comprehensive preprocessing employs condition-aware clustering, wavelet denoising, and intelligent feature selection. Experimental validation on NASA CMAPSS benchmarks (FD001-FD004) demonstrates competitive overall performance with RMSE values of 16.22, 19.29, 16.84, and 19.98 respectively. Remarkably, our framework achieves breakthrough critical zone performance (RUL <= 30 cycles) with RMSE of 5.14, 6.89, 5.27, and 7.16, representing 25-40 percent improvements over conventional approaches and establishing new benchmarks for safety-critical predictions. The learned uncertainty provides well-calibrated 95 percent confidence intervals with coverage ranging from 93.5 percent to 95.2 percent, enabling risk-aware maintenance scheduling previously unattainable in CMAPSS literature.",
    "authors": [
      "Krishang Sharma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19124v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19124v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.19078v1",
    "title": "GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.",
    "authors": [
      "Yutong Li",
      "Yitian Zhou",
      "Xudong Wang",
      " GuoChen",
      "Caiyan Qin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19078v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19078v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.19273v1",
    "title": "Scalable Bayesian Network Structure Learning Using Tsetlin Machine to Constrain the Search Space",
    "summary": "The PC algorithm is a widely used method in causal inference for learning the structure of Bayesian networks. Despite its popularity, the PC algorithm suffers from significant time complexity, particularly as the size of the dataset increases, which limits its applicability in large-scale real-world problems. In this study, we propose a novel approach that utilises the Tsetlin Machine (TM) to construct Bayesian structures more efficiently. Our method leverages the most significant literals extracted from the TM and performs conditional independence (CI) tests on these selected literals instead of the full set of variables, resulting in a considerable reduction in computational time. We implemented our approach and compared it with various state-of-the-art methods. Our evaluation includes categorical datasets from the bnlearn repository, such as Munin1, Hepar2. The findings indicate that the proposed TM-based method not only reduces computational complexity but also maintains competitive accuracy in causal discovery, making it a viable alternative to traditional PC algorithm implementations by offering improved efficiency without compromising performance.",
    "authors": [
      "Kunal Dumbre",
      "Lei Jiao",
      "Ole-Christoffer Granmo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19273v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19273v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.19257v1",
    "title": "Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation",
    "summary": "With the rapid advancement of retrieval-augmented vision-language models, multimodal medical retrieval-augmented generation (MMed-RAG) systems are increasingly adopted in clinical decision support. These systems enhance medical applications by performing cross-modal retrieval to integrate relevant visual and textual evidence for tasks, e.g., report generation and disease diagnosis. However, their complex architecture also introduces underexplored adversarial vulnerabilities, particularly via visual input perturbations. In this paper, we propose Medusa, a novel framework for crafting cross-modal transferable adversarial attacks on MMed-RAG systems under a black-box setting. Specifically, Medusa formulates the attack as a perturbation optimization problem, leveraging a multi-positive InfoNCE loss (MPIL) to align adversarial visual embeddings with medically plausible but malicious textual targets, thereby hijacking the retrieval process. To enhance transferability, we adopt a surrogate model ensemble and design a dual-loop optimization strategy augmented with invariant risk minimization (IRM). Extensive experiments on two real-world medical tasks, including medical report generation and disease diagnosis, demonstrate that Medusa achieves over 90% average attack success rate across various generation models and retrievers under appropriate parameter configuration, while remaining robust against four mainstream defenses, outperforming state-of-the-art baselines. Our results reveal critical vulnerabilities in the MMed-RAG systems and highlight the necessity of robustness benchmarking in safety-critical medical applications. The code and data are available at https://anonymous.4open.science/r/MMed-RAG-Attack-F05A.",
    "authors": [
      "Yingjia Shang",
      "Yi Liu",
      "Huimin Wang",
      "Furong Li",
      "Wenfang Sun",
      "Wu Chengyu",
      "Yefeng Zheng"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19257v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19257v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.18950v1",
    "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation",
    "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.",
    "authors": [
      "Juntao Gao",
      "Feiyang Ye",
      "Jing Zhang",
      "Wenjing Qian"
    ],
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18950v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18950v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.19218v1",
    "title": "Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization",
    "summary": "Large Language Models (LLMs) have developed rapidly in web services, delivering unprecedented capabilities while amplifying societal risks. Existing works tend to focus on either isolated jailbreak attacks or static defenses, neglecting the dynamic interplay between evolving threats and safeguards in real-world web contexts. To mitigate these challenges, we propose ACE-Safety (Adversarial Co-Evolution for LLM Safety), a novel framework that jointly optimize attack and defense models by seamlessly integrating two key innovative procedures: (1) Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS), which efficiently explores jailbreak strategies to uncover vulnerabilities and generate diverse adversarial samples; (2) Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO), which jointly trains attack and defense LLMs with challenging samples via curriculum reinforcement learning, enabling robust mutual improvement. Evaluations across multiple benchmarks demonstrate that our method outperforms existing attack and defense approaches, and provides a feasible pathway for developing LLMs that can sustainably support responsible AI ecosystems.",
    "authors": [
      "Xurui Li",
      "Kaisong Song",
      "Rui Zhu",
      "Pin-Yu Chen",
      "Haixu Tang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19218v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19218v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.18960v1",
    "title": "AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention",
    "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.",
    "authors": [
      "Lei Xiao",
      "Jifeng Li",
      "Juntao Gao",
      "Feiyang Ye",
      "Yan Jin",
      "Jingjing Qian",
      "Jing Zhang",
      "Yong Wu",
      "Xiaoyuan Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18960v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18960v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.19428v1",
    "title": "Flow Map Distillation Without Data",
    "summary": "State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.",
    "authors": [
      "Shangyuan Tong",
      "Nanye Ma",
      "Saining Xie",
      "Tommi Jaakkola"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19428v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19428v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19425v1",
    "title": "SAM3-Adapter: Efficient Adaptation of Segment Anything 3 for Camouflage Object Segmentation, Shadow Detection, and Medical Image Segmentation",
    "summary": "The rapid rise of large-scale foundation models has reshaped the landscape of image segmentation, with models such as Segment Anything achieving unprecedented versatility across diverse vision tasks. However, previous generations-including SAM and its successor-still struggle with fine-grained, low-level segmentation challenges such as camouflaged object detection, medical image segmentation, cell image segmentation, and shadow detection. To address these limitations, we originally proposed SAM-Adapter in 2023, demonstrating substantial gains on these difficult scenarios. With the emergence of Segment Anything 3 (SAM3)-a more efficient and higher-performing evolution with a redesigned architecture and improved training pipeline-we revisit these long-standing challenges. In this work, we present SAM3-Adapter, the first adapter framework tailored for SAM3 that unlocks its full segmentation capability. SAM3-Adapter not only reduces computational overhead but also consistently surpasses both SAM and SAM2-based solutions, establishing new state-of-the-art results across multiple downstream tasks, including medical imaging, camouflaged (concealed) object segmentation, and shadow detection. Built upon the modular and composable design philosophy of the original SAM-Adapter, SAM3-Adapter provides stronger generalizability, richer task adaptability, and significantly improved segmentation precision. Extensive experiments confirm that integrating SAM3 with our adapter yields superior accuracy, robustness, and efficiency compared to all prior SAM-based adaptations. We hope SAM3-Adapter can serve as a foundation for future research and practical segmentation applications. Code, pre-trained models, and data processing pipelines are available.",
    "authors": [
      "Tianrun Chen",
      "Runlong Cao",
      "Xinda Yu",
      "Lanyun Zhu",
      "Chaotao Ding",
      "Deyi Ji",
      "Cheng Chen",
      "Qi Zhu",
      "Chunyan Xu",
      "Papa Mao",
      "Ying Zang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19425v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19425v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19422v1",
    "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning",
    "summary": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.",
    "authors": [
      "David Jiahao Fu",
      "Aryan Gupta",
      "Aaron Councilman",
      "David Grove",
      "Yu-Xiong Wang",
      "Vikram Adve"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19422v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19422v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19304v1",
    "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
    "summary": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
    "authors": [
      "Jiayi Zhang",
      "Yiran Peng",
      "Fanqi Kong",
      "Yang Cheng",
      "Yifan Wu",
      "Zhaoyang Yu",
      "Jinyu Xiang",
      "Jianhao Ruan",
      "Jinlin Wang",
      "Maojia Song",
      "HongZhang Liu",
      "Xiangru Tang",
      "Bang Liu",
      "Chenglin Wu",
      "Yuyu Luo"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19304v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19304v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19294v1",
    "title": "DensifyBeforehand: LiDAR-assisted Content-aware Densification for Efficient and Quality 3D Gaussian Splatting",
    "summary": "This paper addresses the limitations of existing 3D Gaussian Splatting (3DGS) methods, particularly their reliance on adaptive density control, which can lead to floating artifacts and inefficient resource usage. We propose a novel densify beforehand approach that enhances the initialization of 3D scenes by combining sparse LiDAR data with monocular depth estimation from corresponding RGB images. Our ROI-aware sampling scheme prioritizes semantically and geometrically important regions, yielding a dense point cloud that improves visual fidelity and computational efficiency. This densify beforehand approach bypasses the adaptive density control that may introduce redundant Gaussians in the original pipeline, allowing the optimization to focus on the other attributes of 3D Gaussian primitives, reducing overlap while enhancing visual quality. Our method achieves comparable results to state-of-the-art techniques while significantly lowering resource consumption and training time. We validate our approach through extensive comparisons and ablation studies on four newly collected datasets, showcasing its effectiveness in preserving regions of interest in complex scenes.",
    "authors": [
      "Phurtivilai Patt",
      "Leyang Huang",
      "Yinqiang Zhang",
      "Yang Lei"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19294v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19294v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19256v1",
    "title": "SimDiff: Simpler Yet Better Diffusion Model for Time Series Point Forecasting",
    "summary": "Diffusion models have recently shown promise in time series forecasting, particularly for probabilistic predictions. However, they often fail to achieve state-of-the-art point estimation performance compared to regression-based methods. This limitation stems from difficulties in providing sufficient contextual bias to track distribution shifts and in balancing output diversity with the stability and precision required for point forecasts. Existing diffusion-based approaches mainly focus on full-distribution modeling under probabilistic frameworks, often with likelihood maximization objectives, while paying little attention to dedicated strategies for high-accuracy point estimation. Moreover, other existing point prediction diffusion methods frequently rely on pre-trained or jointly trained mature models for contextual bias, sacrificing the generative flexibility of diffusion models.   To address these challenges, we propose SimDiff, a single-stage, end-to-end framework. SimDiff employs a single unified Transformer network carefully tailored to serve as both denoiser and predictor, eliminating the need for external pre-trained or jointly trained regressors. It achieves state-of-the-art point estimation performance by leveraging intrinsic output diversity and improving mean squared error accuracy through multiple inference ensembling. Key innovations, including normalization independence and the median-of-means estimator, further enhance adaptability and stability. Extensive experiments demonstrate that SimDiff significantly outperforms existing methods in time series point forecasting.",
    "authors": [
      "Hang Ding",
      "Xue Wang",
      "Tian Zhou",
      "Tao Yao"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19256v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19256v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19147v1",
    "title": "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation",
    "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data. Recent advances in Foundation Models (FMs) have introduced new opportunities for leveraging external semantic knowledge to guide SFDA. However, relying on a single FM is often insufficient, as it tends to bias adaptation toward a restricted semantic coverage, failing to capture diverse contextual cues under domain shift. To overcome this limitation, we propose a Collaborative Multi-foundation Adaptation (CoMA) framework that jointly leverages two different FMs (e.g., CLIP and BLIP) with complementary properties to capture both global semantics and local contextual cues. Specifically, we employ a bidirectional adaptation mechanism that (1) aligns different FMs with the target model for task adaptation while maintaining their semantic distinctiveness, and (2) transfers complementary knowledge from the FMs to the target model. To ensure stable adaptation under mini-batch training, we introduce Decomposed Mutual Information (DMI) that selectively enhances true dependencies while suppressing false dependencies arising from incomplete class coverage. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art SFDA methods across four benchmarks, including Office-31, Office-Home, DomainNet-126, and VisDA, under the closed-set setting, while also achieving best results on partial-set and open-set variants.",
    "authors": [
      "Huisoo Lee",
      "Jisu Han",
      "Hyunsouk Cho",
      "Wonjun Hwang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19147v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19147v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19067v1",
    "title": "DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling",
    "summary": "Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.",
    "authors": [
      "Timur Mamedov",
      "Anton Konushin",
      "Vadim Konushin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19067v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19067v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19009v1",
    "title": "Understanding and Mitigating Over-refusal for Large Language Models via Safety Representation",
    "summary": "Large language models demonstrate powerful capabilities across various natural language processing tasks, yet they also harbor safety vulnerabilities. To enhance LLM safety, various jailbreak defense methods have been proposed to guard against harmful outputs. However, improvements in model safety often come at the cost of severe over-refusal, failing to strike a good balance between safety and usability. In this paper, we first analyze the causes of over-refusal from a representation perspective, revealing that over-refusal samples reside at the boundary between benign and malicious samples. Based on this, we propose MOSR, designed to mitigate over-refusal by intervening the safety representation of LLMs. MOSR incorporates two novel components: (1) Overlap-Aware Loss Weighting, which determines the erasure weight for malicious samples by quantifying their similarity to pseudo-malicious samples in the representation space, and (2) Context-Aware Augmentation, which supplements the necessary context for rejection decisions by adding harmful prefixes before rejection responses. Experiments demonstrate that our method outperforms existing approaches in mitigating over-refusal while largely maintaining safety. Overall, we advocate that future defense methods should strike a better balance between safety and over-refusal.",
    "authors": [
      "Junbo Zhang",
      "Ran Chen",
      "Qianli Zhou",
      "Xinyang Deng",
      "Wen Jiang"
    ],
    "categories": [
      "cs.CR",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19009v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19009v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.19368v1",
    "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems",
    "summary": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.",
    "authors": [
      "Tianyang Duan",
      "Zongyuan Zhang",
      "Zheng Lin",
      "Songxiao Guo",
      "Xiuxian Guan",
      "Guangyu Wu",
      "Zihan Fang",
      "Haotian Meng",
      "Xia Du",
      "Ji-Zhe Zhou",
      "Heming Cui",
      "Jun Luo",
      "Yue Gao"
    ],
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19368v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19368v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.19046v1",
    "title": "MedSAM3: Delving into Segment Anything with Medical Concepts",
    "summary": "Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.",
    "authors": [
      "Anglin Liu",
      "Rundong Xue",
      "Xu R. Cao",
      "Yifan Shen",
      "Yi Lu",
      "Xiang Li",
      "Qianqian Chen",
      "Jintai Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19046v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19046v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.18977v1",
    "title": "FastForward Pruning: Efficient LLM Pruning via Single-Step Reinforcement Learning",
    "summary": "Pruning is an effective method for compressing Large Language Models, but finding an optimal, non-uniform layer-wise sparsity allocation remains a key challenge. While heuristic methods are fast but yield suboptimal performance, more powerful search-based approaches like Reinforcement Learning are often hindered by prohibitive computational costs on large-scale models. To overcome this efficiency barrier, we propose FastForward Pruning. Its core is a decoupled, single-step RL framework that separates policy optimization from the complex budget satisfaction problem. Such a decoupling is crucial for efficiently searching the vast policy space of LLMs. This curriculum-based strategy begins with low-cost, simple tasks and gradually increases in complexity, significantly reducing the search's computational overhead. Evaluated on the LLaMA, Mistral, and OPT model families, our framework discovers pruning policies that achieve superior performance over strong heuristic baselines. Crucially, when compared to other search-based algorithms, our method achieves competitive or superior results at a fraction of the computational cost, demonstrating a clear advantage in search efficiency.",
    "authors": [
      "Xin Yuan",
      "Siqi Li",
      "Jiateng Wei",
      "Chengrui Zhu",
      "Yanming Wu",
      "Qingpeng Li",
      "Jiajun Lv",
      "Xiaoke Lan",
      "Jun Chen",
      "Yong Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18977v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18977v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.19413v1",
    "title": "UniGame: Turning a Unified Multimodal Model Into Its Own Adversary",
    "summary": "Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame",
    "authors": [
      "Zhaolong Su",
      "Wang Lu",
      "Hao Chen",
      "Sharon Li",
      "Jindong Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19413v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19413v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.19379v1",
    "title": "Efficiency vs. Fidelity: A Comparative Analysis of Diffusion Probabilistic Models and Flow Matching on Low-Resource Hardware",
    "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have established a new state-of-the-art in generative image synthesis, yet their deployment is hindered by significant computational overhead during inference, often requiring up to 1,000 iterative steps. This study presents a rigorous comparative analysis of DDPMs against the emerging Flow Matching (Rectified Flow) paradigm, specifically isolating their geometric and efficiency properties on low-resource hardware. By implementing both frameworks on a shared Time-Conditioned U-Net backbone using the MNIST dataset, we demonstrate that Flow Matching significantly outperforms Diffusion in efficiency. Our geometric analysis reveals that Flow Matching learns a highly rectified transport path (Curvature $\\mathcal{C} \\approx 1.02$), which is near-optimal, whereas Diffusion trajectories remain stochastic and tortuous ($\\mathcal{C} \\approx 3.45$). Furthermore, we establish an ``efficiency frontier'' at $N=10$ function evaluations, where Flow Matching retains high fidelity while Diffusion collapses. Finally, we show via numerical sensitivity analysis that the learned vector field is sufficiently linear to render high-order ODE solvers (Runge-Kutta 4) unnecessary, validating the use of lightweight Euler solvers for edge deployment. \\textbf{This work concludes that Flow Matching is the superior algorithmic choice for real-time, resource-constrained generative tasks.}",
    "authors": [
      "Srishti Gupta",
      "Yashasvee Taiwade"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19379v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19379v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.19131v1",
    "title": "Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization",
    "summary": "Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.",
    "authors": [
      "Zijian Wang",
      "Yanxiang Ma",
      "Chang Xu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19131v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19131v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.19417v1",
    "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.",
    "authors": [
      "James Y. Huang",
      "Sheng Zhang",
      "Qianchu Liu",
      "Guanghui Qin",
      "Tinghui Zhu",
      "Tristan Naumann",
      "Muhao Chen",
      "Hoifung Poon"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19417v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19417v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19359v1",
    "title": "Enhancing Conformal Prediction via Class Similarity",
    "summary": "Conformal Prediction (CP) has emerged as a powerful statistical framework for high-stakes classification applications. Instead of predicting a single class, CP generates a prediction set, guaranteed to include the true label with a pre-specified probability. The performance of different CP methods is typically assessed by their average prediction set size. In setups where the classes can be partitioned into semantic groups, e.g., diseases that require similar treatment, users can benefit from prediction sets that are not only small on average, but also contain a small number of semantically different groups. This paper begins by addressing this problem and ultimately offers a widely applicable tool for boosting any CP method on any dataset. First, given a class partition, we propose augmenting the CP score function with a term that penalizes predictions with out-of-group errors. We theoretically analyze this strategy and prove its advantages for group-related metrics. Surprisingly, we show mathematically that, for common class partitions, it can also reduce the average set size of any CP score function. Our analysis reveals the class similarity factors behind this improvement and motivates us to propose a model-specific variant, which does not require any human semantic partition and can further reduce the prediction set size. Finally, we present an extensive empirical study, encompassing prominent CP methods, multiple models, and several datasets, which demonstrates that our class-similarity-based approach consistently enhances CP methods.",
    "authors": [
      "Ariel Fargion",
      "Lahav Dabah",
      "Tom Tirer"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19359v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19359v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19355v1",
    "title": "Leveraging LLMs for reward function design in reinforcement learning control tasks",
    "summary": "The challenge of designing effective reward functions in reinforcement learning (RL) represents a significant bottleneck, often requiring extensive human expertise and being time-consuming. Previous work and recent advancements in large language models (LLMs) have demonstrated their potential for automating the generation of reward functions. However, existing methodologies often require preliminary evaluation metrics, human-engineered feedback for the refinement process, or the use of environmental source code as context. To address these limitations, this paper introduces LEARN-Opt (LLM-based Evaluator and Analyzer for Reward functioN Optimization). This LLM-based, fully autonomous, and model-agnostic framework eliminates the need for preliminary metrics and environmental source code as context to generate, execute, and evaluate reward function candidates from textual descriptions of systems and task objectives. LEARN-Opt's main contribution lies in its ability to autonomously derive performance metrics directly from the system description and the task objective, enabling unsupervised evaluation and selection of reward functions. Our experiments indicate that LEARN-Opt achieves performance comparable to or better to that of state-of-the-art methods, such as EUREKA, while requiring less prior knowledge. We find that automated reward design is a high-variance problem, where the average-case candidate fails, requiring a multi-run approach to find the best candidates. Finally, we show that LEARN-Opt can unlock the potential of low-cost LLMs to find high-performing candidates that are comparable to, or even better than, those of larger models. This demonstrated performance affirms its potential to generate high-quality reward functions without requiring any preliminary human-defined metrics, thereby reducing engineering overhead and enhancing generalizability.",
    "authors": [
      "Franklin Cardenoso",
      "Wouter Caarls"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19355v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19355v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19350v1",
    "title": "Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric",
    "summary": "Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.",
    "authors": [
      "Nikita Neveditsin",
      "Pawan Lingras",
      "Vijay Mago"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19350v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19350v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19342v1",
    "title": "Explicit Tonal Tension Conditioning via Dual-Level Beam Search for Symbolic Music Generation",
    "summary": "State-of-the-art symbolic music generation models have recently achieved remarkable output quality, yet explicit control over compositional features, such as tonal tension, remains challenging. We propose a novel approach that integrates a computational tonal tension model, based on tonal interval vector analysis, into a Transformer framework. Our method employs a two-level beam search strategy during inference. At the token level, generated candidates are re-ranked using model probability and diversity metrics to maintain overall quality. At the bar level, a tension-based re-ranking is applied to ensure that the generated music aligns with a desired tension curve. Objective evaluations indicate that our approach effectively modulates tonal tension, and subjective listening tests confirm that the system produces outputs that align with the target tension. These results demonstrate that explicit tension conditioning through a dual-level beam search provides a powerful and intuitive tool to guide AI-generated music. Furthermore, our experiments demonstrate that our method can generate multiple distinct musical interpretations under the same tension condition.",
    "authors": [
      "Maral Ebrahimzadeh",
      "Gilberto Bernardes",
      "Sebastian Stober"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19342v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19342v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19299v1",
    "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning",
    "summary": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.",
    "authors": [
      "James R. M. Black",
      "Moritz S. Hanke",
      "Aaron Maiwald",
      "Tina Hernandez-Boussard",
      "Oliver M. Crook",
      "Jaspreet Pannu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19299v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19299v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19246v1",
    "title": "Neural Architecture Search for Quantum Autoencoders",
    "summary": "In recent years, machine learning and deep learning have driven advances in domains such as image classification, speech recognition, and anomaly detection by leveraging multi-layer neural networks to model complex data. Simultaneously, quantum computing (QC) promises to address classically intractable problems via quantum parallelism, motivating research in quantum machine learning (QML). Among QML techniques, quantum autoencoders show promise for compressing high-dimensional quantum and classical data. However, designing effective quantum circuit architectures for quantum autoencoders remains challenging due to the complexity of selecting gates, arranging circuit layers, and tuning parameters.   This paper proposes a neural architecture search (NAS) framework that automates the design of quantum autoencoders using a genetic algorithm (GA). By systematically evolving variational quantum circuit (VQC) configurations, our method seeks to identify high-performing hybrid quantum-classical autoencoders for data reconstruction without becoming trapped in local minima. We demonstrate effectiveness on image datasets, highlighting the potential of quantum autoencoders for efficient feature extraction within a noise-prone, near-term quantum era. Our approach lays a foundation for broader application of genetic algorithms to quantum architecture search, aiming for a robust, automated method that can adapt to varied data and hardware constraints.",
    "authors": [
      "Hibah Agha",
      "Samuel Yen-Chi Chen",
      "Huan-Hsin Tseng",
      "Shinjae Yoo"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19246v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19246v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19240v1",
    "title": "Empirical Comparison of Forgetting Mechanisms for UCB-based Algorithms on a Data-Driven Simulation Platform",
    "summary": "Many real-world bandit problems involve non-stationary reward distributions, where the optimal decision may shift due to evolving environments. However, the performance of some typical Multi-Armed Bandit (MAB) models such as Upper Confidence Bound (UCB) algorithms degrades significantly in non-stationary environments where reward distributions change over time. To address this limitation, this paper introduces and evaluates FDSW-UCB, a novel dual-view algorithm that integrates a discount-based long-term perspective with a sliding-window-based short-term view. A data-driven semi-synthetic simulation platform, built upon the MovieLens-1M and Open Bandit datasets, is developed to test algorithm adaptability under abrupt and gradual drift scenarios. Experimental results demonstrate that a well-configured sliding-window mechanism (SW-UCB) is robust, while the widely used discounting method (D-UCB) suffers from a fundamental learning failure, leading to linear regret. Crucially, the proposed FDSW-UCB, when employing an optimistic aggregation strategy, achieves superior performance in dynamic settings, highlighting that the ensemble strategy itself is a decisive factor for success.",
    "authors": [
      "Minxin Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19240v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19240v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19172v1",
    "title": "MetroGS: Efficient and Stable Reconstruction of Geometrically Accurate High-Fidelity Large-Scale Scenes",
    "summary": "Recently, 3D Gaussian Splatting and its derivatives have achieved significant breakthroughs in large-scale scene reconstruction. However, how to efficiently and stably achieve high-quality geometric fidelity remains a core challenge. To address this issue, we introduce MetroGS, a novel Gaussian Splatting framework for efficient and robust reconstruction in complex urban environments. Our method is built upon a distributed 2D Gaussian Splatting representation as the core foundation, serving as a unified backbone for subsequent modules. To handle potential sparse regions in complex scenes, we propose a structured dense enhancement scheme that utilizes SfM priors and a pointmap model to achieve a denser initialization, while incorporating a sparsity compensation mechanism to improve reconstruction completeness. Furthermore, we design a progressive hybrid geometric optimization strategy that organically integrates monocular and multi-view optimization to achieve efficient and accurate geometric refinement. Finally, to address the appearance inconsistency commonly observed in large-scale scenes, we introduce a depth-guided appearance modeling approach that learns spatial features with 3D consistency, facilitating effective decoupling between geometry and appearance and further enhancing reconstruction stability. Experiments on large-scale urban datasets demonstrate that MetroGS achieves superior geometric accuracy, rendering quality, offering a unified solution for high-fidelity large-scale scene reconstruction.",
    "authors": [
      "Kehua Chen",
      "Tianlu Mao",
      "Zhuxin Ma",
      "Hao Jiang",
      "Zehao Li",
      "Zihan Liu",
      "Shuqi Gao",
      "Honglong Zhao",
      "Feng Dai",
      "Yucheng Zhang",
      "Zhaoqi Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19172v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19172v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19155v1",
    "title": "EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction",
    "summary": "Sleep stage classification based on electroencephalography (EEG) is fundamental for assessing sleep quality and diagnosing sleep-related disorders. However, most traditional machine learning methods rely heavily on prior knowledge and handcrafted features, while existing deep learning models still struggle to jointly capture fine-grained time-frequency patterns and achieve clinical interpretability. Recently, vision-language models (VLMs) have made significant progress in the medical domain, yet their performance remains constrained when applied to physiological waveform data, especially EEG signals, due to their limited visual understanding and insufficient reasoning capability. To address these challenges, we propose EEG-VLM, a hierarchical vision-language framework that integrates multi-level feature alignment with visually enhanced language-guided reasoning for interpretable EEG-based sleep stage classification. Specifically, a specialized visual enhancement module constructs high-level visual tokens from intermediate-layer features to extract rich semantic representations of EEG images. These tokens are further aligned with low-level CLIP features through a multi-level alignment mechanism, enhancing the VLM's image-processing capability. In addition, a Chain-of-Thought (CoT) reasoning strategy decomposes complex medical inference into interpretable logical steps, effectively simulating expert-like decision-making. Experimental results demonstrate that the proposed method significantly improves both the accuracy and interpretability of VLMs in EEG-based sleep stage classification, showing promising potential for automated and explainable EEG analysis in clinical settings.",
    "authors": [
      "Xihe Qiu",
      "Gengchen Ma",
      "Haoyu Wang",
      "Chen Zhan",
      "Xiaoyu Tan",
      "Shuo Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19155v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19155v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19149v1",
    "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
    "summary": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
    "authors": [
      "Moazzam Umer Gondal",
      "Hamad Ul Qudous",
      "Daniya Siddiqui",
      "Asma Ahmad Farhan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19149v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19149v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19122v1",
    "title": "Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis",
    "summary": "Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.",
    "authors": [
      "Yaping Chai",
      "Haoran Xie",
      "Joe S. Qin"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19122v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19122v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.19365v1",
    "title": "DeCo: Frequency-Decoupled Pixel Diffusion for End-to-End Image Generation",
    "summary": "Pixel diffusion aims to generate images directly in pixel space in an end-to-end fashion. This approach avoids the limitations of VAE in the two-stage latent diffusion, offering higher model capacity. Existing pixel diffusion models suffer from slow training and inference, as they usually model both high-frequency signals and low-frequency semantics within a single diffusion transformer (DiT). To pursue a more efficient pixel diffusion paradigm, we propose the frequency-DeCoupled pixel diffusion framework. With the intuition to decouple the generation of high and low frequency components, we leverage a lightweight pixel decoder to generate high-frequency details conditioned on semantic guidance from the DiT. This thus frees the DiT to specialize in modeling low-frequency semantics. In addition, we introduce a frequency-aware flow-matching loss that emphasizes visually salient frequencies while suppressing insignificant ones. Extensive experiments show that DeCo achieves superior performance among pixel diffusion models, attaining FID of 1.62 (256x256) and 2.22 (512x512) on ImageNet, closing the gap with latent diffusion methods. Furthermore, our pretrained text-to-image model achieves a leading overall score of 0.86 on GenEval in system-level comparison. Codes are publicly available at https://github.com/Zehong-Ma/DeCo.",
    "authors": [
      "Zehong Ma",
      "Longhui Wei",
      "Shuai Wang",
      "Shiliang Zhang",
      "Qi Tian"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19365v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19365v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.19275v1",
    "title": "Dynamic Multi-Species Bird Soundscape Generation with Acoustic Patterning and 3D Spatialization",
    "summary": "Generation of dynamic, scalable multi-species bird soundscapes remains a significant challenge in computer music and algorithmic sound design. Birdsongs involve rapid frequency-modulated chirps, complex amplitude envelopes, distinctive acoustic patterns, overlapping calls, and dynamic inter-bird interactions, all of which require precise temporal and spatial control in 3D environments. Existing approaches, whether Digital Signal Processing (DSP)-based or data-driven, typically focus only on single species modeling, static call structures, or synthesis directly from recordings, and often suffer from noise, limited flexibility, or large data needs. To address these challenges, we present a novel, fully algorithm-driven framework that generates dynamic multi-species bird soundscapes using DSP-based chirp generation and 3D spatialization, without relying on recordings or training data. Our approach simulates multiple independently-moving birds per species along different moving 3D trajectories, supporting controllable chirp sequences, overlapping choruses, and realistic 3D motion in scalable soundscapes while preserving species-specific acoustic patterns. A visualization interface provides bird trajectories, spectrograms, activity timelines, and sound waves for analytical and creative purposes. Both visual and audio evaluations demonstrate the ability of the system to generate dense, immersive, and ecologically inspired soundscapes, highlighting its potential for computer music, interactive virtual environments, and computational bioacoustics research.",
    "authors": [
      "Ellie L. Zhang",
      "Duoduo Liao",
      "Callie C. Liao"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19275v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19275v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.19156v1",
    "title": "Information Physics of Intelligence: Unifying Logical Depth and Entropy under Thermodynamic Constraints",
    "summary": "The rapid scaling of artificial intelligence models has revealed a fundamental tension between model capacity (storage) and inference efficiency (computation). While classical information theory focuses on transmission and storage limits, it lacks a unified physical framework to quantify the thermodynamic costs of generating information from compressed laws versus retrieving it from memory. In this paper, we propose a theoretical framework that treats information processing as an enabling mapping from ontological states to carrier states. We introduce a novel metric, Derivation Entropy, which quantifies the effective work required to compute a target state from a given logical depth. By analyzing the interplay between Shannon entropy (storage) and computational complexity (time/energy), we demonstrate the existence of a critical phase transition point. Below this threshold, memory retrieval is thermodynamically favorable; above it, generative computation becomes the optimal strategy. This \"Energy-Time-Space\" conservation law provides a physical explanation for the efficiency of generative models and offers a rigorous mathematical bound for designing next-generation, energy-efficient AI architectures. Our findings suggest that the minimization of Derivation Entropy is a governing principle for the evolution of both biological and artificial intelligence.",
    "authors": [
      "Jianfeng Xu",
      "Zeyan Li"
    ],
    "categories": [
      "cs.IT",
      "cs.AI",
      "cs.LO"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19156v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19156v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.19005v1",
    "title": "Introducing Visual Scenes and Reasoning: A More Realistic Benchmark for Spoken Language Understanding",
    "summary": "Spoken Language Understanding (SLU) consists of two sub-tasks: intent detection (ID) and slot filling (SF). Given its broad range of real-world applications, enhancing SLU for practical deployment is increasingly critical. Profile-based SLU addresses ambiguous user utterances by incorporating context awareness (CA), user profiles (UP), and knowledge graphs (KG) to support disambiguation, thereby advancing SLU research toward real-world applicability. However, existing SLU datasets still fall short in representing real-world scenarios. Specifically, (1) CA uses one-hot vectors for representation, which is overly idealized, and (2) models typically focuses solely on predicting intents and slot labels, neglecting the reasoning process that could enhance performance and interpretability. To overcome these limitations, we introduce VRSLU, a novel SLU dataset that integrates both Visual images and explicit Reasoning. For over-idealized CA, we use GPT-4o and FLUX.1-dev to generate images reflecting users' environments and statuses, followed by human verification to ensure quality. For reasoning, GPT-4o is employed to generate explanations for predicted labels, which are then refined by human annotators to ensure accuracy and coherence. Additionally, we propose an instructional template, LR-Instruct, which first predicts labels and then generates corresponding reasoning. This two-step approach helps mitigate the influence of reasoning bias on label prediction. Experimental results confirm the effectiveness of incorporating visual information and highlight the promise of explicit reasoning in advancing SLU.",
    "authors": [
      "Di Wu",
      "Liting Jiang",
      "Ruiyu Fang",
      " Bianjing",
      "Hongyan Xie",
      "Haoxiang Su",
      "Hao Huang",
      "Zhongjiang He",
      "Shuangyong Song",
      "Xuelong Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19005v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19005v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.18936v1",
    "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression",
    "summary": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.",
    "authors": [
      "Santhosh G S",
      "Saurav Prakash",
      "Balaraman Ravindran"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18936v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18936v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.18934v1",
    "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query",
    "summary": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.",
    "authors": [
      "Yuchen Ji",
      "Bo Xu",
      "Jie Shi",
      "Jiaqing Liang",
      "Deqing Yang",
      "Yu Mao",
      "Hai Chen",
      "Yanghua Xiao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18934v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18934v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.19399v1",
    "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
    "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
    "authors": [
      "Rulin Shao",
      "Akari Asai",
      "Shannon Zejiang Shen",
      "Hamish Ivison",
      "Varsha Kishore",
      "Jingming Zhuo",
      "Xinran Zhao",
      "Molly Park",
      "Samuel G. Finlayson",
      "David Sontag",
      "Tyler Murray",
      "Sewon Min",
      "Pradeep Dasigi",
      "Luca Soldaini",
      "Faeze Brahman",
      "Wen-tau Yih",
      "Tongshuang Wu",
      "Luke Zettlemoyer",
      "Yoon Kim",
      "Hannaneh Hajishirzi",
      "Pang Wei Koh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19399v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19399v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19398v1",
    "title": "PTF Testing Lower Bounds for Non-Gaussian Component Analysis",
    "summary": "This work studies information-computation gaps for statistical problems. A common approach for providing evidence of such gaps is to show sample complexity lower bounds (that are stronger than the information-theoretic optimum) against natural models of computation. A popular such model in the literature is the family of low-degree polynomial tests. While these tests are defined in such a way that make them easy to analyze, the class of algorithms that they rule out is somewhat restricted. An important goal in this context has been to obtain lower bounds against the stronger and more natural class of low-degree Polynomial Threshold Function (PTF) tests, i.e., any test that can be expressed as comparing some low-degree polynomial of the data to a threshold. Proving lower bounds against PTF tests has turned out to be challenging. Indeed, we are not aware of any non-trivial PTF testing lower bounds in the literature.   In this paper, we establish the first non-trivial PTF testing lower bounds for a range of statistical tasks. Specifically, we prove a near-optimal PTF testing lower bound for Non-Gaussian Component Analysis (NGCA). Our NGCA lower bound implies similar lower bounds for a number of other statistical problems. Our proof leverages a connection to recent work on pseudorandom generators for PTFs and recent techniques developed in that context. At the technical level, we develop several tools of independent interest, including novel structural results for analyzing the behavior of low-degree polynomials restricted to random directions.",
    "authors": [
      "Ilias Diakonikolas",
      "Daniel M. Kane",
      "Sihan Liu",
      "Thanasis Pittas"
    ],
    "categories": [
      "cs.DS",
      "cs.IT",
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19398v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19398v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19364v1",
    "title": "Neural surrogates for designing gravitational wave detectors",
    "summary": "Physics simulators are essential in science and engineering, enabling the analysis, control, and design of complex systems. In experimental sciences, they are increasingly used to automate experimental design, often via combinatorial search and optimization. However, as the setups grow more complex, the computational cost of traditional, CPU-based simulators becomes a major limitation. Here, we show how neural surrogate models can significantly reduce reliance on such slow simulators while preserving accuracy. Taking the design of interferometric gravitational wave detectors as a representative example, we train a neural network to surrogate the gravitational wave physics simulator Finesse, which was developed by the LIGO community. Despite that small changes in physical parameters can change the output by orders of magnitudes, the model rapidly predicts the quality and feasibility of candidate designs, allowing an efficient exploration of large design spaces. Our algorithm loops between training the surrogate, inverse designing new experiments, and verifying their properties with the slow simulator for further training. Assisted by auto-differentiation and GPU parallelism, our method proposes high-quality experiments much faster than direct optimization. Solutions that our algorithm finds within hours outperform designs that take five days for the optimizer to reach. Though shown in the context of gravitational wave detectors, our framework is broadly applicable to other domains where simulator bottlenecks hinder optimization and discovery.",
    "authors": [
      "Carlos Ruiz-Gonzalez",
      "Sören Arlt",
      "Sebastian Lehner",
      "Arturs Berzins",
      "Yehonathan Drori",
      "Rana X Adhikari",
      "Johannes Brandstetter",
      "Mario Krenn"
    ],
    "categories": [
      "cs.LG",
      "astro-ph.IM",
      "gr-qc",
      "quant-ph"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19364v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19364v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19263v1",
    "title": "Solar-GECO: Perovskite Solar Cell Property Prediction with Geometric-Aware Co-Attention",
    "summary": "Perovskite solar cells are promising candidates for next-generation photovoltaics. However, their performance as multi-scale devices is determined by complex interactions between their constituent layers. This creates a vast combinatorial space of possible materials and device architectures, making the conventional experimental-based screening process slow and expensive. Machine learning models try to address this problem, but they only focus on individual material properties or neglect the important geometric information of the perovskite crystal. To address this problem, we propose to predict perovskite solar cell power conversion efficiency with a geometric-aware co-attention (Solar-GECO) model. Solar-GECO combines a geometric graph neural network (GNN) - that directly encodes the atomic structure of the perovskite absorber - with language model embeddings that process the textual strings representing the chemical compounds of the transport layers and other device components. Solar-GECO also integrates a co-attention module to capture intra-layer dependencies and inter-layer interactions, while a probabilistic regression head predicts both power conversion efficiency (PCE) and its associated uncertainty. Solar-GECO achieves state-of-the-art performance, significantly outperforming several baselines, reducing the mean absolute error (MAE) for PCE prediction from 3.066 to 2.936 compared to semantic GNN (the previous state-of-the-art model). Solar-GECO demonstrates that integrating geometric and textual information provides a more powerful and accurate framework for PCE prediction.",
    "authors": [
      "Lucas Li",
      "Jean-Baptiste Puel",
      "Florence Carton",
      "Dounya Barrit",
      "Jhony H. Giraldo"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19263v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19263v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19187v1",
    "title": "SpectraNet: FFT-assisted Deep Learning Classifier for Deepfake Face Detection",
    "summary": "Detecting deepfake images is crucial in combating misinformation. We present a lightweight, generalizable binary classification model based on EfficientNet-B6, fine-tuned with transformation techniques to address severe class imbalances. By leveraging robust preprocessing, oversampling, and optimization strategies, our model achieves high accuracy, stability, and generalization. While incorporating Fourier transform-based phase and amplitude features showed minimal impact, our proposed framework helps non-experts to effectively identify deepfake images, making significant strides toward accessible and reliable deepfake detection.",
    "authors": [
      "Nithira Jayarathne",
      "Naveen Basnayake",
      "Keshawa Jayasundara",
      "Pasindu Dodampegama",
      "Praveen Wijesinghe",
      "Hirushika Pelagewatta",
      "Kavishka Abeywardana",
      "Sandushan Ranaweera",
      "Chamira Edussooriya"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19187v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19187v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19168v1",
    "title": "RAVEN++: Pinpointing Fine-Grained Violations in Advertisement Videos with Active Reinforcement Reasoning",
    "summary": "Advertising (Ad) is a cornerstone of the digital economy, yet the moderation of video advertisements remains a significant challenge due to their complexity and the need for precise violation localization. While recent advancements, such as the RAVEN model, have improved coarse-grained violation detection, critical gaps persist in fine-grained understanding, explainability, and generalization. To address these limitations, we propose RAVEN++, a novel framework that introduces three key innovations: 1) Active Reinforcement Learning (RL), which dynamically adapts training to samples of varying difficulty; 2) Fine-Grained Violation Understanding, achieved through hierarchical reward functions and reasoning distillation; and 3) Progressive Multi-Stage Training, which systematically combines knowledge injection, curriculum-based passive RL, and active RL. Extensive experiments on both public and proprietary datasets, on both offline scenarios and online deployed A/B Testing, demonstrate that RAVEN++ outperforms general-purpose LLMs and specialized models like RAVEN in terms of fine-grained violation understanding, reasoning capabilities, and generalization ability.",
    "authors": [
      "Deyi Ji",
      "Yuekui Yang",
      "Liqun Liu",
      "Peng Shu",
      "Haiyang Wu",
      "Shaogang Tang",
      "Xudong Chen",
      "Shaoping Ma",
      "Tianrun Chen",
      "Lanyun Zhu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19168v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19168v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19103v1",
    "title": "Edge-Based Predictive Data Reduction for Smart Agriculture: A Lightweight Approach to Efficient IoT Communication",
    "summary": "The rapid growth of IoT devices has led to an enormous amount of sensor data that requires transmission to cloud servers for processing, resulting in excessive network congestion, increased latency and high energy consumption. This is particularly problematic in resource-constrained and remote environments where bandwidth is limited, and battery-dependent devices further emphasize the problem. Moreover, in domains such as agriculture, consecutive sensor readings often have minimal variation, making continuous data transmission inefficient and unnecessarily resource intensive. To overcome these challenges, we propose an analytical prediction algorithm designed for edge computing environments and validated through simulation. The proposed solution utilizes a predictive filter at the network edge that forecasts the next sensor data point and triggers data transmission only when the deviation from the predicted value exceeds a predefined tolerance. A complementary cloud-based model ensures data integrity and overall system consistency. This dual-model strategy effectively reduces communication overhead and demonstrates potential for improving energy efficiency by minimizing redundant transmissions. In addition to reducing communication load, our approach leverages both in situ and satellite observations from the same locations to enhance model robustness. It also supports cross-site generalization, enabling models trained in one region to be effectively deployed elsewhere without retraining. This makes our solution highly scalable, energy-aware, and well-suited for optimizing sensor data transmission in remote and bandwidth-constrained IoT environments.",
    "authors": [
      "Dora Krekovic",
      "Mario Kusek",
      "Ivana Podnar Zarko",
      "Danh Le-Phuoc"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19103v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19103v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19083v1",
    "title": "A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis",
    "summary": "In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.",
    "authors": [
      "Wenxuan Mu",
      "Jinzhong Ning",
      "Di Zhao",
      "Yijia Zhang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19083v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19083v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.18983v1",
    "title": "UMCL: Unimodal-generated Multimodal Contrastive Learning for Cross-compression-rate Deepfake Detection",
    "summary": "In deepfake detection, the varying degrees of compression employed by social media platforms pose significant challenges for model generalization and reliability. Although existing methods have progressed from single-modal to multimodal approaches, they face critical limitations: single-modal methods struggle with feature degradation under data compression in social media streaming, while multimodal approaches require expensive data collection and labeling and suffer from inconsistent modal quality or accessibility in real-world scenarios. To address these challenges, we propose a novel Unimodal-generated Multimodal Contrastive Learning (UMCL) framework for robust cross-compression-rate (CCR) deepfake detection. In the training stage, our approach transforms a single visual modality into three complementary features: compression-robust rPPG signals, temporal landmark dynamics, and semantic embeddings from pre-trained vision-language models. These features are explicitly aligned through an affinity-driven semantic alignment (ASA) strategy, which models inter-modal relationships through affinity matrices and optimizes their consistency through contrastive learning. Subsequently, our cross-quality similarity learning (CQSL) strategy enhances feature robustness across compression rates. Extensive experiments demonstrate that our method achieves superior performance across various compression rates and manipulation types, establishing a new benchmark for robust deepfake detection. Notably, our approach maintains high detection accuracy even when individual features degrade, while providing interpretable insights into feature relationships through explicit alignment.",
    "authors": [
      "Ching-Yi Lai",
      "Chih-Yu Jian",
      "Pei-Cheng Chuang",
      "Chia-Ming Lee",
      "Chih-Chung Hsu",
      "Chiou-Ting Hsu",
      "Chia-Wen Lin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18983v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18983v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.19433v1",
    "title": "Mixture of Horizons in Action Chunking",
    "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $π_0$, $π_{0.5}$, and one-step regression policy $π_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $π_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons",
    "authors": [
      "Dong Jing",
      "Gang Wang",
      "Jiaqi Liu",
      "Weiliang Tang",
      "Zelong Sun",
      "Yunchao Yao",
      "Zhenyu Wei",
      "Yunhui Liu",
      "Zhiwu Lu",
      "Mingyu Ding"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19433v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19433v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.19024v1",
    "title": "Life-IQA: Boosting Blind Image Quality Assessment through GCN-enhanced Layer Interaction and MoE-based Feature Decoupling",
    "summary": "Blind image quality assessment (BIQA) plays a crucial role in evaluating and optimizing visual experience. Most existing BIQA approaches fuse shallow and deep features extracted from backbone networks, while overlooking the unequal contributions to quality prediction. Moreover, while various vision encoder backbones are widely adopted in BIQA, the effective quality decoding architectures remain underexplored. To address these limitations, this paper investigates the contributions of shallow and deep features to BIQA, and proposes a effective quality feature decoding framework via GCN-enhanced \\underline{l}ayer\\underline{i}nteraction and MoE-based \\underline{f}eature d\\underline{e}coupling, termed \\textbf{(Life-IQA)}. Specifically, the GCN-enhanced layer interaction module utilizes the GCN-enhanced deepest-layer features as query and the penultimate-layer features as key, value, then performs cross-attention to achieve feature interaction. Moreover, a MoE-based feature decoupling module is proposed to decouple fused representations though different experts specialized for specific distortion types or quality dimensions. Extensive experiments demonstrate that Life-IQA shows more favorable balance between accuracy and cost than a vanilla Transformer decoder and achieves state-of-the-art performance on multiple BIQA benchmarks.The code is available at: \\href{https://github.com/TANGLONG2/Life-IQA/tree/main}{\\texttt{Life-IQA}}.",
    "authors": [
      "Long Tang",
      "Guoquan Zhen",
      "Jie Hao",
      "Jianbo Zhang",
      "Huiyu Duan",
      "Liang Yuan",
      "Guangtao Zhai"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19024v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19024v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.19023v1",
    "title": "OrdMoE: Preference Alignment via Hierarchical Expert Group Ranking in Multimodal Mixture-of-Experts LLMs",
    "summary": "Preference learning has recently emerged as a pivotal strategy for post-training alignment of Multimodal Large Language Models (MLLMs). However, existing approaches predominantly rely on external human-annotated preference data, which is costly and labor-intensive to collect. In this work, we propose OrdMoE, a novel preference alignment framework that bypasses the reliance on external human preferences entirely by leveraging intrinsic signals within Mixture-of-Experts (MoE) architectures. Specifically, we observe that the router's expert selection scores implicitly encode a quality-aware ranking of responses (i.e. higher-scoring experts consistently generate higher-quality outputs). Building on this insight, OrdMoE constructs an internal preference hierarchy by grouping experts into ranked tiers based on their per-token routing scores and activating each tier separately to produce a sequence of responses with increasing quality. This yields a zero-cost, self-supervised preference ordering over generated responses, which can be directly optimized using standard preference learning objectives. Extensive experiments across multiple multimodal benchmarks demnstrate that OrdMoE significantly enhances both alignment and overall performance of multimodal Mixture-of-Experts LLMs, achieving competitive results without requiring any human-annotated preference data.",
    "authors": [
      "Yuting Gao",
      "Weihao Chen",
      "Lan Wang",
      "Ruihan Xu",
      "Qingpei Guo"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19023v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19023v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.19427v1",
    "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering",
    "summary": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.",
    "authors": [
      "Jayanaka L. Dantanarayana",
      "Savini Kashmira",
      "Thakee Nathees",
      "Zichen Zhang",
      "Krisztian Flautner",
      "Lingjia Tang",
      "Jason Mars"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19427v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19427v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19326v1",
    "title": "MonoMSK: Monocular 3D Musculoskeletal Dynamics Estimation",
    "summary": "Reconstructing biomechanically realistic 3D human motion - recovering both kinematics (motion) and kinetics (forces) - is a critical challenge. While marker-based systems are lab-bound and slow, popular monocular methods use oversimplified, anatomically inaccurate models (e.g., SMPL) and ignore physics, fundamentally limiting their biomechanical fidelity. In this work, we introduce MonoMSK, a hybrid framework that bridges data-driven learning and physics-based simulation for biomechanically realistic 3D human motion estimation from monocular video. MonoMSK jointly recovers both kinematics (motions) and kinetics (forces and torques) through an anatomically accurate musculoskeletal model. By integrating transformer-based inverse dynamics with differentiable forward kinematics and dynamics layers governed by ODE-based simulation, MonoMSK establishes a physics-regulated inverse-forward loop that enforces biomechanical causality and physical plausibility. A novel forward-inverse consistency loss further aligns motion reconstruction with the underlying kinetic reasoning. Experiments on BML-MoVi, BEDLAM, and OpenCap show that MonoMSK significantly outperforms state-of-the-art methods in kinematic accuracy, while for the first time enabling precise monocular kinetics estimation.",
    "authors": [
      "Farnoosh Koleini",
      "Hongfei Xue",
      "Ahmed Helmy",
      "Pu Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19326v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19326v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19306v1",
    "title": "Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection",
    "summary": "Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.",
    "authors": [
      "Zixuan Wang",
      "Haoran Sun",
      "Jiaming Lu",
      "Wenxuan Wang",
      "Zhongling Huang",
      "Dingwen Zhang",
      "Xuelin Qian",
      "Junwei Han"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19306v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19306v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19265v1",
    "title": "Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks",
    "summary": "The black box nature of deep neural networks poses a significant challenge for the deployment of transparent and trustworthy artificial intelligence (AI) systems. With the growing presence of AI in society, it becomes increasingly important to develop methods that can explain and interpret the decisions made by these systems. To address this, mechanistic interpretability (MI) emerged as a promising and distinctive research program within the broader field of explainable artificial intelligence (XAI). MI is the process of studying the inner computations of neural networks and translating them into human-understandable algorithms. It encompasses reverse engineering techniques aimed at uncovering the computational algorithms implemented by neural networks. In this article, we propose a unified taxonomy of MI approaches and provide a detailed analysis of key techniques, illustrated with concrete examples and pseudo-code. We contextualize MI within the broader interpretability landscape, comparing its goals, methods, and insights to other strands of XAI. Additionally, we trace the development of MI as a research area, highlighting its conceptual roots and the accelerating pace of recent work. We argue that MI holds significant potential to support a more scientific understanding of machine learning systems -- treating models not only as tools for solving tasks, but also as systems to be studied and understood. We hope to invite new researchers into the field of mechanistic interpretability.",
    "authors": [
      "Bianka Kowalska",
      "Halina Kwaśnicka"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19265v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19265v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19229v1",
    "title": "Learning Plug-and-play Memory for Guiding Video Diffusion Models",
    "summary": "Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.",
    "authors": [
      "Selena Song",
      "Ziming Xu",
      "Zijun Zhang",
      "Kun Zhou",
      "Jiaxian Guo",
      "Lianhui Qin",
      "Biwei Huang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19229v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19114v1",
    "title": "Physics-informed Neural Operator Learning for Nonlinear Grad-Shafranov Equation",
    "summary": "As artificial intelligence emerges as a transformative enabler for fusion energy commercialization, fast and accurate solvers become increasingly critical. In magnetic confinement nuclear fusion, rapid and accurate solution of the Grad-Shafranov equation (GSE) is essential for real-time plasma control and analysis. Traditional numerical solvers achieve high precision but are computationally prohibitive, while data-driven surrogates infer quickly but fail to enforce physical laws and generalize poorly beyond training distributions. To address this challenge, we present a Physics-Informed Neural Operator (PINO) that directly learns the GSE solution operator, mapping shape parameters of last closed flux surface to equilibrium solutions for realistic nonlinear current profiles. Comprehensive benchmarking of five neural architectures identifies the novel Transformer-KAN (Kolmogorov-Arnold Network) Neural Operator (TKNO) as achieving highest accuracy (0.25% mean L2 relative error) under supervised training (only data-driven). However, all data-driven models exhibit large physics residuals, indicating poor physical consistency. Our unsupervised training can reduce the residuals by nearly four orders of magnitude through embedding physics-based loss terms without labeled data. Critically, semi-supervised learning--integrating sparse labeled data (100 interior points) with physics constraints--achieves optimal balance: 0.48% interpolation error and the most robust extrapolation performance (4.76% error, 8.9x degradation factor vs 39.8x for supervised models). Accelerated by TensorRT optimization, our models enable millisecond-level inference, establishing PINO as a promising pathway for next-generation fusion control systems.",
    "authors": [
      "Siqi Ding",
      "Zitong Zhang",
      "Guoyang Shi",
      "Xingyu Li",
      "Xiang Gu",
      "Yanan Xu",
      "Huasheng Xie",
      "Hanyue Zhao",
      "Yuejiang Shi",
      "Tianyuan Liu"
    ],
    "categories": [
      "physics.plasm-ph",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19114v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19114v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19080v1",
    "title": "Towards Generalizable Deepfake Detection via Forgery-aware Audio-Visual Adaptation: A Variational Bayesian Approach",
    "summary": "The widespread application of AIGC contents has brought not only unprecedented opportunities, but also potential security concerns, e.g., audio-visual deepfakes. Therefore, it is of great importance to develop an effective and generalizable method for multi-modal deepfake detection. Typically, the audio-visual correlation learning could expose subtle cross-modal inconsistencies, e.g., audio-visual misalignment, which serve as crucial clues in deepfake detection. In this paper, we reformulate the correlation learning with variational Bayesian estimation, where audio-visual correlation is approximated as a Gaussian distributed latent variable, and thus develop a novel framework for deepfake detection, i.e., Forgery-aware Audio-Visual Adaptation with Variational Bayes (FoVB). Specifically, given the prior knowledge of pre-trained backbones, we adopt two core designs to estimate audio-visual correlations effectively. First, we exploit various difference convolutions and a high-pass filter to discern local and global forgery traces from both modalities. Second, with the extracted forgery-aware features, we estimate the latent Gaussian variable of audio-visual correlation via variational Bayes. Then, we factorize the variable into modality-specific and correlation-specific ones with orthogonality constraint, allowing them to better learn intra-modal and cross-modal forgery traces with less entanglement. Extensive experiments demonstrate that our FoVB outperforms other state-of-the-art methods in various benchmarks.",
    "authors": [
      "Fan Nie",
      "Jiangqun Ni",
      "Jian Zhang",
      "Bin Zhang",
      "Weizhe Zhang",
      "Bin Li"
    ],
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19080v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19080v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.18989v1",
    "title": "Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning",
    "summary": "Recent advances in deep learning have enabled significant progress in plant disease classification using leaf images. Much of the existing research in this field has relied on the PlantVillage dataset, which consists of well-centered plant images captured against uniform, uncluttered backgrounds. Although models trained on this dataset achieve high accuracy, they often fail to generalize to real-world field images, such as those submitted by farmers to plant diagnostic systems. This has created a significant gap between published studies and practical application requirements, highlighting the necessity of investigating and addressing this issue. In this study, we investigate whether attention-based architectures and zero-shot learning approaches can bridge the gap between curated academic datasets and real-world agricultural conditions in plant disease classification. We evaluate three model categories: Convolutional Neural Networks (CNNs), Vision Transformers, and Contrastive Language-Image Pre-training (CLIP)-based zero-shot models. While CNNs exhibit limited robustness under domain shift, Vision Transformers demonstrate stronger generalization by capturing global contextual features. Most notably, CLIP models classify diseases directly from natural language descriptions without any task-specific training, offering strong adaptability and interpretability. These findings highlight the potential of zero-shot learning as a practical and scalable domain adaptation strategy for plant health diagnosis in diverse field environments.",
    "authors": [
      "Wassim Benabbas",
      "Mohammed Brahimi",
      "Samir Akhrouf",
      "Bilal Fortas"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18989v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18989v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.19380v1",
    "title": "UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval",
    "summary": "Enterprise software companies maintain thousands of user interface screens across products and versions, creating critical challenges for design consistency, pattern discovery, and compliance check. Existing approaches rely on visual similarity or text semantics, lacking explicit modeling of structural properties fundamental to user interface (UI) composition. We present a novel graph-based representation that converts UI screenshots into attributed graphs encoding hierarchical relationships and spatial arrangements, potentially generalizable to document layouts, architectural diagrams, and other structured visual domains. A contrastive graph autoencoder learns embeddings preserving multi-level similarity across visual, structural, and semantic properties. The comprehensive analysis demonstrates that our structural embeddings achieve better discriminative power than state-of-the-art Vision Encoders, representing a fundamental advance in the expressiveness of the UI representation. We implement this representation in UISearch, a multi-modal search framework that combines structural embeddings with semantic search through a composable query language. On 20,396 financial software UIs, UISearch achieves 0.92 Top-5 accuracy with 47.5ms median latency (P95: 124ms), scaling to 20,000+ screens. The hybrid indexing architecture enables complex queries and supports fine-grained UI distinction impossible with vision-only approaches.",
    "authors": [
      "Maroun Ayli",
      "Youssef Bakouny",
      "Tushar Sharma",
      "Nader Jalloul",
      "Hani Seifeddine",
      "Rima Kilany"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19380v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19380v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.19267v1",
    "title": "Leveraging Spatiotemporal Graph Neural Networks for Multi-Store Sales Forecasting",
    "summary": "This work evaluates the effectiveness of spatiotemporal Graph Neural Networks (GNNs) for multi-store retail sales forecasting and compares their performance against ARIMA, LSTM, and XGBoost baselines. Using weekly sales data from 45 Walmart stores, we construct a relational forecasting framework that models inter-store dependencies through a learned adaptive graph. The proposed STGNN predicts log-differenced sales and reconstructs final values through a residual path, enabling stable training and improved generalisation. Experiments show that STGNN achieves the lowest overall forecasting error, outperforming all baselines in Normalised Total Absolute Error, P90 MAPE, and variance of MAPE across stores. Analysis of the learned adjacency matrix reveals meaningful functional store clusters and high-influence nodes that emerge without geographic metadata. These results demonstrate that relational structure significantly improves forecast quality in interconnected retail environments and establishes STGNNs as a robust modelling choice for multi-store demand prediction.",
    "authors": [
      "Manish Singh",
      "Arpita Dayama"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19267v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19267v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.19199v1",
    "title": "CLASH: A Benchmark for Cross-Modal Contradiction Detection",
    "summary": "Contradictory multimodal inputs are common in real-world settings, yet existing benchmarks typically assume input consistency and fail to evaluate cross-modal contradiction detection - a fundamental capability for preventing hallucinations and ensuring reliability. We introduce CLASH, a novel benchmark for multimodal contradiction detection, featuring COCO images paired with contradictory captions containing controlled object-level or attribute-level contradictions. The samples include targeted questions evaluated in both multiple-choice and open-ended formats. The benchmark provides an extensive fine-tuning set filtered through automated quality checks, alongside a smaller human-verified diagnostic set. Our analysis of state-of-the-art models reveals substantial limitations in recognizing cross-modal conflicts, exposing systematic modality biases and category-specific weaknesses. Furthermore, we empirically demonstrate that targeted fine-tuning on CLASH substantially enhances conflict detection capabilities.",
    "authors": [
      "Teodora Popordanoska",
      "Jiameng Li",
      "Matthew B. Blaschko"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19199v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19199v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.18920v1",
    "title": "EventSTU: Event-Guided Efficient Spatio-Temporal Understanding for Video Large Language Models",
    "summary": "Video large language models have demonstrated strong video understanding capabilities but suffer from high inference costs due to the massive number of tokens in long videos. Inspired by event-based vision, we propose an event-guided, training-free framework for efficient spatio-temporal understanding, named EventSTU. In the temporal domain, we design a coarse-to-fine keyframe sampling algorithm that exploits the change-triggered property of event cameras to eliminate redundant frames. In the spatial domain, we design an adaptive token pruning algorithm that leverages the visual saliency of events as a zero-cost prior to guide spatial reduction. From a holistic spatio-temporal perspective, we further integrate question relevance from keyframe sampling to adaptively allocate token pruning budgets. To facilitate evaluation, we construct EventBench, the first event-inclusive, human-annotated multimodal benchmark that covers diverse real-world scenarios. Beyond physical event cameras, EventSTU also supports general video understanding using simulated events. Comprehensive experiments show that EventSTU achieves 3.01x FLOPs reduction and 3.10x prefilling speedup over the strongest baseline while still improving performance.",
    "authors": [
      "Wenhao Xu",
      "Xin Dong",
      "Yue Li",
      "Haoyuan Shi",
      "Zhiwei Xiong"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18920v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18920v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.19396v1",
    "title": "Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments",
    "summary": "Advances in object tracking and acoustic beamforming are driving new capabilities in surveillance, human-computer interaction, and robotics. This work presents an embedded system that integrates deep learning-based tracking with beamforming to achieve precise sound source localization and directional audio capture in dynamic environments. The approach combines single-camera depth estimation and stereo vision to enable accurate 3D localization of moving objects. A planar concentric circular microphone array constructed with MEMS microphones provides a compact, energy-efficient platform supporting 2D beam steering across azimuth and elevation. Real-time tracking outputs continuously adapt the array's focus, synchronizing the acoustic response with the target's position. By uniting learned spatial awareness with dynamic steering, the system maintains robust performance in the presence of multiple or moving sources. Experimental evaluation demonstrates significant gains in signal-to-interference ratio, making the design well-suited for teleconferencing, smart home devices, and assistive technologies.",
    "authors": [
      "Jorge Ortigoso-Narro",
      "Jose A. Belloch",
      "Adrian Amor-Martin",
      "Sandra Roger",
      "Maximo Cobos"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19396v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19396v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.19328v1",
    "title": "Understanding the Staged Dynamics of Transformers in Learning Latent Structure",
    "summary": "While transformers can discover latent structure from context, the dynamics of how they acquire different components of the latent structure remain poorly understood. In this work, we use the Alchemy benchmark, to investigate the dynamics of latent structure learning. We train a small decoder-only transformer on three task variants: 1) inferring missing rules from partial contextual information, 2) composing simple rules to solve multi-step sequences, and 3) decomposing complex multi-step examples to infer intermediate steps. By factorizing each task into interpretable events, we show that the model acquires capabilities in discrete stages, first learning the coarse grained rules, before learning the complete latent structure. We also identify a crucial asymmetry, where the model can compose fundamental rules robustly, but struggles to decompose complex examples to discover the fundamental rules. These findings offer new insights into understanding how a transformer model learns latent structures, providing a granular view of how these capabilities evolve during training.",
    "authors": [
      "Rohan Saha",
      "Farzane Aminmansour",
      "Alona Fyshe"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19328v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19328v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.19221v1",
    "title": "Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving",
    "summary": "Autonomous driving heavily relies on accurate and robust spatial perception. Many failures arise from inaccuracies and instability, especially in long-tail scenarios and complex interactions. However, current vision-language models are weak at spatial grounding and understanding, and VLA systems built on them therefore show limited perception and localization ability. To address these challenges, we introduce Percept-WAM, a perception-enhanced World-Awareness-Action Model that is the first to implicitly integrate 2D/3D scene understanding abilities within a single vision-language model (VLM). Instead of relying on QA-style spatial reasoning, Percept-WAM unifies 2D/3D perception tasks into World-PV and World-BEV tokens, which encode both spatial coordinates and confidence. We propose a grid-conditioned prediction mechanism for dense object perception, incorporating IoU-aware scoring and parallel autoregressive decoding, improving stability in long-tail, far-range, and small-object scenarios. Additionally, Percept-WAM leverages pretrained VLM parameters to retain general intelligence (e.g., logical reasoning) and can output perception results and trajectory control outputs directly. Experiments show that Percept-WAM matches or surpasses classical detectors and segmenters on downstream perception benchmarks, achieving 51.7/58.9 mAP on COCO 2D detection and nuScenes BEV 3D detection. When integrated with trajectory decoders, it further improves planning performance on nuScenes and NAVSIM, e.g., surpassing DiffusionDrive by 2.1 in PMDS on NAVSIM. Qualitative results further highlight its strong open-vocabulary and long-tail generalization.",
    "authors": [
      "Jianhua Han",
      "Meng Tian",
      "Jiangtong Zhu",
      "Fan He",
      "Huixin Zhang",
      "Sitong Guo",
      "Dechang Zhu",
      "Hao Tang",
      "Pei Xu",
      "Yuze Guo",
      "Minzhe Niu",
      "Haojie Zhu",
      "Qichao Dong",
      "Xuechao Yan",
      "Siyuan Dong",
      "Lu Hou",
      "Qingqiu Huang",
      "Xiaosong Jia",
      "Hang Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19221v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19221v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.19176v1",
    "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
    "summary": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
    "authors": [
      "Jeeho Shin",
      "Kyungho Kim",
      "Kijung Shin"
    ],
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19176v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19176v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.19055v1",
    "title": "Large Language Model-Assisted Planning of Electric Vehicle Charging Infrastructure with Real-World Case Study",
    "summary": "The growing demand for electric vehicle (EV) charging infrastructure presents significant planning challenges, requiring efficient strategies for investment and operation to deliver cost-effective charging services. However, the potential benefits of EV charging assignment, particularly in response to varying spatial-temporal patterns of charging demand, remain under-explored in infrastructure planning. This paper proposes an integrated approach that jointly optimizes investment decisions and charging assignments while accounting for spatial-temporal demand dynamics and their interdependencies. To support efficient model development, we leverage a large language model (LLM) to assist in generating and refining the mathematical formulation from structured natural-language descriptions, significantly reducing the modeling burden. The resulting optimization model enables optimal joint decision-making for investment and operation. Additionally, we propose a distributed optimization algorithm based on the Alternating Direction Method of Multipliers (ADMM) to address computational complexity in high-dimensional scenarios, which can be executed on standard computing platforms. We validate our approach through a case study using 1.5 million real-world travel records from Chengdu, China, demonstrating a 30% reduction in total cost compared to a baseline without EV assignment.",
    "authors": [
      "Xinda Zheng",
      "Canchen Jiang",
      "Hao Wang"
    ],
    "categories": [
      "eess.SY",
      "cs.AI",
      "math.OC"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19055v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19055v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.19035v1",
    "title": "CSD: Change Semantic Detection with only Semantic Change Masks for Damage Assessment in Conflict Zones",
    "summary": "Accurately and swiftly assessing damage from conflicts is crucial for humanitarian aid and regional stability. In conflict zones, damaged zones often share similar architectural styles, with damage typically covering small areas and exhibiting blurred boundaries. These characteristics lead to limited data, annotation difficulties, and significant recognition challenges, including high intra-class similarity and ambiguous semantic changes. To address these issues, we introduce a pre-trained DINOv3 model and propose a multi-scale cross-attention difference siamese network (MC-DiSNet). The powerful visual representation capability of the DINOv3 backbone enables robust and rich feature extraction from bi-temporal remote sensing images. We also release a new Gaza-change dataset containing high-resolution satellite image pairs from 2023-2024 with pixel-level semantic change annotations. It is worth emphasizing that our annotations only include semantic pixels of changed areas. Unlike conventional semantic change detection (SCD), our approach eliminates the need for large-scale semantic annotations of bi-temporal images, instead focusing directly on the changed regions. We term this new task change semantic detection (CSD). The CSD task represents a direct extension of binary change detection (BCD). Due to the limited spatial extent of semantic regions, it presents greater challenges than traditional SCD tasks. We evaluated our method under the CSD framework on both the Gaza-Change and SECOND datasets. Experimental results demonstrate that our proposed approach effectively addresses the CSD task, and its outstanding performance paves the way for practical applications in rapid damage assessment across conflict zones.",
    "authors": [
      "Kai Zhenga",
      "Zhenkai Wu",
      "Fupeng Wei",
      "Miaolan Zhou",
      "Kai Lie",
      "Haitao Guo",
      "Lei Ding",
      "Wei Zhang",
      "Hang-Cheng Dong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19035v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19035v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.18980v1",
    "title": "MOCLIP: A Foundation Model for Large-Scale Nanophotonic Inverse Design",
    "summary": "Foundation models (FM) are transforming artificial intelligence by enabling generalizable, data-efficient solutions across different domains for a broad range of applications. However, the lack of large and diverse datasets limits the development of FM in nanophotonics. This work presents MOCLIP (Metasurface Optics Contrastive Learning Pretrained), a nanophotonic foundation model that integrates metasurface geometry and spectra within a shared latent space. MOCLIP employs contrastive learning to align geometry and spectral representations using an experimentally acquired dataset with a sample density comparable to ImageNet-1K. The study demonstrates MOCLIP inverse design capabilities for high-throughput zero-shot prediction at a rate of 0.2 million samples per second, enabling the design of a full 4-inch wafer populated with high-density metasurfaces in minutes. It also shows generative latent-space optimization reaching 97 percent accuracy. Finally, we introduce an optical information storage concept that uses MOCLIP to achieve a density of 0.1 Gbit per square millimeter at the resolution limit, exceeding commercial optical media by a factor of six. These results position MOCLIP as a scalable and versatile platform for next-generation photonic design and data-driven applications.",
    "authors": [
      "S. Rodionov",
      "A. Burguete-Lopez",
      "M. Makarenko",
      "Q. Wang",
      "F. Getman",
      "A. Fratalocchi"
    ],
    "categories": [
      "physics.optics",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18980v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18980v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.18931v1",
    "title": "Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs",
    "summary": "Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.",
    "authors": [
      "Sahil Kale"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18931v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18931v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.19434v1",
    "title": "Breaking the Likelihood-Quality Trade-off in Diffusion Models by Merging Pretrained Experts",
    "summary": "Diffusion models for image generation often exhibit a trade-off between perceptual sample quality and data likelihood: training objectives emphasizing high-noise denoising steps yield realistic images but poor likelihoods, whereas likelihood-oriented training overweights low-noise steps and harms visual fidelity. We introduce a simple plug-and-play sampling method that combines two pretrained diffusion experts by switching between them along the denoising trajectory. Specifically, we apply an image-quality expert at high noise levels to shape global structure, then switch to a likelihood expert at low noise levels to refine pixel statistics. The approach requires no retraining or fine-tuning -- only the choice of an intermediate switching step. On CIFAR-10 and ImageNet32, the merged model consistently matches or outperforms its base components, improving or preserving both likelihood and sample quality relative to each expert alone. These results demonstrate that expert switching across noise levels is an effective way to break the likelihood-quality trade-off in image diffusion models.",
    "authors": [
      "Yasin Esfandiari",
      "Stefan Bauer",
      "Sebastian U. Stich",
      "Andrea Dittadi"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19434v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19434v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.19335v1",
    "title": "High-throughput validation of phase formability and simulation accuracy of Cantor alloys",
    "summary": "High-throughput methods enable accelerated discovery of novel materials in complex systems such as high-entropy alloys, which exhibit intricate phase stability across vast compositional spaces. Computational approaches, including Density Functional Theory (DFT) and calculation of phase diagrams (CALPHAD), facilitate screening of phase formability as a function of composition and temperature. However, the integration of computational predictions with experimental validation remains challenging in high-throughput studies. In this work, we introduce a quantitative confidence metric to assess the agreement between predictions and experimental observations, providing a quantitative measure of the confidence of machine learning models trained on either DFT or CALPHAD input in accounting for experimental evidence. The experimental dataset was generated via high-throughput in-situ synchrotron X-ray diffraction on compositionally varied FeNiMnCr alloy libraries, heated from room temperature to ~1000 °C. Agreement between the observed and predicted phases was evaluated using either temperature-independent phase classification or a model that incorporates a temperature-dependent probability of phase formation. This integrated approach demonstrates where strong overall agreement between computation and experiment exists, while also identifying key discrepancies, particularly in FCC/BCC predictions at Mn-rich regions to inform future model refinement.",
    "authors": [
      "Changjun Cheng",
      "Daniel Persaud",
      "Kangming Li",
      "Michael J. Moorehead",
      "Natalie Page",
      "Christian Lavoie",
      "Beatriz Diaz Moreno",
      "Adrien Couet",
      "Samuel E Lofland",
      "Jason Hattrick-Simpers"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19335v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19335v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.19330v1",
    "title": "Targeted Manipulation: Slope-Based Attacks on Financial Time-Series Data",
    "summary": "A common method of attacking deep learning models is through adversarial attacks, which occur when an attacker specifically modifies the input of a model to produce an incorrect result. Adversarial attacks have been deeply investigated in the image domain; however, there is less research in the time-series domain and very little for forecasting financial data. To address these concerns, this study aims to build upon previous research on adversarial attacks for time-series data by introducing two new slope-based methods aimed to alter the trends of the predicted stock forecast generated by an N-HiTS model. Compared to the normal N-HiTS predictions, the two new slope-based methods, the General Slope Attack and Least-Squares Slope Attack, can manipulate N-HiTS predictions by doubling the slope. These new slope attacks can bypass standard security mechanisms, such as a discriminator that filters real and perturbed inputs, reducing a 4-layered CNN's specificity to 28% and accuracy to 57%. Furthermore, the slope based methods were incorporated into a GAN architecture as a means of generating realistic synthetic data, while simultaneously fooling the model. Finally, this paper also proposes a sample malware designed to inject an adversarial attack in the model inference library, proving that ML-security research should not only focus on making the model safe, but also securing the entire pipeline.",
    "authors": [
      "Dominik Luszczynski"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19330v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19330v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.19301v1",
    "title": "IDEAL-M3D: Instance Diversity-Enriched Active Learning for Monocular 3D Detection",
    "summary": "Monocular 3D detection relies on just a single camera and is therefore easy to deploy. Yet, achieving reliable 3D understanding from monocular images requires substantial annotation, and 3D labels are especially costly. To maximize performance under constrained labeling budgets, it is essential to prioritize annotating samples expected to deliver the largest performance gains. This prioritization is the focus of active learning. Curiously, we observed two significant limitations in active learning algorithms for 3D monocular object detection. First, previous approaches select entire images, which is inefficient, as non-informative instances contained in the same image also need to be labeled. Secondly, existing methods rely on uncertainty-based selection, which in monocular 3D object detection creates a bias toward depth ambiguity. Consequently, distant objects are selected, while nearby objects are overlooked.   To address these limitations, we propose IDEAL-M3D, the first instance-level pipeline for monocular 3D detection. For the first time, we demonstrate that an explicitly diverse, fast-to-train ensemble improves diversity-driven active learning for monocular 3D. We induce diversity with heterogeneous backbones and task-agnostic features, loss weight perturbation, and time-dependent bagging. IDEAL-M3D shows superior performance and significant resource savings: with just 60% of the annotations, we achieve similar or better AP3D on KITTI validation and test set results compared to training the same detector on the whole dataset.",
    "authors": [
      "Johannes Meier",
      "Florian Günther",
      "Riccardo Marin",
      "Oussema Dhaouadi",
      "Jacques Kaiser",
      "Daniel Cremers"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19301v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19301v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.19253v1",
    "title": "MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization",
    "summary": "Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.",
    "authors": [
      "Boyuan Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19253v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19253v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.19100v1",
    "title": "Extracting Robust Register Automata from Neural Networks over Data Sequences",
    "summary": "Automata extraction is a method for synthesising interpretable surrogates for black-box neural models that can be analysed symbolically. Existing techniques assume a finite input alphabet, and thus are not directly applicable to data sequences drawn from continuous domains. We address this challenge with deterministic register automata (DRAs), which extend finite automata with registers that store and compare numeric values. Our main contribution is a framework for robust DRA extraction from black-box models: we develop a polynomial-time robustness checker for DRAs with a fixed number of registers, and combine it with passive and active automata learning algorithms. This combination yields surrogate DRAs with statistical robustness and equivalence guarantees. As a key application, we use the extracted automata to assess the robustness of neural networks: for a given sequence and distance metric, the DRA either certifies local robustness or produces a concrete counterexample. Experiments on recurrent neural networks and transformer architectures show that our framework reliably learns accurate automata and enables principled robustness evaluation. Overall, our results demonstrate that robust DRA extraction effectively bridges neural network interpretability and formal reasoning without requiring white-box access to the underlying network.",
    "authors": [
      "Chih-Duo Hong",
      "Hongjian Jiang",
      "Anthony W. Lin",
      "Oliver Markgraf",
      "Julian Parsert",
      "Tony Tan"
    ],
    "categories": [
      "cs.AI",
      "cs.FL",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19100v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19100v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.18955v1",
    "title": "Active Inference is a Subtype of Variational Inference",
    "summary": "Automated decision-making under uncertainty requires balancing exploitation and exploration. Classical methods treat these separately using heuristics, while Active Inference unifies them through Expected Free Energy (EFE) minimization. However, EFE minimization is computationally expensive, limiting scalability. We build on recent theory recasting EFE minimization as variational inference, formally unifying it with Planning-as-Inference and showing the epistemic drive as a unique entropic contribution. Our main contribution is a novel message-passing scheme for this unified objective, enabling scalable Active Inference in factored-state MDPs and overcoming high-dimensional planning intractability.",
    "authors": [
      "Wouter W. L. Nuijten",
      "Mykola Lukashchuk"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18955v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18955v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.18940v1",
    "title": "Geometry-Aware Deep Congruence Networks for Manifold Learning in Cross-Subject Motor Imagery",
    "summary": "Cross-subject motor-imagery decoding remains a major challenge in EEG-based brain-computer interfaces due to strong subject variability and the curved geometry of covariance matrices on the symmetric positive definite (SPD) manifold. We address the zero-shot cross-subject setting, where no target-subject labels or adaptation are allowed, by introducing novel geometry-aware preprocessing modules and deep congruence networks that operate directly on SPD covariance matrices. Our preprocessing modules, DCR and RiFU, extend Riemannian Alignment by improving action separation while reducing subject-specific distortions. We further propose two manifold classifiers, SPD-DCNet and RiFUNet, which use hierarchical congruence transforms to learn discriminative, subject-invariant covariance representations. On the BCI-IV 2a benchmark, our framework improves cross-subject accuracy by 3-4% over the strongest classical baselines, demonstrating the value of geometry-aware transformations for robust EEG decoding.",
    "authors": [
      "Sanjeev Manivannan",
      "Chandrashekar Lakshminarayan"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18940v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18940v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.18927v1",
    "title": "FineXtrol: Controllable Motion Generation via Fine-Grained Text",
    "summary": "Recent works have sought to enhance the controllability and precision of text-driven motion generation. Some approaches leverage large language models (LLMs) to produce more detailed texts, while others incorporate global 3D coordinate sequences as additional control signals. However, the former often introduces misaligned details and lacks explicit temporal cues, and the latter incurs significant computational cost when converting coordinates to standard motion representations. To address these issues, we propose FineXtrol, a novel control framework for efficient motion generation guided by temporally-aware, precise, user-friendly, and fine-grained textual control signals that describe specific body part movements over time. In support of this framework, we design a hierarchical contrastive learning module that encourages the text encoder to produce more discriminative embeddings for our novel control signals, thereby improving motion controllability. Quantitative results show that FineXtrol achieves strong performance in controllable motion generation, while qualitative analysis demonstrates its flexibility in directing specific body part movements.",
    "authors": [
      "Keming Shen",
      "Bizhu Wu",
      "Junliang Chen",
      "Xiaoqin Wang",
      "Linlin Shen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18927v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18927v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.19390v1",
    "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme",
    "summary": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability.",
    "authors": [
      "Rudy Morel",
      "Francesco Pio Ramunno",
      "Jeff Shen",
      "Alberto Bietti",
      "Kyunghyun Cho",
      "Miles Cranmer",
      "Siavash Golkar",
      "Olexandr Gugnin",
      "Geraud Krawezik",
      "Tanya Marwah",
      "Michael McCabe",
      "Lucas Meyer",
      "Payel Mukhopadhyay",
      "Ruben Ohana",
      "Liam Parker",
      "Helen Qu",
      "François Rozet",
      "K. D. Leka",
      "François Lanusse",
      "David Fouhey",
      "Shirley Ho"
    ],
    "categories": [
      "cs.LG",
      "astro-ph.SR",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19390v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19390v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19333v1",
    "title": "Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces",
    "summary": "Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.",
    "authors": [
      "Shaltiel Shmidman",
      "Asher Fredman",
      "Oleg Sudakov",
      "Meriem Bendris"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19333v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19333v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19320v1",
    "title": "SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation",
    "summary": "Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.",
    "authors": [
      "Jiaming Zhang",
      "Shengming Cao",
      "Rui Li",
      "Xiaotong Zhao",
      "Yutao Cui",
      "Xinglin Hou",
      "Gangshan Wu",
      "Haolan Chen",
      "Yu Xu",
      "Limin Wang",
      "Kai Ma"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19320v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19320v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19279v1",
    "title": "MapFormer: Self-Supervised Learning of Cognitive Maps with Input-Dependent Positional Embeddings",
    "summary": "A cognitive map is an internal model which encodes the abstract relationships among entities in the world, giving humans and animals the flexibility to adapt to new situations, with a strong out-of-distribution (OOD) generalization that current AI systems still do not possess. To bridge this gap, we introduce MapFormers, new architectures based on Transformer models, which can learn cognitive maps from observational data and perform path integration in parallel, in a self-supervised manner. Cognitive maps are learned in the model by disentangling structural relationships in the inputs from their specific content, a property that can be achieved naturally by updating the positional encoding in Transformers with input-dependent matrices. We developed two variants of MapFormers that unify absolute and relative positional encoding to model episodic (EM) and working memory (WM), respectively. We tested MapFormers on several tasks, including a classic 2D navigation task, showing that our models can learn a cognitive map of the underlying space and generalize OOD (e.g., to longer sequences) with near-perfect performance, unlike current architectures. Together, these results demonstrate the superiority of models designed to learn a cognitive map, and the importance of introducing a structural bias for structure-content disentanglement, which can be achieved in Transformers with input-dependent positional encoding. MapFormers have broad applications in both neuroscience and AI, by explaining the neural mechanisms giving rise to cognitive maps, while allowing these relation models to be learned at scale.",
    "authors": [
      "Victor Rambaud",
      "Salvador Mascarenhas",
      "Yair Lakretz"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19279v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19279v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19269v1",
    "title": "CDLM: Consistency Diffusion Language Models For Faster Sampling",
    "summary": "Diffusion Language Models (DLMs) offer a promising parallel generation paradigm but suffer from slow inference due to numerous refinement steps and the inability to use standard KV caching. We introduce CDLM (Consistency Diffusion Language Models), a training-based acceleration method that simultaneously tackles both bottlenecks. CDLM integrates consistency modeling to drastically reduce the number of required sampling steps by enabling multi-token finalization. Furthermore, we enforce a block-wise causal attention mask during fine-tuning, making the model fully compatible with KV caching. Experiments show CDLM achieves 3.6x-14.5x lower latency while maintaining competitive accuracy on math and coding tasks. The full training and evaluation code is available at https://github.com/SqueezeAILab/CDLM.",
    "authors": [
      "Minseo Kim",
      "Chenfeng Xu",
      "Coleman Hooper",
      "Harman Singh",
      "Ben Athiwaratkun",
      "Ce Zhang",
      "Kurt Keutzer",
      "Amir Gholami"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19269v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19269v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19220v1",
    "title": "Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering",
    "summary": "Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.",
    "authors": [
      "Federico Felizzi",
      "Olivia Riccomi",
      "Michele Ferramola",
      "Francesco Andrea Causio",
      "Manuel Del Medico",
      "Vittorio De Vita",
      "Lorenzo De Mori",
      "Alessandra Piscitelli Pietro Eric Risuleo",
      "Bianca Destro Castaniti",
      "Antonio Cristiano Alessia Longo",
      "Luigi De Angelis",
      "Mariapia Vassalli",
      "Marcello Di Pumpo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19220v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19220v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19202v1",
    "title": "NVGS: Neural Visibility for Occlusion Culling in 3D Gaussian Splatting",
    "summary": "3D Gaussian Splatting can exploit frustum culling and level-of-detail strategies to accelerate rendering of scenes containing a large number of primitives. However, the semi-transparent nature of Gaussians prevents the application of another highly effective technique: occlusion culling. We address this limitation by proposing a novel method to learn the viewpoint-dependent visibility function of all Gaussians in a trained model using a small, shared MLP across instances of an asset in a scene. By querying it for Gaussians within the viewing frustum prior to rasterization, our method can discard occluded primitives during rendering. Leveraging Tensor Cores for efficient computation, we integrate these neural queries directly into a novel instanced software rasterizer. Our approach outperforms the current state of the art for composed scenes in terms of VRAM usage and image quality, utilizing a combination of our instanced rasterizer and occlusion culling MLP, and exhibits complementary properties to existing LoD techniques.",
    "authors": [
      "Brent Zoomers",
      "Florian Hahlbohm",
      "Joni Vanherck",
      "Lode Jorissen",
      "Marcus Magnor",
      "Nick Michiels"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19202v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19202v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19175v1",
    "title": "LLM-Based Agentic Negotiation for 6G: Addressing Uncertainty Neglect and Tail-Event Risk",
    "summary": "A critical barrier to the trustworthiness of sixth-generation (6G) agentic autonomous networks is the uncertainty neglect bias; a cognitive tendency for large language model (LLM)-powered agents to make high-stakes decisions based on simple averages while ignoring the tail risk of extreme events. This paper proposes an unbiased, risk-aware framework for agentic negotiation, designed to ensure robust resource allocation in 6G network slicing. Specifically, agents leverage Digital Twins (DTs) to predict full latency distributions, which are then evaluated using a formal framework from extreme value theory, namely, Conditional Value-at-Risk (CVaR). This approach fundamentally shifts the agent's objective from reasoning over the mean to reasoning over the tail, thereby building a statistically-grounded buffer against worst-case outcomes. Furthermore, our framework ensures full uncertainty awareness by requiring agents to quantify epistemic uncertainty -- confidence in their own DTs predictions -- and propagate this meta-verification to make robust decisions, preventing them from acting on unreliable data. We validate this framework in a 6G inter-slice negotiation use-case between an eMBB and a URLLC agent. The results demonstrate the profound failure of the biased, mean-based baseline, which consistently fails its SLAs with a 25\\% rate. Our unbiased, CVaR-aware agent successfully mitigates this bias, eliminating SLA violations and reducing the URLLC and eMBB p99.999 latencies by around 11\\%. We show this reliability comes at the rational and quantifiable cost of slightly reduced energy savings to 17\\%, exposing the false economy of the biased approach. This work provides a concrete methodology for building the trustworthy autonomous systems required for 6G.",
    "authors": [
      "Hatim Chergui",
      "Farhad Rezazadeh",
      "Mehdi Bennis",
      "Merouane Debbah"
    ],
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19175v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19175v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19157v1",
    "title": "A Robust State Filter Against Unmodeled Process And Measurement Noise",
    "summary": "This paper introduces a novel Kalman filter framework designed to achieve robust state estimation under both process and measurement noise. Inspired by the Weighted Observation Likelihood Filter (WoLF), which provides robustness against measurement outliers, we applied generalized Bayesian approach to build a framework considering both process and measurement noise outliers.",
    "authors": [
      "Weitao Liu"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19157v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19157v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19075v1",
    "title": "Structured Matching via Cost-Regularized Unbalanced Optimal Transport",
    "summary": "Unbalanced optimal transport (UOT) provides a flexible way to match or compare nonnegative finite Radon measures. However, UOT requires a predefined ground transport cost, which may misrepresent the data's underlying geometry. Choosing such a cost is particularly challenging when datasets live in heterogeneous spaces, often motivating practitioners to adopt Gromov-Wasserstein formulations. To address this challenge, we introduce cost-regularized unbalanced optimal transport (CR-UOT), a framework that allows the ground cost to vary while allowing mass creation and removal. We show that CR-UOT incorporates unbalanced Gromov-Wasserstein type problems through families of inner-product costs parameterized by linear transformations, enabling the matching of measures or point clouds across Euclidean spaces. We develop algorithms for such CR-UOT problems using entropic regularization and demonstrate that this approach improves the alignment of heterogeneous single-cell omics profiles, especially when many cells lack direct matches.",
    "authors": [
      "Emanuele Pardini",
      "Katerina Papagiannouli"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.AP"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19075v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19075v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.19436v1",
    "title": "VDC-Agent: When Video Detailed Captioners Evolve Themselves via Agentic Self-Reflection",
    "summary": "We present VDC-Agent, a self-evolving framework for Video Detailed Captioning that requires neither human annotations nor larger teacher models. The agent forms a closed loop of caption generation, principle-guided scoring (score and textual suggestions), and prompt refinement. When caption quality regresses, a self-reflection path leverages the previous chain-of-thought to amend the update. Running this process on unlabeled videos produces trajectories of (caption, score) pairs. We convert the trajectories into preference tuples and filter out samples with JSON parsing errors, resulting in VDC-Agent-19K, which contains 18,886 automatically constructed pairs. We then fine-tune the base MLLM on this dataset using an easy-to-hard curriculum direct preference optimization. Built on Qwen2.5-VL-7B-Instruct, our VDC-Agent-7B attains state-of-the-art performance on the VDC benchmark with 49.08% average accuracy and 2.50 score, surpassing specialized video captioners and improving over the base model by +5.13% accuracy and +0.27 score at similar inference cost.",
    "authors": [
      "Qiang Wang",
      "Xinyuan Gao",
      "SongLin Dong",
      "Jizhou Han",
      "Jiangyang Li",
      "Yuhang He",
      "Yihong Gong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19436v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19436v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19317v1",
    "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset",
    "summary": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.",
    "authors": [
      "Md. Tanzim Ferdous",
      "Naeem Ahsan Chowdhury",
      "Prithwiraj Bhattacharjee"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19317v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19317v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19232v1",
    "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations",
    "summary": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.",
    "authors": [
      "Christos-Nikolaos Zacharopoulos",
      "Revekka Kyriakoglou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19232v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19232v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19184v1",
    "title": "Torsion-Space Diffusion for Protein Backbone Generation with Geometric Refinement",
    "summary": "Designing new protein structures is fundamental to computational biology, enabling advances in therapeutic molecule discovery and enzyme engineering. Existing diffusion-based generative models typically operate in Cartesian coordinate space, where adding noise disrupts strict geometric constraints such as fixed bond lengths and angles, often producing physically invalid structures. To address this limitation, we propose a Torsion-Space Diffusion Model that generates protein backbones by denoising torsion angles, ensuring perfect local geometry by construction. A differentiable forward-kinematics module reconstructs 3D coordinates with fixed 3.8 Angstrom backbone bond lengths while a constrained post-processing refinement optimizes global compactness via Radius of Gyration (Rg) correction, without violating bond constraints. Experiments on standard PDB proteins demonstrate 100% bond-length accuracy and significantly improved structural compactness, reducing Rg error from 70% to 18.6% compared to Cartesian diffusion baselines. Overall, this hybrid torsion-diffusion plus geometric-refinement framework generates physically valid and compact protein backbones, providing a promising path toward full-atom protein generation.",
    "authors": [
      "Lakshaditya Singh",
      "Adwait Shelke",
      "Divyansh Agrawal"
    ],
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19184v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19184v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19166v1",
    "title": "Representational Stability of Truth in Large Language Models",
    "summary": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\\leq 8.2\\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
    "authors": [
      "Samantha Dies",
      "Courtney Maynard",
      "Germans Savcisens",
      "Tina Eliassi-Rad"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19166v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19166v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19063v1",
    "title": "Logic of Montage",
    "summary": "In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form \"Effect of Contradictory Structure.\" \"Effect of Contradictory Structure\" is not static but dynamic. Effect in \"Effect of Contradictory Structure\" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, \"Effect of Contradictory Structure\" can be overlapped with each other. This overlapping operation is called \"montage.\" A broader \"Structure\" that includes related \"Effect of Contradictory Structure\" and \"Effect of Structure\" are set up. Montage produces \"Effect of Structure\". In montage, it is necessary to set something like \"strength,\" so we adopted Deleuze and Deleuze/Guattari's word \"intensity\" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of \"intensity\" through Austin's use of the word \"force.\" \"Effect of Structure\" process is demonstrated using the example of proceeding to the next level of education.",
    "authors": [
      "Hayami Takahashi",
      "Kensuke Takahashi"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19063v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19063v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19062v1",
    "title": "Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation",
    "summary": "Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.",
    "authors": [
      "Qiyang Yu",
      "Yu Fang",
      "Tianrui Li",
      "Xuemei Cao",
      "Yan Chen",
      "Jianghao Li",
      "Fan Min",
      "Yi Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19062v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19062v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19019v1",
    "title": "3D Dynamic Radio Map Prediction Using Vision Transformers for Low-Altitude Wireless Networks",
    "summary": "Low-altitude wireless networks (LAWN) are rapidly expanding with the growing deployment of unmanned aerial vehicles (UAVs) for logistics, surveillance, and emergency response. Reliable connectivity remains a critical yet challenging task due to three-dimensional (3D) mobility, time-varying user density, and limited power budgets. The transmit power of base stations (BSs) fluctuates dynamically according to user locations and traffic demands, leading to a highly non-stationary 3D radio environment. Radio maps (RMs) have emerged as an effective means to characterize spatial power distributions and support radio-aware network optimization. However, most existing works construct static or offline RMs, overlooking real-time power variations and spatio-temporal dependencies in multi-UAV networks. To overcome this limitation, we propose a {3D dynamic radio map (3D-DRM)} framework that learns and predicts the spatio-temporal evolution of received power. Specially, a Vision Transformer (ViT) encoder extracts high-dimensional spatial representations from 3D RMs, while a Transformer-based module models sequential dependencies to predict future power distributions. Experiments unveil that 3D-DRM accurately captures fast-varying power dynamics and substantially outperforms baseline models in both RM reconstruction and short-term prediction.",
    "authors": [
      "Nguyen Duc Minh Quang",
      "Chang Liu",
      "Huy-Trung Nguyen",
      "Shuangyang Li",
      "Derrick Wing Kwan Ng",
      "Wei Xiang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19019v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19019v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.18937v1",
    "title": "Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials",
    "summary": "We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.",
    "authors": [
      "Francois Vandenhende",
      "Anna Georgiou",
      "Michalis Georgiou",
      "Theodoros Psaras",
      "Ellie Karekla",
      "Elena Hadjicosta"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18937v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18937v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.18933v1",
    "title": "Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations",
    "summary": "Large Language Models (LLMs) remain susceptible to jailbreak exploits that bypass safety filters and induce harmful or unethical behavior. This work presents a systematic taxonomy of existing jailbreak defenses across prompt-level, model-level, and training-time interventions, followed by three proposed defense strategies. First, a Prompt-Level Defense Framework detects and neutralizes adversarial inputs through sanitization, paraphrasing, and adaptive system guarding. Second, a Logit-Based Steering Defense reinforces refusal behavior through inference-time vector steering in safety-sensitive layers. Third, a Domain-Specific Agent Defense employs the MetaGPT framework to enforce structured, role-based collaboration and domain adherence. Experiments on benchmark datasets show substantial reductions in attack success rate, achieving full mitigation under the agent-based defense. Overall, this study highlights how jailbreaks pose a significant security threat to LLMs and identifies key intervention points for prevention, while noting that defense strategies often involve trade-offs between safety, performance, and scalability. Code is available at: https://github.com/Kuro0911/CS5446-Project",
    "authors": [
      "Ryan Wong",
      "Hosea David Yu Fei Ng",
      "Dhananjai Sharma",
      "Glenn Jun Jie Ng",
      "Kavishvaran Srinivasan"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18933v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18933v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.18930v1",
    "title": "Learning Solution Operators for Partial Differential Equations via Monte Carlo-Type Approximation",
    "summary": "The Monte Carlo-type Neural Operator (MCNO) introduces a lightweight architecture for learning solution operators for parametric PDEs by directly approximating the kernel integral using a Monte Carlo approach. Unlike Fourier Neural Operators, MCNO makes no spectral or translation-invariance assumptions. The kernel is represented as a learnable tensor over a fixed set of randomly sampled points. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with low computational cost, providing a simple and practical alternative to spectral and graph-based neural operators.",
    "authors": [
      "Salah Eddine Choutri",
      "Prajwal Chauhan",
      "Othmane Mazhar",
      "Saif Eddin Jabari"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18930v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18930v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.18926v1",
    "title": "MoodBench 1.0: An Evaluation Benchmark for Emotional Companionship Dialogue Systems",
    "summary": "With the rapid development of Large Language Models, dialogue systems are shifting from information tools to emotional companions, heralding the era of Emotional Companionship Dialogue Systems (ECDs) that provide personalized emotional support for users. However, the field lacks clear definitions and systematic evaluation standards for ECDs. To address this, we first propose a definition of ECDs with formal descriptions. Then, based on this theory and the design principle of \"Ability Layer-Task Layer (three level)-Data Layer-Method Layer\", we design and implement the first ECD evaluation benchmark - MoodBench 1.0. Through extensive evaluations of 30 mainstream models, we demonstrate that MoodBench 1.0 has excellent discriminant validity and can effectively quantify the differences in emotional companionship abilities among models. Furthermore, the results reveal current models' shortcomings in deep emotional companionship, guiding future technological optimization and significantly aiding developers in enhancing ECDs' user experience.",
    "authors": [
      "Haifeng Jing",
      "Yujie Hou",
      "Junfei Liu",
      "Rui Xie",
      "alan Xu",
      "Jinlong Ma",
      "Qichun Deng"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18926v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18926v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.19437v1",
    "title": "LumiTex: Towards High-Fidelity PBR Texture Generation with Illumination Context",
    "summary": "Physically-based rendering (PBR) provides a principled standard for realistic material-lighting interactions in computer graphics. Despite recent advances in generating PBR textures, existing methods fail to address two fundamental challenges: 1) materials decomposition from image prompts under limited illumination cues, and 2) seamless and view-consistent texture completion. To this end, we propose LumiTex, an end-to-end framework that comprises three key components: (1) a multi-branch generation scheme that disentangles albedo and metallic-roughness under shared illumination priors for robust material understanding, (2) a lighting-aware material attention mechanism that injects illumination context into the decoding process for physically grounded generation of albedo, metallic, and roughness maps, and (3) a geometry-guided inpainting module based on a large view synthesis model that enriches texture coverage and ensures seamless, view-consistent UV completion. Extensive experiments demonstrate that LumiTex achieves state-of-the-art performance in texture quality, surpassing both existing open-source and commercial methods.",
    "authors": [
      "Jingzhi Bao",
      "Hongze Chen",
      "Lingting Zhu",
      "Chenyu Liu",
      "Runze Zhang",
      "Keyang Luo",
      "Zeyu Hu",
      "Weikai Chen",
      "Yingda Yin",
      "Xin Wang",
      "Zehong Lin",
      "Jun Zhang",
      "Xiaoguang Han"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19437v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19437v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19401v1",
    "title": "In-Video Instructions: Visual Signals as Generative Control",
    "summary": "Large-scale video generative models have recently demonstrated strong visual capabilities, enabling the prediction of future frames that adhere to the logical and physical cues in the current observation. In this work, we investigate whether such capabilities can be harnessed for controllable image-to-video generation by interpreting visual signals embedded within the frames as instructions, a paradigm we term In-Video Instruction. In contrast to prompt-based control, which provides textual descriptions that are inherently global and coarse, In-Video Instruction encodes user guidance directly into the visual domain through elements such as overlaid text, arrows, or trajectories. This enables explicit, spatial-aware, and unambiguous correspondences between visual subjects and their intended actions by assigning distinct instructions to different objects. Extensive experiments on three state-of-the-art generators, including Veo 3.1, Kling 2.5, and Wan 2.2, show that video models can reliably interpret and execute such visually embedded instructions, particularly in complex multi-object scenarios.",
    "authors": [
      "Gongfan Fang",
      "Xinyin Ma",
      "Xinchao Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19401v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19401v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19347v1",
    "title": "Artificial Intelligence Driven Workflow for Accelerating Design of Novel Photosensitizers",
    "summary": "The discovery of high-performance photosensitizers has long been hindered by the time-consuming and resource-intensive nature of traditional trial-and-error approaches. Here, we present \\textbf{A}I-\\textbf{A}ccelerated \\textbf{P}hoto\\textbf{S}ensitizer \\textbf{I}nnovation (AAPSI), a closed-loop workflow that integrates expert knowledge, scaffold-based molecule generation, and Bayesian optimization to accelerate the design of novel photosensitizers. The scaffold-driven generation in AAPSI ensures structural novelty and synthetic feasibility, while the iterative AI-experiment loop accelerates the discovery of novel photosensitizers. AAPSI leverages a curated database of 102,534 photosensitizer-solvent pairs and generate 6,148 synthetically accessible candidates. These candidates are screened via graph transformers trained to predict singlet oxygen quantum yield ($φ_Δ$) and absorption maxima ($λ_{max}$), following experimental validation. This work generates several novel candidates for photodynamic therapy (PDT), among which the hypocrellin-based candidate HB4Ph exhibits exceptional performance at the Pareto frontier of high quantum yield of singlet oxygen and long absorption maxima among current photosensitizers ($φ_Δ$=0.85, $λ_{max}$=650nm).",
    "authors": [
      "Hongyi Wang",
      "Xiuli Zheng",
      "Weimin Liu",
      "Zitian Tang",
      "Sheng Gong"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG",
      "physics.chem-ph"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19347v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19347v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19316v1",
    "title": "Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach",
    "summary": "Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.",
    "authors": [
      "Xincheng Wang",
      "Hanchi Sun",
      "Wenjun Sun",
      "Kejun Xue",
      "Wangqiu Zhou",
      "Jianbo Zhang",
      "Wei Sun",
      "Dandan Zhu",
      "Xiongkuo Min",
      "Jun Jia",
      "Zhijun Fang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19316v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19316v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19283v1",
    "title": "Data Flows and Colonial Regimes in Africa: A Critical Analysis of the Colonial Futurities Embedded in AI Ecosystems",
    "summary": "This chapter seeks to frame the elemental and invisible problems of AI and big data in the African context by examining digital sites and infrastructure through the lens of power and interests. It will present reflections on how these sites are using AI recommendation algorithms to recreate new digital societies in the region, how they have the potential to propagate algorithmic colonialism and negative gender norms, and what this means for the regional sustainable development agenda. The chapter proposes adopting business models that embrace response-ability and consider the existence of alternative socio-material worlds of AI. These reflections will mainly come from ongoing discussions with Kenyan social media users in this authors' user space talks, personal experiences and six months of active participant observations done by the authors.",
    "authors": [
      "Ndaka. A",
      "Avila-Acosta. F",
      "Mbula-Ndaka. H",
      "Amera. C",
      "Chauke. S",
      "Majiwa. E"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19283v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19283v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19241v1",
    "title": "Local Entropy Search over Descent Sequences for Bayesian Optimization",
    "summary": "Searching large and complex design spaces for a global optimum can be infeasible and unnecessary. A practical alternative is to iteratively refine the neighborhood of an initial design using local optimization methods such as gradient descent. We propose local entropy search (LES), a Bayesian optimization paradigm that explicitly targets the solutions reachable by the descent sequences of iterative optimizers. The algorithm propagates the posterior belief over the objective through the optimizer, resulting in a probability distribution over descent sequences. It then selects the next evaluation by maximizing mutual information with that distribution, using a combination of analytic entropy calculations and Monte-Carlo sampling of descent sequences. Empirical results on high-complexity synthetic objectives and benchmark problems show that LES achieves strong sample efficiency compared to existing local and global Bayesian optimization methods.",
    "authors": [
      "David Stenger",
      "Armin Lindicke",
      "Alexander von Rohr",
      "Sebastian Trimpe"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19241v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19241v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19165v1",
    "title": "First-order Sobolev Reinforcement Learning",
    "summary": "We propose a refinement of temporal-difference learning that enforces first-order Bellman consistency: the learned value function is trained to match not only the Bellman targets in value but also their derivatives with respect to states and actions. By differentiating the Bellman backup through differentiable dynamics, we obtain analytically consistent gradient targets. Incorporating these into the critic objective using a Sobolev-type loss encourages the critic to align with both the value and local geometry of the target function. This first-order TD matching principle can be seamlessly integrated into existing algorithms, such as Q-learning or actor-critic methods (e.g., DDPG, SAC), potentially leading to faster critic convergence and more stable policy gradients without altering their overall structure.",
    "authors": [
      "Fabian Schramm",
      "Nicolas Perrin-Gilbert",
      "Justin Carpentier"
    ],
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19165v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19165v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19134v1",
    "title": "MambaRefine-YOLO: A Dual-Modality Small Object Detector for UAV Imagery",
    "summary": "Small object detection in Unmanned Aerial Vehicle (UAV) imagery is a persistent challenge, hindered by low resolution and background clutter. While fusing RGB and infrared (IR) data offers a promising solution, existing methods often struggle with the trade-off between effective cross-modal interaction and computational efficiency. In this letter, we introduce MambaRefine-YOLO. Its core contributions are a Dual-Gated Complementary Mamba fusion module (DGC-MFM) that adaptively balances RGB and IR modalities through illumination-aware and difference-aware gating mechanisms, and a Hierarchical Feature Aggregation Neck (HFAN) that uses a ``refine-then-fuse'' strategy to enhance multi-scale features. Our comprehensive experiments validate this dual-pronged approach. On the dual-modality DroneVehicle dataset, the full model achieves a state-of-the-art mAP of 83.2%, an improvement of 7.9% over the baseline. On the single-modality VisDrone dataset, a variant using only the HFAN also shows significant gains, demonstrating its general applicability. Our work presents a superior balance between accuracy and speed, making it highly suitable for real-world UAV applications.",
    "authors": [
      "Shuyu Cao",
      "Minxin Chen",
      "Yucheng Song",
      "Zhaozhong Chen",
      "Xinyou Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19134v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19134v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.18964v1",
    "title": "Synthesizing Visual Concepts as Vision-Language Programs",
    "summary": "Vision-Language models (VLMs) achieve strong performance on multimodal tasks but often fail at systematic visual reasoning tasks, leading to inconsistent or illogical outputs. Neuro-symbolic methods promise to address this by inducing interpretable logical rules, though they exploit rigid, domain-specific perception modules. We propose Vision-Language Programs (VLP), which combine the perceptual flexibility of VLMs with systematic reasoning of program synthesis. Rather than embedding reasoning inside the VLM, VLP leverages the model to produce structured visual descriptions that are compiled into neuro-symbolic programs. The resulting programs execute directly on images, remain consistent with task constraints, and provide human-interpretable explanations that enable easy shortcut mitigation. Experiments on synthetic and real-world datasets demonstrate that VLPs outperform direct and structured prompting, particularly on tasks requiring complex logical reasoning.",
    "authors": [
      "Antonia Wüst",
      "Wolfgang Stammer",
      "Hikaru Shindo",
      "Lukas Helff",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18964v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18964v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.18958v1",
    "title": "Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation",
    "summary": "As graph-structured data grow increasingly large, evaluating their robustness under adversarial attacks becomes computationally expensive and difficult to scale. To address this challenge, we propose to compress graphs into compact representations that preserve both topological structure and robustness profile, enabling efficient and reliable evaluation.We propose Cutter, a dual-agent reinforcement learning framework composed of a Vital Detection Agent (VDA) and a Redundancy Detection Agent (RDA), which collaboratively identify structurally vital and redundant nodes for guided compression. Cutter incorporates three key strategies to enhance learning efficiency and compression quality: trajectory-level reward shaping to transform sparse trajectory returns into dense, policy-equivalent learning signals; prototype-based shaping to guide decisions using behavioral patterns from both highand low-return trajectories; and cross-agent imitation to enable safer and more transferable exploration. Experiments on multiple real-world graphs demonstrate that Cutter generates compressed graphs that retain essential static topological properties and exhibit robustness degradation trends highly consistent with the original graphs under various attack scenarios, thereby significantly improving evaluation efficiency without compromising assessment fidelity.",
    "authors": [
      "Qisen Chai",
      "Yansong Wang",
      "Junjie Huang",
      "Tao Jia"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18958v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18958v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.19423v1",
    "title": "Beyond Protein Language Models: An Agentic LLM Framework for Mechanistic Enzyme Design",
    "summary": "We present Genie-CAT, a tool-augmented large-language-model (LLM) system designed to accelerate scientific hypothesis generation in protein design. Using metalloproteins (e.g., ferredoxins) as a case study, Genie-CAT integrates four capabilities -- literature-grounded reasoning through retrieval-augmented generation (RAG), structural parsing of Protein Data Bank files, electrostatic potential calculations, and machine-learning prediction of redox properties -- into a unified agentic workflow. By coupling natural-language reasoning with data-driven and physics-based computation, the system generates mechanistically interpretable, testable hypotheses linking sequence, structure, and function. In proof-of-concept demonstrations, Genie-CAT autonomously identifies residue-level modifications near [Fe--S] clusters that affect redox tuning, reproducing expert-derived hypotheses in a fraction of the time. The framework highlights how AI agents combining language models with domain-specific tools can bridge symbolic reasoning and numerical simulation, transforming LLMs from conversational assistants into partners for computational discovery.",
    "authors": [
      "Bruno Jacob",
      "Khushbu Agarwal",
      "Marcel Baer",
      "Peter Rice",
      "Simone Raugei"
    ],
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19423v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19423v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19324v1",
    "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
    "summary": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
    "authors": [
      "Roksana Goworek",
      "Olivia Macmillan-Scott",
      "Eda B. Özyiğit"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19324v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19324v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19319v1",
    "title": "SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis",
    "summary": "Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.",
    "authors": [
      "Lingwei Dang",
      "Zonghan Li",
      "Juntong Li",
      "Hongwen Zhang",
      "Liang An",
      "Yebin Liu",
      "Qingyao Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19319v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19319v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19236v1",
    "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control",
    "summary": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.",
    "authors": [
      "Yuxuan Wang",
      "Haobin Jiang",
      "Shiqing Yao",
      "Ziluo Ding",
      "Zongqing Lu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19236v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19236v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19217v1",
    "title": "ReAlign: Text-to-Motion Generation via Step-Aware Reward-Guided Alignment",
    "summary": "Text-to-motion generation, which synthesizes 3D human motions from text inputs, holds immense potential for applications in gaming, film, and robotics. Recently, diffusion-based methods have been shown to generate more diversity and realistic motion. However, there exists a misalignment between text and motion distributions in diffusion models, which leads to semantically inconsistent or low-quality motions. To address this limitation, we propose Reward-guided sampling Alignment (ReAlign), comprising a step-aware reward model to assess alignment quality during the denoising sampling and a reward-guided strategy that directs the diffusion process toward an optimally aligned distribution. This reward model integrates step-aware tokens and combines a text-aligned module for semantic consistency and a motion-aligned module for realism, refining noisy motions at each timestep to balance probability density and alignment. Extensive experiments of both motion generation and retrieval tasks demonstrate that our approach significantly improves text-motion alignment and motion quality compared to existing state-of-the-art methods.",
    "authors": [
      "Wanjiang Weng",
      "Xiaofeng Tan",
      "Junbo Wang",
      "Guo-Sen Xie",
      "Pan Zhou",
      "Hongsong Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19217v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19217v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19126v1",
    "title": "When Semantics Regulate: Rethinking Patch Shuffle and Internal Bias for Generated Image Detection with CLIP",
    "summary": "The rapid progress of GANs and Diffusion Models poses new challenges for detecting AI-generated images. Although CLIP-based detectors exhibit promising generalization, they often rely on semantic cues rather than generator artifacts, leading to brittle performance under distribution shifts. In this work, we revisit the nature of semantic bias and uncover that Patch Shuffle provides an unusually strong benefit for CLIP, that disrupts global semantic continuity while preserving local artifact cues, which reduces semantic entropy and homogenizes feature distributions between natural and synthetic images. Through a detailed layer-wise analysis, we further show that CLIP's deep semantic structure functions as a regulator that stabilizes cross-domain representations once semantic bias is suppressed. Guided by these findings, we propose SemAnti, a semantic-antagonistic fine-tuning paradigm that freezes the semantic subspace and adapts only artifact-sensitive layers under shuffled semantics. Despite its simplicity, SemAnti achieves state-of-the-art cross-domain generalization on AIGCDetectBenchmark and GenImage, demonstrating that regulating semantics is key to unlocking CLIP's full potential for robust AI-generated image detection.",
    "authors": [
      "Beilin Chu",
      "Weike You",
      "Mengtao Li",
      "Tingting Zheng",
      "Kehan Zhao",
      "Xuan Xu",
      "Zhigao Lu",
      "Jia Song",
      "Moxuan Xu",
      "Linna Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19126v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19126v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19090v1",
    "title": "Optimization of Deep Learning Models for Dynamic Market Behavior Prediction",
    "summary": "The advent of financial technology has witnessed a surge in the utilization of deep learning models to anticipate consumer conduct, a trend that has demonstrated considerable potential in enhancing lending strategies and bolstering market efficiency. We study multi-horizon demand forecasting on e-commerce transactions using the UCI Online Retail II dataset. Unlike prior versions of this manuscript that mixed financial-loan narratives with retail data, we focus exclusively on retail market behavior and define a clear prediction target: per SKU daily demand (or revenue) for horizons H=1,7,14. We present a hybrid sequence model that combines multi-scale temporal convolutions, a gated recurrent module, and time-aware self-attention. The model is trained with standard regression losses and evaluated under MAE, RMSE, sMAPE, MASE, and Theil's U_2 with strict time-based splits to prevent leakage. We benchmark against ARIMA/Prophet, LSTM/GRU, LightGBM, and state-of-the-art Transformer forecasters (TFT, Informer, Autoformer, N-BEATS). Results show consistent accuracy gains and improved robustness on peak/holiday periods. We further provide ablations and statistical significance tests to ensure the reliability of improvements, and we release implementation details to facilitate reproducibility.",
    "authors": [
      "Shenghan Zhao",
      "Yuzhen Lin",
      "Ximeng Yang",
      "Qiaochu Lu",
      "Haozhong Xue",
      "Gaozhe Jiang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19090v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19090v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.19343v1",
    "title": "Syn-GRPO: Self-Evolving Data Synthesis for MLLM Perception Reasoning",
    "summary": "RL (reinforcement learning) methods (e.g., GRPO) for MLLM (Multimodal LLM) perception ability has attracted wide research interest owing to its remarkable generalization ability. Nevertheless, existing reinforcement learning methods still face the problem of low data quality, where data samples cannot elicit diverse responses from MLLMs, thus restricting the exploration scope for MLLM reinforcement learning. Some methods attempt to mitigate this problem by imposing constraints on entropy, but none address it at its root. Therefore, to tackle this problem, this work proposes Syn-GRPO (Synthesis-GRPO), which employs an online data generator to synthesize high-quality training data with diverse responses in GRPO training. Specifically, Syn-GRPO consists of two components: (1) data server; (2) GRPO workflow. The data server synthesizes new samples from existing ones using an image generation model, featuring a decoupled and asynchronous scheme to achieve high generation efficiency. The GRPO workflow provides the data server with the new image descriptions, and it leverages a diversity reward to supervise the MLLM to predict image descriptions for synthesizing samples with diverse responses. Experiment results across three visual perception tasks demonstrate that Syn-GRPO improves the data quality by a large margin, achieving significant superior performance to existing MLLM perception methods, and Syn-GRPO presents promising potential for scaling long-term self-evolving RL. Our code is available at https://github.com/hqhQAQ/Syn-GRPO.",
    "authors": [
      "Qihan Huang",
      "Haofei Zhang",
      "Rong Wei",
      "Yi Wang",
      "Rui Tang",
      "Mingli Song",
      "Jie Song"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19343v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19343v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19325v1",
    "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
    "summary": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
    "authors": [
      "Olivia Macmillan-Scott",
      "Roksana Goworek",
      "Eda B. Özyiğit"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19325v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19325v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19278v1",
    "title": "ReMatch: Boosting Representation through Matching for Multimodal Retrieval",
    "summary": "We present ReMatch, a framework that leverages the generative strength of MLLMs for multimodal retrieval. Previous approaches treated an MLLM as a simple encoder, ignoring its generative nature, and under-utilising its compositional reasoning and world knowledge. We instead train the embedding MLLM end-to-end with a chat-style generative matching stage. The matching stage uses the same MLLM to autoregressively decide relevance from multi-view inputs, including both raw data and its own projected embeddings for each query and document. It provides instance-wise discrimination supervision that complements a standard contrastive loss, offering stronger gradients on hard negatives and preserving the compositional strengths of the original MLLM. To obtain semantically richer multimodal embeddings, we use multiple learnable tokens to augment each input, generating fine-grained contextual, mutually orthogonal embeddings with low inference cost. Leveraging our established high-performance baseline,we assemble the ideas mentioned above into a powerful training recipe and achieve a new state-of-the-art on the Massive Multimodal Embedding Benchmark (MMEB). Our experiments show particularly strong zero-shot generalization results on five datasets, highlighting the robustness and transferability of ReMatch.",
    "authors": [
      "Qianying Liu",
      "Xiao Liang",
      "Zhiqiang Zhang",
      "Yibo Chen",
      "Xu Tang",
      "Zhongfei Qing",
      "Fengfan Zhou",
      "Yao Hu",
      "Paul Henderson"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19278v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19278v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19254v1",
    "title": "Adversarial Patch Attacks on Vision-Based Cargo Occupancy Estimation via Differentiable 3D Simulation",
    "summary": "Computer vision systems are increasingly adopted in modern logistics operations, including the estimation of trailer occupancy for planning, routing, and billing. Although effective, such systems may be vulnerable to physical adversarial attacks, particularly adversarial patches that can be printed and placed on interior surfaces. In this work, we study the feasibility of such attacks on a convolutional cargo-occupancy classifier using fully simulated 3D environments. Using Mitsuba 3 for differentiable rendering, we optimize patch textures across variations in geometry, lighting, and viewpoint, and compare their effectiveness to a 2D compositing baseline. Our experiments demonstrate that 3D-optimized patches achieve high attack success rates, especially in a denial-of-service scenario (empty to full), where success reaches 84.94 percent. Concealment attacks (full to empty) prove more challenging but still reach 30.32 percent. We analyze the factors influencing attack success, discuss implications for the security of automated logistics pipelines, and highlight directions for strengthening physical robustness. To our knowledge, this is the first study to investigate adversarial patch attacks for cargo-occupancy estimation in physically realistic, fully simulated 3D scenes.",
    "authors": [
      "Mohamed Rissal Hedna",
      "Sesugh Samuel Nder"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19254v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19254v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19145v1",
    "title": "ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation",
    "summary": "We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.",
    "authors": [
      "Dongha Lee",
      "Jinhee Park",
      "Minjun Kim",
      "Junseok Kwon"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19145v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19145v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19087v1",
    "title": "EnfoPath: Energy-Informed Analysis of Generative Trajectories in Flow Matching",
    "summary": "Flow-based generative models synthesize data by integrating a learned velocity field from a reference distribution to the target data distribution. Prior work has focused on endpoint metrics (e.g., fidelity, likelihood, perceptual quality) while overlooking a deeper question: what do the sampling trajectories reveal? Motivated by classical mechanics, we introduce kinetic path energy (KPE), a simple yet powerful diagnostic that quantifies the total kinetic effort along each generation path of ODE-based samplers. Through comprehensive experiments on CIFAR-10 and ImageNet-256, we uncover two key phenomena: ({i}) higher KPE predicts stronger semantic quality, indicating that semantically richer samples require greater kinetic effort, and ({ii}) higher KPE inversely correlates with data density, with informative samples residing in sparse, low-density regions. Together, these findings reveal that semantically informative samples naturally reside on the sparse frontier of the data distribution, demanding greater generative effort. Our results suggest that trajectory-level analysis offers a physics-inspired and interpretable framework for understanding generation difficulty and sample characteristics.",
    "authors": [
      "Ziyun Li",
      "Ben Dai",
      "Huancheng Hu",
      "Henrik Boström",
      "Soon Hoe Lim"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19087v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19087v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19065v1",
    "title": "Understanding, Accelerating, and Improving MeanFlow Training",
    "summary": "MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.",
    "authors": [
      "Jin-Young Kim",
      "Hyojun Go",
      "Lea Bogensperger",
      "Julius Erbach",
      "Nikolai Kalischek",
      "Federico Tombari",
      "Konrad Schindler",
      "Dominik Narnhofer"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19065v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19065v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.18992v1",
    "title": "Classification EM-PCA for clustering and embedding",
    "summary": "The mixture model is undoubtedly one of the greatest contributions to clustering. For continuous data, Gaussian models are often used and the Expectation-Maximization (EM) algorithm is particularly suitable for estimating parameters from which clustering is inferred. If these models are particularly popular in various domains including image clustering, they however suffer from the dimensionality and also from the slowness of convergence of the EM algorithm. However, the Classification EM (CEM) algorithm, a classifying version, offers a fast convergence solution while dimensionality reduction still remains a challenge. Thus we propose in this paper an algorithm combining simultaneously and non-sequentially the two tasks --Data embedding and Clustering-- relying on Principal Component Analysis (PCA) and CEM. We demonstrate the interest of such approach in terms of clustering and data embedding. We also establish different connections with other clustering approaches.",
    "authors": [
      "Zineddine Tighidet",
      "Lazhar Labiod",
      "Mohamed Nadif"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18992v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18992v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.18968v1",
    "title": "CataractCompDetect: Intraoperative Complication Detection in Cataract Surgery",
    "summary": "Cataract surgery is one of the most commonly performed surgeries worldwide, yet intraoperative complications such as iris prolapse, posterior capsule rupture (PCR), and vitreous loss remain major causes of adverse outcomes. Automated detection of such events could enable early warning systems and objective training feedback. In this work, we propose CataractCompDetect, a complication detection framework that combines phase-aware localization, SAM 2-based tracking, complication-specific risk scoring, and vision-language reasoning for final classification. To validate CataractCompDetect, we curate CataComp, the first cataract surgery video dataset annotated for intraoperative complications, comprising 53 surgeries, including 23 with clinical complications. On CataComp, CataractCompDetect achieves an average F1 score of 70.63%, with per-complication performance of 81.8% (Iris Prolapse), 60.87% (PCR), and 69.23% (Vitreous Loss). These results highlight the value of combining structured surgical priors with vision-language reasoning for recognizing rare but high-impact intraoperative events. Our dataset and code will be publicly released upon acceptance.",
    "authors": [
      "Bhuvan Sachdeva",
      "Sneha Kumari",
      "Rudransh Agarwal",
      "Shalaka Kumaraswamy",
      "Niharika Singri Prasad",
      "Simon Mueller",
      "Raphael Lechtenboehmer",
      "Maximilian W. M. Wintergerst",
      "Thomas Schultz",
      "Kaushik Murali",
      "Mohit Jain"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18968v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18968v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.19430v1",
    "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution",
    "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT",
    "authors": [
      "Dingkang Liang",
      "Cheng Zhang",
      "Xiaopeng Xu",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Xiang Bai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19430v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19430v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19339v1",
    "title": "POUR: A Provably Optimal Method for Unlearning Representations via Neural Collapse",
    "summary": "In computer vision, machine unlearning aims to remove the influence of specific visual concepts or training images without retraining from scratch. Studies show that existing approaches often modify the classifier while leaving internal representations intact, resulting in incomplete forgetting. In this work, we extend the notion of unlearning to the representation level, deriving a three-term interplay between forgetting efficacy, retention fidelity, and class separation. Building on Neural Collapse theory, we show that the orthogonal projection of a simplex Equiangular Tight Frame (ETF) remains an ETF in a lower dimensional space, yielding a provably optimal forgetting operator. We further introduce the Representation Unlearning Score (RUS) to quantify representation-level forgetting and retention fidelity. Building on this, we introduce POUR (Provably Optimal Unlearning of Representations), a geometric projection method with closed-form (POUR-P) and a feature-level unlearning variant under a distillation scheme (POUR-D). Experiments on CIFAR-10/100 and PathMNIST demonstrate that POUR achieves effective unlearning while preserving retained knowledge, outperforming state-of-the-art unlearning methods on both classification-level and representation-level metrics.",
    "authors": [
      "Anjie Le",
      "Can Peng",
      "Yuyuan Liu",
      "J. Alison Noble"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19339v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19339v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19268v1",
    "title": "BideDPO: Conditional Image Generation with Simultaneous Text and Condition Alignment",
    "summary": "Conditional image generation enhances text-to-image synthesis with structural, spatial, or stylistic priors, but current methods face challenges in handling conflicts between sources. These include 1) input-level conflicts, where the conditioning image contradicts the text prompt, and 2) model-bias conflicts, where generative biases disrupt alignment even when conditions match the text. Addressing these conflicts requires nuanced solutions, which standard supervised fine-tuning struggles to provide. Preference-based optimization techniques like Direct Preference Optimization (DPO) show promise but are limited by gradient entanglement between text and condition signals and lack disentangled training data for multi-constraint tasks. To overcome this, we propose a bidirectionally decoupled DPO framework (BideDPO). Our method creates two disentangled preference pairs-one for the condition and one for the text-to reduce gradient entanglement. The influence of pairs is managed using an Adaptive Loss Balancing strategy for balanced optimization. We introduce an automated data pipeline to sample model outputs and generate conflict-aware data. This process is embedded in an iterative optimization strategy that refines both the model and the data. We construct a DualAlign benchmark to evaluate conflict resolution between text and condition. Experiments show BideDPO significantly improves text success rates (e.g., +35%) and condition adherence. We also validate our approach using the COCO dataset. Project Pages: https://limuloo.github.io/BideDPO/.",
    "authors": [
      "Dewei Zhou",
      "Mingwei Li",
      "Zongxin Yang",
      "Yu Lu",
      "Yunqiu Xu",
      "Zhizhong Wang",
      "Zeyi Huang",
      "Yi Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19268v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19268v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19262v1",
    "title": "Psychometric Tests for AI Agents and Their Moduli Space",
    "summary": "We develop a moduli-theoretic view of psychometric test batteries for AI agents and connect it explicitly to the AAI score developed previously. First, we make precise the notion of an AAI functional on a battery and set out axioms that any reasonable autonomy/general intelligence score should satisfy. Second, we show that the composite index ('AAI-Index') defined previously is a special case of our AAI functional. Third, we introduce the notion of a cognitive core of an agent relative to a battery and define the associated AAI$_{\\textrm{core}}$ score as the restriction of an AAI functional to that core. Finally, we use these notions to describe invariants of batteries under evaluation-preserving symmetries and outline how moduli of equivalent batteries are organized.",
    "authors": [
      "Przemyslaw Chojecki"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.ST"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19262v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19262v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19260v1",
    "title": "A Nutrition Multimodal Photoplethysmography Language Model",
    "summary": "Hunger and satiety dynamics shape dietary behaviors and metabolic health, yet remain difficult to capture in everyday settings. We present a Nutrition Photoplethysmography Language Model (NPLM), integrating continuous photoplethysmography (PPG) from wearables with meal descriptions. NPLM projects PPG into embeddings interpretable by language models, enabling joint reasoning over physiology and meal context. Trained on 19,340 participants and 1.1 million meal-PPG pairs, the model improved daily caloric intake prediction by 11% over text-only baselines, with accuracy maintained when 80% of meal text was removed. In an independent validation study (n=140) with controlled dining and detailed meal information, the model replicated these findings. These results demonstrate the value of integrating physiological measurements from consumer wearables with meal information for noninvasive dietary monitoring at scale.",
    "authors": [
      "Kyle Verrier",
      "Achille Nazaret",
      "Joseph Futoma",
      "Andrew C. Miller",
      "Guillermo Sapiro"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19260v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19260v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19198v1",
    "title": "Three-Dimensional Anatomical Data Generation Based on Artificial Neural Networks",
    "summary": "Surgical planning and training based on machine learning requires a large amount of 3D anatomical models reconstructed from medical imaging, which is currently one of the major bottlenecks. Obtaining these data from real patients and during surgery is very demanding, if even possible, due to legal, ethical, and technical challenges. It is especially difficult for soft tissue organs with poor imaging contrast, such as the prostate. To overcome these challenges, we present a novel workflow for automated 3D anatomical data generation using data obtained from physical organ models. We additionally use a 3D Generative Adversarial Network (GAN) to obtain a manifold of 3D models useful for other downstream machine learning tasks that rely on 3D data. We demonstrate our workflow using an artificial prostate model made of biomimetic hydrogels with imaging contrast in multiple zones. This is used to physically simulate endoscopic surgery. For evaluation and 3D data generation, we place it into a customized ultrasound scanner that records the prostate before and after the procedure. A neural network is trained to segment the recorded ultrasound images, which outperforms conventional, non-learning-based computer vision techniques in terms of intersection over union (IoU). Based on the segmentations, a 3D mesh model is reconstructed, and performance feedback is provided.",
    "authors": [
      "Ann-Sophia Müller",
      "Moonkwang Jeong",
      "Meng Zhang",
      "Jiyuan Tian",
      "Arkadiusz Miernik",
      "Stefanie Speidel",
      "Tian Qiu"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19198v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19198v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19107v1",
    "title": "The Core in Max-Loss Non-Centroid Clustering Can Be Empty",
    "summary": "We study core stability in non-centroid clustering under the max-loss objective, where each agent's loss is the maximum distance to other members of their cluster. We prove that for all $k\\geq 3$ there exist metric instances with $n\\ge 9$ agents, with $n$ divisible by $k$, for which no clustering lies in the $α$-core for any $α<2^{\\frac{1}{5}}\\sim 1.148$. The bound is tight for our construction. Using a computer-aided proof, we also identify a two-dimensional Euclidean point set whose associated lower bound is slightly smaller than that of our general construction. This is, to our knowledge, the first impossibility result showing that the core can be empty in non-centroid clustering under the max-loss objective.",
    "authors": [
      "Robert Bredereck",
      "Eva Deltl",
      "Leon Kellerhals",
      "Jannik Peters"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19107v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19107v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19049v1",
    "title": "Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation",
    "summary": "Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.",
    "authors": [
      "Ruojun Xu",
      "Yu Kai",
      "Xuhua Ren",
      "Jiaxiang Cheng",
      "Bing Ma",
      "Tianxiang Zheng",
      "Qinhlin Lu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19049v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19049v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19004v1",
    "title": "A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation",
    "summary": "Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.",
    "authors": [
      "Wentao Qu",
      "Guofeng Mei",
      "Yang Wu",
      "Yongshun Gong",
      "Xiaoshui Huang",
      "Liang Xiao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19004v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19004v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.18987v1",
    "title": "Dynamic Mixture of Experts Against Severe Distribution Shifts",
    "summary": "The challenge of building neural networks that can continuously learn and adapt to evolving data streams is central to the fields of continual learning (CL) and reinforcement learning (RL). This lifelong learning problem is often framed in terms of the plasticity-stability dilemma, focusing on issues like loss of plasticity and catastrophic forgetting. Unlike neural networks, biological brains maintain plasticity through capacity growth, inspiring researchers to explore similar approaches in artificial networks, such as adding capacity dynamically. Prior solutions often lack parameter efficiency or depend on explicit task indices, but Mixture-of-Experts (MoE) architectures offer a promising alternative by specializing experts for distinct distributions. This paper aims to evaluate a DynamicMoE approach for continual and reinforcement learning environments and benchmark its effectiveness against existing network expansion methods.",
    "authors": [
      "Donghu Kim"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18987v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18987v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.18929v1",
    "title": "Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search",
    "summary": "Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that directly assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across multiple plausible futures. To facilitate this study, we propose an HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes the complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.",
    "authors": [
      "Zijian Song",
      "Xiaoxin Lin",
      "Tao Pu",
      "Zhenlong Yuan",
      "Guangrun Wang",
      "Liang Lin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18929v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18929v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.18924v1",
    "title": "LLM-Driven Kernel Evolution: Automating Driver Updates in Linux",
    "summary": "Linux kernel evolution breaks drivers through API/ABI changes, semantic shifts, and security-hardening updates. We introduce DRIVEBENCH, an executable corpus of kernel$\\rightarrow$driver co-evolution cases, and AUTODRIVER, a closed-loop, LLM-driven system for automating driver maintenance. The system integrates prompt engineering, multi-agent collaboration, static analysis, and iterative validation to ensure that generated patches are not only syntactically correct but also functionally and semantically consistent with kernel conventions. The corpus spans v5.10-v6.10 with 235 validated cases drawn from 612 candidates. In evaluation across 55 cases, AUTODRIVER achieves 56.4% compilation success; QEMU-based boot verification indicates that compiled patches preserve driver initialization in most instances. By releasing DRIVEBENCH and tooling, we enable reproducible research and a practical route to continuous, safe co-evolution of drivers with the Linux kernel.",
    "authors": [
      "Arina Kharlamova",
      "Jiawen Liu",
      "Tianyi Zhang",
      "Xinrui Yang",
      "Humaid Alqasimi",
      "Youcheng Sun",
      "Chun Jason Xue"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18924v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18924v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.19314v1",
    "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
    "summary": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
    "authors": [
      "Jaewoo Lee",
      "Archiki Prasad",
      "Justin Chih-Yao Chen",
      "Zaid Khan",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19314v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19314v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19289v1",
    "title": "Performance Guarantees for Quantum Neural Estimation of Entropies",
    "summary": "Estimating quantum entropies and divergences is an important problem in quantum physics, information theory, and machine learning. Quantum neural estimators (QNEs), which utilize a hybrid classical-quantum architecture, have recently emerged as an appealing computational framework for estimating these measures. Such estimators combine classical neural networks with parametrized quantum circuits, and their deployment typically entails tedious tuning of hyperparameters controlling the sample size, network architecture, and circuit topology. This work initiates the study of formal guarantees for QNEs of measured (Rényi) relative entropies in the form of non-asymptotic error risk bounds. We further establish exponential tail bounds showing that the error is sub-Gaussian, and thus sharply concentrates about the ground truth value. For an appropriate sub-class of density operator pairs on a space of dimension $d$ with bounded Thompson metric, our theory establishes a copy complexity of $O(|Θ(\\mathcal{U})|d/ε^2)$ for QNE with a quantum circuit parameter set $Θ(\\mathcal{U})$, which has minimax optimal dependence on the accuracy $ε$. Additionally, if the density operator pairs are permutation invariant, we improve the dimension dependence above to $O(|Θ(\\mathcal{U})|\\mathrm{polylog}(d)/ε^2)$. Our theory aims to facilitate principled implementation of QNEs for measured relative entropies and guide hyperparameter tuning in practice.",
    "authors": [
      "Sreejith Sreekumar",
      "Ziv Goldfeld",
      "Mark M. Wilde"
    ],
    "categories": [
      "quant-ph",
      "cs.IT",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19289v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19289v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19284v1",
    "title": "The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility",
    "summary": "This document proposes a Unified Robust Framework that re-engineers the estimation of the Average Treatment Effect on the Overlap (ATO). It synthesizes gamma-Divergence for outlier robustness, Graduated Non-Convexity (GNC) for global optimization, and a \"Gatekeeper\" mechanism to address the impossibility of higher-order orthogonality in Gaussian regimes.",
    "authors": [
      "Eichi Uehara"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19284v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19284v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19274v1",
    "title": "Diffusion Reconstruction-based Data Likelihood Estimation for Core-Set Selection",
    "summary": "Existing core-set selection methods predominantly rely on heuristic scoring signals such as training dynamics or model uncertainty, lacking explicit modeling of data likelihood. This omission may hinder the constructed subset from capturing subtle yet critical distributional structures that underpin effective model training. In this work, we propose a novel, theoretically grounded approach that leverages diffusion models to estimate data likelihood via reconstruction deviation induced by partial reverse denoising. Specifically, we establish a formal connection between reconstruction error and data likelihood, grounded in the Evidence Lower Bound (ELBO) of Markovian diffusion processes, thereby enabling a principled, distribution-aware scoring criterion for data selection. Complementarily, we introduce an efficient information-theoretic method to identify the optimal reconstruction timestep, ensuring that the deviation provides a reliable signal indicative of underlying data likelihood. Extensive experiments on ImageNet demonstrate that reconstruction deviation offers an effective scoring criterion, consistently outperforming existing baselines across selection ratios, and closely matching full-data training using only 50% of the data. Further analysis shows that the likelihood-informed nature of our score reveals informative insights in data selection, shedding light on the interplay between data distributional characteristics and model learning preferences.",
    "authors": [
      "Mingyang Chen",
      "Jiawei Du",
      "Bo Huang",
      "Yi Wang",
      "Xiaobo Zhang",
      "Wei Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19274v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19274v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19264v1",
    "title": "Interpreting GFlowNets for Drug Discovery: Extracting Actionable Insights for Medicinal Chemistry",
    "summary": "Generative Flow Networks, or GFlowNets, offer a promising framework for molecular design, but their internal decision policies remain opaque. This limits adoption in drug discovery, where chemists require clear and interpretable rationales for proposed structures. We present an interpretability framework for SynFlowNet, a GFlowNet trained on documented chemical reactions and purchasable starting materials that generates both molecules and the synthetic routes that produce them. Our approach integrates three complementary components. Gradient based saliency combined with counterfactual perturbations identifies which atomic environments influence reward and how structural edits change molecular outcomes. Sparse autoencoders reveal axis aligned latent factors that correspond to physicochemical properties such as polarity, lipophilicity, and molecular size. Motif probes show that functional groups including aromatic rings and halogens are explicitly encoded and linearly decodable from the internal embeddings. Together, these results expose the chemical logic inside SynFlowNet and provide actionable and mechanistic insight that supports transparent and controllable molecular design.",
    "authors": [
      "Amirtha Varshini A S",
      "Duminda S. Ranasinghe",
      "Hok Hei Tam"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19264v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19264v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19183v1",
    "title": "nnActive: A Framework for Evaluation of Active Learning in 3D Biomedical Segmentation",
    "summary": "Semantic segmentation is crucial for various biomedical applications, yet its reliance on large annotated datasets presents a bottleneck due to the high cost and specialized expertise required for manual labeling. Active Learning (AL) aims to mitigate this challenge by querying only the most informative samples, thereby reducing annotation effort. However, in the domain of 3D biomedical imaging, there is no consensus on whether AL consistently outperforms Random sampling. Four evaluation pitfalls hinder the current methodological assessment. These are (1) restriction to too few datasets and annotation budgets, (2) using 2D models on 3D images without partial annotations, (3) Random baseline not being adapted to the task, and (4) measuring annotation cost only in voxels. In this work, we introduce nnActive, an open-source AL framework that overcomes these pitfalls by (1) means of a large scale study spanning four biomedical imaging datasets and three label regimes, (2) extending nnU-Net by using partial annotations for training with 3D patch-based query selection, (3) proposing Foreground Aware Random sampling strategies tackling the foreground-background class imbalance of medical images and (4) propose the foreground efficiency metric, which captures the low annotation cost of background-regions. We reveal the following findings: (A) while all AL methods outperform standard Random sampling, none reliably surpasses an improved Foreground Aware Random sampling; (B) benefits of AL depend on task specific parameters; (C) Predictive Entropy is overall the best performing AL method, but likely requires the most annotation effort; (D) AL performance can be improved with more compute intensive design choices. As a holistic, open-source framework, nnActive can serve as a catalyst for research and application of AL in 3D biomedical imaging. Code is at: https://github.com/MIC-DKFZ/nnActive",
    "authors": [
      "Carsten T. Lüth",
      "Jeremias Traub",
      "Kim-Celine Kahl",
      "Till J. Bungert",
      "Lukas Klein",
      "Lars Krämer",
      "Paul F. Jaeger",
      "Fabian Isensee",
      "Klaus Maier-Hein"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19183v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19183v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19152v1",
    "title": "Masked Diffusion Models are Secretly Learned-Order Autoregressive Models",
    "summary": "Masked Diffusion Models (MDMs) have emerged as one of the most promising paradigms for generative modeling over discrete domains. It is known that MDMs effectively train to decode tokens in a random order, and that this ordering has significant performance implications in practice. This observation raises a fundamental question: can we design a training framework that optimizes for a favorable decoding order? We answer this in the affirmative, showing that the continuous-time variational objective of MDMs, when equipped with multivariate noise schedules, can identify and optimize for a decoding order during training. We establish a direct correspondence between decoding order and the multivariate noise schedule and show that this setting breaks invariance of the MDM objective to the noise schedule. Furthermore, we prove that the MDM objective decomposes precisely into a weighted auto-regressive losses over these orders, which establishes them as auto-regressive models with learnable orders.",
    "authors": [
      "Prateek Garg",
      "Bhavya Kohli",
      "Sunita Sarawagi"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19152v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19152v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19118v1",
    "title": "A symbolic Perl algorithm for the unification of Nahuatl word spellings",
    "summary": "In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences",
    "authors": [
      "Juan-José Guzmán-Landa",
      "Jesús Vázquez-Osorio",
      "Juan-Manuel Torres-Moreno",
      "Ligia Quintana Torres",
      "Miguel Figueroa-Saavedra",
      "Martha-Lorena Avendaño-Garrido",
      "Graham Ranger",
      "Patricia Velázquez-Morales",
      "Gerardo Eugenio Sierra Martínez"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19118v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19118v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19111v1",
    "title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection",
    "summary": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k",
    "authors": [
      "Hai Ci",
      "Ziheng Peng",
      "Pei Yang",
      "Yingxin Xuan",
      "Mike Zheng Shou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19111v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19111v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19033v1",
    "title": "ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay",
    "summary": "Embodied exploration is a target-driven process that requires embodied agents to possess fine-grained perception and knowledge-enhanced decision making. While recent attempts leverage MLLMs for exploration due to their strong perceptual and reasoning abilities, we find that MLLM-based embodied agents remain suboptimal in exploring new environments: (i) they rely on profound but stale pre-trained knowledge, (ii) training-based approaches such as imitation learning or reinforcement learning are expensive for long-horizon tasks with sparse outcome rewards, and (iii) frontier-based exploration yields a large, visually nuanced action space that is difficult for MLLMs to make reliable decisions. We address these challenges with ReEXplore, a training-free framework that performs retrospective experience replay to inject distilled, abstract experience at inference time, and hierarchical frontier selection to decompose frontier ranking into coarse-to-fine decisions. Our approach enables robust, traceable, and efficient exploration. Across multiple embodied exploration benchmarks, ReEXplore yields great improvements over strong MLLM baselines, up to 3x higher performance in both success rate and in navigation efficiency under open-source backbones.",
    "authors": [
      "Gengyuan Zhang",
      "Mingcong Ding",
      "Jingpei Wu",
      "Ruotong Liao",
      "Volker Tresp"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19033v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19033v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.18976v1",
    "title": "Peregrine: One-Shot Fine-Tuning for FHE Inference of General Deep CNNs",
    "summary": "We address two fundamental challenges in adapting general deep CNNs for FHE-based inference: approximating non-linear activations such as ReLU with low-degree polynomials while minimizing accuracy degradation, and overcoming the ciphertext capacity barrier that constrains high-resolution image processing on FHE inference. Our contributions are twofold: (1) a single-stage fine-tuning (SFT) strategy that directly converts pre-trained CNNs into FHE-friendly forms using low-degree polynomials, achieving competitive accuracy with minimal training overhead; and (2) a generalized interleaved packing (GIP) scheme that is compatible with feature maps of virtually arbitrary spatial resolutions, accompanied by a suite of carefully designed homomorphic operators that preserve the GIP-form encryption throughout computation. These advances enable efficient, end-to-end FHE inference across diverse CNN architectures. Experiments on CIFAR-10, ImageNet, and MS COCO demonstrate that the FHE-friendly CNNs obtained via our SFT strategy achieve accuracy comparable to baselines using ReLU or SiLU activations. Moreover, this work presents the first demonstration of FHE-based inference for YOLO architectures in object detection leveraging low-degree polynomial activations.",
    "authors": [
      "Huaming Ling",
      "Ying Wang",
      "Si Chen",
      "Junfeng Fan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18976v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18976v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.18966v1",
    "title": "LLM-CSEC: Empirical Evaluation of Security in C/C++ Code Generated by Large Language Models",
    "summary": "The security of code generated by large language models (LLMs) is a significant concern, as studies indicate that such code often contains vulnerabilities and lacks essential defensive programming constructs. This work focuses on examining and evaluating the security of LLM-generated code, particularly in the context of C/C++. We categorized known vulnerabilities using the Common Weakness Enumeration (CWE) and, to study their criticality, mapped them to CVEs. We used ten different LLMs for code generation and analyzed the outputs through static analysis. The amount of CWEs present in AI-generated code is concerning. Our findings highlight the need for developers to be cautious when using LLM-generated code. This study provides valuable insights to advance automated code generation and encourage further research in this domain.",
    "authors": [
      "Muhammad Usman Shahid",
      "Chuadhry Mujeeb Ahmed",
      "Rajiv Ranjan"
    ],
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18966v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18966v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.18957v1",
    "title": "Eevee: Towards Close-up High-resolution Video-based Virtual Try-on",
    "summary": "Video virtual try-on technology provides a cost-effective solution for creating marketing videos in fashion e-commerce. However, its practical adoption is hindered by two critical limitations. First, the reliance on a single garment image as input in current virtual try-on datasets limits the accurate capture of realistic texture details. Second, most existing methods focus solely on generating full-shot virtual try-on videos, neglecting the business's demand for videos that also provide detailed close-ups. To address these challenges, we introduce a high-resolution dataset for video-based virtual try-on. This dataset offers two key features. First, it provides more detailed information on the garments, which includes high-fidelity images with detailed close-ups and textual descriptions; Second, it uniquely includes full-shot and close-up try-on videos of real human models. Furthermore, accurately assessing consistency becomes significantly more critical for the close-up videos, which demand high-fidelity preservation of garment details. To facilitate such fine-grained evaluation, we propose a new garment consistency metric VGID (Video Garment Inception Distance) that quantifies the preservation of both texture and structure. Our experiments validate these contributions. We demonstrate that by utilizing the detailed images from our dataset, existing video generation models can extract and incorporate texture features, significantly enhancing the realism and detail fidelity of virtual try-on results. Furthermore, we conduct a comprehensive benchmark of recent models. The benchmark effectively identifies the texture and structural preservation problems among current methods.",
    "authors": [
      "Jianhao Zeng",
      "Yancheng Bai",
      "Ruidong Chen",
      "Xuanpu Zhang",
      "Lei Sun",
      "Dongyang Jin",
      "Ryan Xu",
      "Nannan Zhang",
      "Dan Song",
      "Xiangxiang Chu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18957v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18957v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.18945v1",
    "title": "MIST: Mutual Information Via Supervised Training",
    "summary": "We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.",
    "authors": [
      "German Gritsai",
      "Megan Richards",
      "Maxime Méloux",
      "Kyunghyun Cho",
      "Maxime Peyrard"
    ],
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18945v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18945v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.18925v1",
    "title": "AttenDence: Maximizing Attention Confidence for Test Time Adaptation",
    "summary": "Test-time adaptation (TTA) enables models to adapt to distribution shifts at inference time. While entropy minimization over the output distribution has proven effective for TTA, transformers offer an additional unsupervised learning signal through their attention mechanisms. We propose minimizing the entropy of attention distributions from the CLS token to image patches as a novel TTA objective.This approach encourages the model to attend more confidently to relevant image regions under distribution shift and is effective even when only a single test image is available. We demonstrate that attention entropy minimization improves robustness across diverse corruption types while not hurting performance on clean data on a single sample stream of images at test time.",
    "authors": [
      "Yash Mali"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18925v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18925v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.19431v1",
    "title": "Cloud4D",
    "summary": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four-dimensional cloud state using only synchronized ground-based cameras. Leveraging a homography-guided 2D-to-3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\\%$) against collocated radar measurements. Code and data are available on our project page https://cloud4d.jacob-lin.com/.",
    "authors": [
      "Jacob Lin",
      "Edward Gryspeerdt",
      "Ronald Clark"
    ],
    "categories": [
      "cs.CV",
      "physics.ao-ph"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19431v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19431v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.19351v1",
    "title": "CellFMCount: A Fluorescence Microscopy Dataset, Benchmark, and Methods for Cell Counting",
    "summary": "Accurate cell counting is essential in various biomedical research and clinical applications, including cancer diagnosis, stem cell research, and immunology. Manual counting is labor-intensive and error-prone, motivating automation through deep learning techniques. However, training reliable deep learning models requires large amounts of high-quality annotated data, which is difficult and time-consuming to produce manually. Consequently, existing cell-counting datasets are often limited, frequently containing fewer than $500$ images. In this work, we introduce a large-scale annotated dataset comprising $3{,}023$ images from immunocytochemistry experiments related to cellular differentiation, containing over $430{,}000$ manually annotated cell locations. The dataset presents significant challenges: high cell density, overlapping and morphologically diverse cells, a long-tailed distribution of cell count per image, and variation in staining protocols. We benchmark three categories of existing methods: regression-based, crowd-counting, and cell-counting techniques on a test set with cell counts ranging from $10$ to $2{,}126$ cells per image. We also evaluate how the Segment Anything Model (SAM) can be adapted for microscopy cell counting using only dot-annotated datasets. As a case study, we implement a density-map-based adaptation of SAM (SAM-Counter) and report a mean absolute error (MAE) of $22.12$, which outperforms existing approaches (second-best MAE of $27.46$). Our results underscore the value of the dataset and the benchmarking framework for driving progress in automated cell counting and provide a robust foundation for future research and development.",
    "authors": [
      "Abdurahman Ali Mohammed",
      "Catherine Fonder",
      "Ying Wei",
      "Wallapak Tavanapong",
      "Donald S Sakaguchi",
      "Qi Li",
      "Surya K. Mallapragada"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19351v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19351v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.19121v1",
    "title": "ReLU-Based and DNN-Based Generalized Maximum Score Estimators",
    "summary": "We propose a new formulation of the maximum score estimator that uses compositions of rectified linear unit (ReLU) functions, instead of indicator functions as in Manski (1975,1985), to encode the sign alignment restrictions. Since the ReLU function is Lipschitz, our new ReLU-based maximum score criterion function is substantially easier to optimize using standard gradient-based optimization pacakges. We also show that our ReLU-based maximum score (RMS) estimator can be generalized to an umbrella framework defined by multi-index single-crossing (MISC) conditions, while the original maximum score estimator cannot be applied. We establish the $n^{-s/(2s+1)}$ convergence rate and asymptotic normality for the RMS estimator under order-$s$ Holder smoothness. In addition, we propose an alternative estimator using a further reformulation of RMS as a special layer in a deep neural network (DNN) architecture, which allows the estimation procedure to be implemented via state-of-the-art software and hardware for DNN.",
    "authors": [
      "Xiaohong Chen",
      "Wayne Yuan Gao",
      "Likang Wen"
    ],
    "categories": [
      "econ.EM",
      "stat.ML"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19121v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19121v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.19120v1",
    "title": "On the Optimality of Discrete Object Naming: a Kinship Case Study",
    "summary": "The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.",
    "authors": [
      "Phong Le",
      "Mees Lindeman",
      "Raquel G. Alhama"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19120v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19120v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.19071v1",
    "title": "DEAP-3DSAM: Decoder Enhanced and Auto Prompt SAM for 3D Medical Image Segmentation",
    "summary": "The Segment Anything Model (SAM) has recently demonstrated significant potential in medical image segmentation. Although SAM is primarily trained on 2D images, attempts have been made to apply it to 3D medical image segmentation. However, the pseudo 3D processing used to adapt SAM results in spatial feature loss, limiting its performance. Additionally, most SAM-based methods still rely on manual prompts, which are challenging to implement in real-world scenarios and require extensive external expert knowledge. To address these limitations, we introduce the Decoder Enhanced and Auto Prompt SAM (DEAP-3DSAM) to tackle these limitations. Specifically, we propose a Feature Enhanced Decoder that fuses the original image features with rich and detailed spatial information to enhance spatial features. We also design a Dual Attention Prompter to automatically obtain prompt information through Spatial Attention and Channel Attention. We conduct comprehensive experiments on four public abdominal tumor segmentation datasets. The results indicate that our DEAP-3DSAM achieves state-of-the-art performance in 3D image segmentation, outperforming or matching existing manual prompt methods. Furthermore, both quantitative and qualitative ablation studies confirm the effectiveness of our proposed modules.",
    "authors": [
      "Fangda Chen",
      "Jintao Tang",
      "Pancheng Wang",
      "Ting Wang",
      "Shasha Li",
      "Ting Deng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19071v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19071v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.19037v1",
    "title": "Resolving Node Identifiability in Graph Neural Processes via Laplacian Spectral Encodings",
    "summary": "Message passing graph neural networks are widely used for learning on graphs, yet their expressive power is limited by the one-dimensional Weisfeiler-Lehman test and can fail to distinguish structurally different nodes. We provide rigorous theory for a Laplacian positional encoding that is invariant to eigenvector sign flips and to basis rotations within eigenspaces. We prove that this encoding yields node identifiability from a constant number of observations and establishes a sample-complexity separation from architectures constrained by the Weisfeiler-Lehman test. The analysis combines a monotone link between shortest-path and diffusion distance, spectral trilateration with a constant set of anchors, and quantitative spectral injectivity with logarithmic embedding size. As an instantiation, pairing this encoding with a neural-process style decoder yields significant gains on a drug-drug interaction task on chemical graphs, improving both the area under the ROC curve and the F1 score and demonstrating the practical benefits of resolving theoretical expressiveness limitations with principled positional information.",
    "authors": [
      "Zimo Yan",
      "Zheng Xie",
      "Chang Liu",
      "Yuan Wang"
    ],
    "categories": [
      "cs.LG",
      "math.PR"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19037v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19037v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.18999v1",
    "title": "Enhancing low energy reconstruction and classification in KM3NeT/ORCA with transformers",
    "summary": "The current KM3NeT/ORCA neutrino telescope, still under construction, has not yet reached its full potential in neutrino reconstruction capability. When training any deep learning model, no explicit information about the physics or the detector is provided, thus they remain unknown to the model. This study leverages the strengths of transformers by incorporating attention masks inspired by the physics and detector design, making the model understand both the telescope design and the neutrino physics measured on it. The study also shows the efficacy of transformers on retaining valuable information between detectors when doing fine-tuning from one configurations to another.",
    "authors": [
      "Iván Mozún Mateo"
    ],
    "categories": [
      "hep-ex",
      "astro-ph.IM",
      "cs.AI"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18999v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18999v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.18942v1",
    "title": "VeCoR - Velocity Contrastive Regularization for Flow Matching",
    "summary": "Flow Matching (FM) has recently emerged as a principled and efficient alternative to diffusion models. Standard FM encourages the learned velocity field to follow a target direction; however, it may accumulate errors along the trajectory and drive samples off the data manifold, leading to perceptual degradation, especially in lightweight or low-step configurations.   To enhance stability and generalization, we extend FM into a balanced attract-repel scheme that provides explicit guidance on both \"where to go\" and \"where not to go.\" To be formal, we propose \\textbf{Velocity Contrastive Regularization (VeCoR)}, a complementary training scheme for flow-based generative modeling that augments the standard FM objective with contrastive, two-sided supervision. VeCoR not only aligns the predicted velocity with a stable reference direction (positive supervision) but also pushes it away from inconsistent, off-manifold directions (negative supervision). This contrastive formulation transforms FM from a purely attractive, one-sided objective into a two-sided training signal, regularizing trajectory evolution and improving perceptual fidelity across datasets and backbones.   On ImageNet-1K 256$\\times$256, VeCoR yields 22\\% and 35\\% relative FID reductions on SiT-XL/2 and REPA-SiT-XL/2 backbones, respectively, and achieves further FID gains (32\\% relative) on MS-COCO text-to-image generation, demonstrating consistent improvements in stability, convergence, and image quality, particularly in low-step and lightweight settings. Project page: https://p458732.github.io/VeCoR_Project_Page/",
    "authors": [
      "Zong-Wei Hong",
      "Jing-lun Li",
      "Lin-Ze Li",
      "Shen Zhang",
      "Yao Tang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18942v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18942v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.19356v1",
    "title": "Growing with the Generator: Self-paced GRPO for Video Generation",
    "summary": "Group Relative Policy Optimization (GRPO) has emerged as a powerful reinforcement learning paradigm for post-training video generation models. However, existing GRPO pipelines rely on static, fixed-capacity reward models whose evaluation behavior is frozen during training. Such rigid rewards introduce distributional bias, saturate quickly as the generator improves, and ultimately limit the stability and effectiveness of reinforcement-based alignment. We propose Self-Paced GRPO, a competence-aware GRPO framework in which reward feedback co-evolves with the generator. Our method introduces a progressive reward mechanism that automatically shifts its emphasis from coarse visual fidelity to temporal coherence and fine-grained text-video semantic alignment as generation quality increases. This self-paced curriculum alleviates reward-policy mismatch, mitigates reward exploitation, and yields more stable optimization. Experiments on VBench across multiple video generation backbones demonstrate consistent improvements in both visual quality and semantic alignment over GRPO baselines with static rewards, validating the effectiveness and generality of Self-Paced GRPO.",
    "authors": [
      "Rui Li",
      "Yuanzhi Liang",
      "Ziqi Ni",
      "Haibing Huang",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19356v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19356v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.19105v1",
    "title": "Graph-based 3D Human Pose Estimation using WiFi Signals",
    "summary": "WiFi-based human pose estimation (HPE) has attracted increasing attention due to its resilience to occlusion and privacy-preserving compared to camera-based methods. However, existing WiFi-based HPE approaches often employ regression networks that directly map WiFi channel state information (CSI) to 3D joint coordinates, ignoring the inherent topological relationships among human joints. In this paper, we present GraphPose-Fi, a graph-based framework that explicitly models skeletal topology for WiFi-based 3D HPE. Our framework comprises a CNN encoder shared across antennas for subcarrier-time feature extraction, a lightweight attention module that adaptively reweights features over time and across antennas, and a graph-based regression head that combines GCN layers with self-attention to capture local topology and global dependencies. Our proposed method significantly outperforms existing methods on the MM-Fi dataset in various settings. The source code is available at: https://github.com/Cirrick/GraphPose-Fi.",
    "authors": [
      "Jichao Chen",
      "YangYang Qu",
      "Ruibo Tang",
      "Dirk Slock"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19105v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19105v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.18993v1",
    "title": "AuViRe: Audio-visual Speech Representation Reconstruction for Deepfake Temporal Localization",
    "summary": "With the rapid advancement of sophisticated synthetic audio-visual content, e.g., for subtle malicious manipulations, ensuring the integrity of digital media has become paramount. This work presents a novel approach to temporal localization of deepfakes by leveraging Audio-Visual Speech Representation Reconstruction (AuViRe). Specifically, our approach reconstructs speech representations from one modality (e.g., lip movements) based on the other (e.g., audio waveform). Cross-modal reconstruction is significantly more challenging in manipulated video segments, leading to amplified discrepancies, thereby providing robust discriminative cues for precise temporal forgery localization. AuViRe outperforms the state of the art by +8.9 AP@0.95 on LAV-DF, +9.6 AP@0.5 on AV-Deepfake1M, and +5.1 AUC on an in-the-wild experiment. Code available at https://github.com/mever-team/auvire.",
    "authors": [
      "Christos Koutlis",
      "Symeon Papadopoulos"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18993v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18993v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.19291v1",
    "title": "TorchQuantumDistributed",
    "summary": "TorchQuantumDistributed (tqd) is a PyTorch-based [Paszke et al., 2019] library for accelerator-agnostic differentiable quantum state vector simulation at scale. This enables studying the behavior of learnable parameterized near-term and fault- tolerant quantum circuits with high qubit counts.",
    "authors": [
      "Oliver Knitter",
      "Jonathan Mei",
      "Masako Yamada",
      "Martin Roetteler"
    ],
    "categories": [
      "quant-ph",
      "cs.CE",
      "cs.LG"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19291v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19291v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.19261v1",
    "title": "LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models",
    "summary": "Humans can perceive and understand 3D space and long videos from sequential visual observations. But do vision-language models (VLMs) can? Recent work demonstrates that even state-of-the-art VLMs still struggle to understand 3D space and long videos, although they are powerful in typical vision-language tasks. Current methods often rely on specialized architectural designs to improve performance for 3D tasks and video understanding tasks separately. In contrast, we propose LAST, short for LeArn to Think in Space and Time, to jointly improve 3D spatial and long video understanding for general VLMs with only a set of 2D images as inputs. LAST makes VLMs think in space and time rather than only with text before giving the final answer, building visual thinking trajectories in 3D space and temporal dimension. We demonstrate the effectiveness of LAST in two scenarios: 1) zero-shot, where we directly prompt proprietary models; and 2) fine-tuning general VLMs with data that include thinking trajectories in 3D space and time. We show that LAST brings substantial gains in various benchmarks, including 3 spatial understanding, 4 video understanding, and 3 image understanding tasks. Notably, 15.8% gains on EgoSchema with GPT-4o in a zero-shot manner and 8.3 gains on VSI-Bench compared with Qwen2.5-VL-7B.",
    "authors": [
      "Shuai Wang",
      "Daoan Zhang",
      "Tianyi Bai",
      "Shitong Shao",
      "Jiebo Luo",
      "Jiaheng Wei"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19261v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19261v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.19235v1",
    "title": "IDSplat: Instance-Decomposed 3D Gaussian Splatting for Driving Scenes",
    "summary": "Reconstructing dynamic driving scenes is essential for developing autonomous systems through sensor-realistic simulation. Although recent methods achieve high-fidelity reconstructions, they either rely on costly human annotations for object trajectories or use time-varying representations without explicit object-level decomposition, leading to intertwined static and dynamic elements that hinder scene separation. We present IDSplat, a self-supervised 3D Gaussian Splatting framework that reconstructs dynamic scenes with explicit instance decomposition and learnable motion trajectories, without requiring human annotations. Our key insight is to model dynamic objects as coherent instances undergoing rigid transformations, rather than unstructured time-varying primitives. For instance decomposition, we employ zero-shot, language-grounded video tracking anchored to 3D using lidar, and estimate consistent poses via feature correspondences. We introduce a coordinated-turn smoothing scheme to obtain temporally and physically consistent motion trajectories, mitigating pose misalignments and tracking failures, followed by joint optimization of object poses and Gaussian parameters. Experiments on the Waymo Open Dataset demonstrate that our method achieves competitive reconstruction quality while maintaining instance-level decomposition and generalizes across diverse sequences and view densities without retraining, making it practical for large-scale autonomous driving applications. Code will be released.",
    "authors": [
      "Carl Lindström",
      "Mahan Rafidashti",
      "Maryam Fatemi",
      "Lars Hammarstrand",
      "Martin R. Oswald",
      "Lennart Svensson"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19235v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19235v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.19117v1",
    "title": "3M-TI: High-Quality Mobile Thermal Imaging via Calibration-free Multi-Camera Cross-Modal Diffusion",
    "summary": "The miniaturization of thermal sensors for mobile platforms inherently limits their spatial resolution and textural fidelity, leading to blurry and less informative images. Existing thermal super-resolution (SR) methods can be grouped into single-image and RGB-guided approaches: the former struggles to recover fine structures from limited information, while the latter relies on accurate and laborious cross-camera calibration, which hinders practical deployment and robustness. Here, we propose 3M-TI, a calibration-free Multi-camera cross-Modality diffusion framework for Mobile Thermal Imaging. At its core, 3M-TI integrates a cross-modal self-attention module (CSM) into the diffusion UNet, replacing the original self-attention layers to adaptively align thermal and RGB features throughout the denoising process, without requiring explicit camera calibration. This design enables the diffusion network to leverage its generative prior to enhance spatial resolution, structural fidelity, and texture detail in the super-resolved thermal images. Extensive evaluations on real-world mobile thermal cameras and public benchmarks validate our superior performance, achieving state-of-the-art results in both visual quality and quantitative metrics. More importantly, the thermal images enhanced by 3M-TI lead to substantial gains in critical downstream tasks like object detection and segmentation, underscoring its practical value for robust mobile thermal perception systems. More materials: https://github.com/work-submit/3MTI.",
    "authors": [
      "Minchong Chen",
      "Xiaoyun Yuan",
      "Junzhe Wan",
      "Jianing Zhang",
      "Jun Zhang"
    ],
    "categories": [
      "cs.CV",
      "physics.optics"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19117v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19117v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.19115v1",
    "title": "AI Consciousness and Existential Risk",
    "summary": "In AI, the existential risk denotes the hypothetical threat posed by an artificial system that would possess both the capability and the objective, either directly or indirectly, to eradicate humanity. This issue is gaining prominence in scientific debate due to recent technical advancements and increased media coverage. In parallel, AI progress has sparked speculation and studies about the potential emergence of artificial consciousness. The two questions, AI consciousness and existential risk, are sometimes conflated, as if the former entailed the latter. Here, I explain that this view stems from a common confusion between consciousness and intelligence. Yet these two properties are empirically and theoretically distinct. Arguably, while intelligence is a direct predictor of an AI system's existential threat, consciousness is not. There are, however, certain incidental scenarios in which consciousness could influence existential risk, in either direction. Consciousness could be viewed as a means towards AI alignment, thereby lowering existential risk; or, it could be a precondition for reaching certain capabilities or levels of intelligence, and thus positively related to existential risk. Recognizing these distinctions can help AI safety researchers and public policymakers focus on the most pressing issues.",
    "authors": [
      "Rufin VanRullen"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19115v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19115v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.18978v1",
    "title": "Zero-shot segmentation of skin tumors in whole-slide images with vision-language foundation models",
    "summary": "Accurate annotation of cutaneous neoplasm biopsies represents a major challenge due to their wide morphological variability, overlapping histological patterns, and the subtle distinctions between benign and malignant lesions. Vision-language foundation models (VLMs), pre-trained on paired image-text corpora, learn joint representations that bridge visual features and diagnostic terminology, enabling zero-shot localization and classification of tissue regions without pixel-level labels. However, most existing VLM applications in histopathology remain limited to slide-level tasks or rely on coarse interactive prompts, and they struggle to produce fine-grained segmentations across gigapixel whole-slide images (WSIs). In this work, we introduce a zero-shot visual-language segmentation pipeline for whole-slide images (ZEUS), a fully automated, zero-shot segmentation framework that leverages class-specific textual prompt ensembles and frozen VLM encoders to generate high-resolution tumor masks in WSIs. By partitioning each WSI into overlapping patches, extracting visual embeddings, and computing cosine similarities against text prompts, we generate a final segmentation mask. We demonstrate competitive performance on two in-house datasets, primary spindle cell neoplasms and cutaneous metastases, highlighting the influence of prompt design, domain shifts, and institutional variability in VLMs for histopathology. ZEUS markedly reduces annotation burden while offering scalable, explainable tumor delineation for downstream diagnostic workflows.",
    "authors": [
      "Santiago Moreno",
      "Pablo Meseguer",
      "Rocío del Amor",
      "Valery Naranjo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18978v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18978v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.18921v1",
    "title": "BackdoorVLM: A Benchmark for Backdoor Attacks on Vision-Language Models",
    "summary": "Backdoor attacks undermine the reliability and trustworthiness of machine learning systems by injecting hidden behaviors that can be maliciously activated at inference time. While such threats have been extensively studied in unimodal settings, their impact on multimodal foundation models, particularly vision-language models (VLMs), remains largely underexplored. In this work, we introduce \\textbf{BackdoorVLM}, the first comprehensive benchmark for systematically evaluating backdoor attacks on VLMs across a broad range of settings. It adopts a unified perspective that injects and analyzes backdoors across core vision-language tasks, including image captioning and visual question answering. BackdoorVLM organizes multimodal backdoor threats into 5 representative categories: targeted refusal, malicious injection, jailbreak, concept substitution, and perceptual hijack. Each category captures a distinct pathway through which an adversary can manipulate a model's behavior. We evaluate these threats using 12 representative attack methods spanning text, image, and bimodal triggers, tested on 2 open-source VLMs and 3 multimodal datasets. Our analysis reveals that VLMs exhibit strong sensitivity to textual instructions, and in bimodal backdoors the text trigger typically overwhelms the image trigger when forming the backdoor mapping. Notably, backdoors involving the textual modality remain highly potent, with poisoning rates as low as 1\\% yielding over 90\\% success across most tasks. These findings highlight significant, previously underexplored vulnerabilities in current VLMs. We hope that BackdoorVLM can serve as a useful benchmark for analyzing and mitigating multimodal backdoor threats. Code is available at: https://github.com/bin015/BackdoorVLM .",
    "authors": [
      "Juncheng Li",
      "Yige Li",
      "Hanxun Huang",
      "Yunhao Chen",
      "Xin Wang",
      "Yixu Wang",
      "Xingjun Ma",
      "Yu-Gang Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18921v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18921v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.19169v1",
    "title": "Test-Time Preference Optimization for Image Restoration",
    "summary": "Image restoration (IR) models are typically trained to recover high-quality images using L1 or LPIPS loss. To handle diverse unknown degradations, zero-shot IR methods have also been introduced. However, existing pre-trained and zero-shot IR approaches often fail to align with human preferences, resulting in restored images that may not be favored. This highlights the critical need to enhance restoration quality and adapt flexibly to various image restoration tasks or backbones without requiring model retraining and ideally without labor-intensive preference data collection. In this paper, we propose the first Test-Time Preference Optimization (TTPO) paradigm for image restoration, which enhances perceptual quality, generates preference data on-the-fly, and is compatible with any IR model backbone. Specifically, we design a training-free, three-stage pipeline: (i) generate candidate preference images online using diffusion inversion and denoising based on the initially restored image; (ii) select preferred and dispreferred images using automated preference-aligned metrics or human feedback; and (iii) use the selected preference images as reward signals to guide the diffusion denoising process, optimizing the restored image to better align with human preferences. Extensive experiments across various image restoration tasks and models demonstrate the effectiveness and flexibility of the proposed pipeline.",
    "authors": [
      "Bingchen Li",
      "Xin Li",
      "Jiaqi Xu",
      "Jiaming Guo",
      "Wenbo Li",
      "Renjing Pei",
      "Zhibo Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19169v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19169v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.19021v1",
    "title": "Dynamic Granularity Matters: Rethinking Vision Transformers Beyond Fixed Patch Splitting",
    "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in capturing global dependencies but often struggle to efficiently represent fine-grained local details. Existing multi-scale approaches alleviate this issue by integrating hierarchical or hybrid features; however, they rely on fixed patch sizes and introduce redundant computation. To address these limitations, we propose Granularity-driven Vision Transformer (Grc-ViT), a dynamic coarse-to-fine framework that adaptively adjusts visual granularity based on image complexity. It comprises two key stages: (1) Coarse Granularity Evaluation module, which assesses visual complexity using edge density, entropy, and frequency-domain cues to estimate suitable patch and window sizes; (2) Fine-grained Refinement module, which refines attention computation according to the selected granularity, enabling efficient and precise feature learning. Two learnable parameters, α and \\b{eta}, are optimized end-to-end to balance global reasoning and local perception. Comprehensive evaluations demonstrate that Grc-ViT enhances fine-grained discrimination while achieving a superior trade-off between accuracy and computational efficiency.",
    "authors": [
      "Qiyang Yu",
      "Yu Fang",
      "Tianrui Li",
      "Xuemei Cao",
      "Yan Chen",
      "Jianghao Li",
      "Fan Min"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19021v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19021v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.18991v1",
    "title": "View-Consistent Diffusion Representations for 3D-Consistent Video Generation",
    "summary": "Video generation models have made significant progress in generating realistic content, enabling applications in simulation, gaming, and film making. However, current generated videos still contain visual artifacts arising from 3D inconsistencies, e.g., objects and structures deforming under changes in camera pose, which can undermine user experience and simulation fidelity. Motivated by recent findings on representation alignment for diffusion models, we hypothesize that improving the multi-view consistency of video diffusion representations will yield more 3D-consistent video generation. Through detailed analysis on multiple recent camera-controlled video diffusion models we reveal strong correlations between 3D-consistent representations and videos. We also propose ViCoDR, a new approach for improving the 3D consistency of video models by learning multi-view consistent diffusion representations. We evaluate ViCoDR on camera controlled image-to-video, text-to-video, and multi-view generation models, demonstrating significant improvements in the 3D consistency of the generated videos. Project page: https://danier97.github.io/ViCoDR.",
    "authors": [
      "Duolikun Danier",
      "Ge Gao",
      "Steven McDonagh",
      "Changjian Li",
      "Hakan Bilen",
      "Oisin Mac Aodha"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18991v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18991v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.18922v1",
    "title": "One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control",
    "summary": "We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D",
    "authors": [
      "Zhenxing Mi",
      "Yuxin Wang",
      "Dan Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18922v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18922v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.19180v1",
    "title": "Evaluating Deep Learning and Traditional Approaches Used in Source Camera Identification",
    "summary": "One of the most important tasks in computer vision is identifying the device using which the image was taken, useful for facilitating further comprehensive analysis of the image. This paper presents comparative analysis of three techniques used in source camera identification (SCI): Photo Response Non-Uniformity (PRNU), JPEG compression artifact analysis, and convolutional neural networks (CNNs). It evaluates each method in terms of device classification accuracy. Furthermore, the research discusses the possible scientific development needed for the implementation of the methods in real-life scenarios.",
    "authors": [
      "Mansur Ozaman"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19180v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19180v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2511.19032v1",
    "title": "Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric",
    "summary": "Despite the remarkable reasoning abilities of large vision-language models (LVLMs), their robustness under visual corruptions remains insufficiently studied. Existing evaluation paradigms exhibit two major limitations: 1) the dominance of low-discriminative samples in current datasets masks the real robustness gap between models; and 2) conventional accuracy-based metric fail to capture the degradation of the underlying prediction structure. To bridge these gaps, we introduce Bench-C, a comprehensive benchmark emphasizing discriminative samples for assessing corruption robustness, where a selection strategy is proposed to jointly consider the prediction inconsistency under corruption and the semantic diversity. Furthermore, we propose the Robustness Alignment Score (RAS), a unified metric that measures degradation in logit-level prediction structure by considering the shifts in prediction uncertainty and calibration alignment. Comprehensive experiments and analysis reveal several interesting findings: 1) model behaviors exhibit distinguish patterns under corruptions, such as erroneous confidence and hesitation; 2) despite subtle corruption may lead to a slight accuracy gain, the overall prediction structure still degrades; 3) by decomposing corruption robustness into destructive and corrective components, the distinct failure and recovery patterns across models can be revealed.",
    "authors": [
      "Xiangjie Sui",
      "Songyang Li",
      "Hanwei Zhu",
      "Baoliang Chen",
      "Yuming Fang",
      "Xin Sun"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19032v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19032v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2511.19435v1",
    "title": "Are Image-to-Video Models Good Zero-Shot Image Editors?",
    "summary": "Large-scale video diffusion models show strong world simulation and temporal reasoning abilities, but their use as zero-shot image editors remains underexplored. We introduce IF-Edit, a tuning-free framework that repurposes pretrained image-to-video diffusion models for instruction-driven image editing. IF-Edit addresses three key challenges: prompt misalignment, redundant temporal latents, and blurry late-stage frames. It includes (1) a chain-of-thought prompt enhancement module that transforms static editing instructions into temporally grounded reasoning prompts; (2) a temporal latent dropout strategy that compresses frame latents after the expert-switch point, accelerating denoising while preserving semantic and temporal coherence; and (3) a self-consistent post-refinement step that sharpens late-stage frames using a short still-video trajectory. Experiments on four public benchmarks, covering non-rigid editing, physical and temporal reasoning, and general instruction edits, show that IF-Edit performs strongly on reasoning-centric tasks while remaining competitive on general-purpose edits. Our study provides a systematic view of video diffusion models as image editors and highlights a simple recipe for unified video-image generative reasoning.",
    "authors": [
      "Zechuan Zhang",
      "Zhenyuan Chen",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19435v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19435v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.19248v1",
    "title": "FedPoisonTTP: A Threat Model and Poisoning Attack for Federated Test-Time Personalization",
    "summary": "Test-time personalization in federated learning enables models at clients to adjust online to local domain shifts, enhancing robustness and personalization in deployment. Yet, existing federated learning work largely overlooks the security risks that arise when local adaptation occurs at test time. Heterogeneous domain arrivals, diverse adaptation algorithms, and limited cross-client visibility create vulnerabilities where compromised participants can craft poisoned inputs and submit adversarial updates that undermine both global and per-client performance. To address this threat, we introduce FedPoisonTTP, a realistic grey-box attack framework that explores test-time data poisoning in the federated adaptation setting. FedPoisonTTP distills a surrogate model from adversarial queries, synthesizes in-distribution poisons using feature-consistency, and optimizes attack objectives to generate high-entropy or class-confident poisons that evade common adaptation filters. These poisons are injected during local adaptation and spread through collaborative updates, leading to broad degradation. Extensive experiments on corrupted vision benchmarks show that compromised participants can substantially diminish overall test-time performance.",
    "authors": [
      "Md Akil Raihan Iftee",
      "Syed Md. Ahnaf Hasan",
      "Amin Ahsan Ali",
      "AKM Mahbubur Rahman",
      "Sajib Mistry",
      "Aneesh Krishna"
    ],
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19248v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19248v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.19137v1",
    "title": "FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation",
    "summary": "Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.",
    "authors": [
      "Zhifeng Xie",
      "Keyi Zhang",
      "Yiye Yan",
      "Yuling Guo",
      "Fan Yang",
      "Jiting Zhou",
      "Mengtian Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19137v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19137v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.19109v1",
    "title": "HABIT: Human Action Benchmark for Interactive Traffic in CARLA",
    "summary": "Current autonomous driving (AD) simulations are critically limited by their inadequate representation of realistic and diverse human behavior, which is essential for ensuring safety and reliability. Existing benchmarks often simplify pedestrian interactions, failing to capture complex, dynamic intentions and varied responses critical for robust system deployment. To overcome this, we introduce HABIT (Human Action Benchmark for Interactive Traffic), a high-fidelity simulation benchmark. HABIT integrates real-world human motion, sourced from mocap and videos, into CARLA (Car Learning to Act, a full autonomous driving simulator) via a modular, extensible, and physically consistent motion retargeting pipeline. From an initial pool of approximately 30,000 retargeted motions, we curate 4,730 traffic-compatible pedestrian motions, standardized in SMPL format for physically consistent trajectories. HABIT seamlessly integrates with CARLA's Leaderboard, enabling automated scenario generation and rigorous agent evaluation. Our safety metrics, including Abbreviated Injury Scale (AIS) and False Positive Braking Rate (FPBR), reveal critical failure modes in state-of-the-art AD agents missed by prior evaluations. Evaluating three state-of-the-art autonomous driving agents, InterFuser, TransFuser, and BEVDriver, demonstrates how HABIT exposes planner weaknesses that remain hidden in scripted simulations. Despite achieving close or equal to zero collisions per kilometer on the CARLA Leaderboard, the autonomous agents perform notably worse on HABIT, with up to 7.43 collisions/km and a 12.94% AIS 3+ injury risk, and they brake unnecessarily in up to 33% of cases. All components are publicly released to support reproducible, pedestrian-aware AI research.",
    "authors": [
      "Mohan Ramesh",
      "Mark Azer",
      "Fabian B. Flohr"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19109v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19109v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.19057v1",
    "title": "LAA3D: A Benchmark of Detecting and Tracking Low-Altitude Aircraft in 3D Space",
    "summary": "Perception of Low-Altitude Aircraft (LAA) in 3D space enables precise 3D object localization and behavior understanding. However, datasets tailored for 3D LAA perception remain scarce. To address this gap, we present LAA3D, a large-scale dataset designed to advance 3D detection and tracking of low-altitude aerial vehicles. LAA3D contains 15,000 real images and 600,000 synthetic frames, captured across diverse scenarios, including urban and suburban environments. It covers multiple aerial object categories, including electric Vertical Take-Off and Landing (eVTOL) aircraft, Micro Aerial Vehicles (MAVs), and Helicopters. Each instance is annotated with 3D bounding box, class label, and instance identity, supporting tasks such as 3D object detection, 3D multi-object tracking (MOT), and 6-DoF pose estimation. Besides, we establish the LAA3D Benchmark, integrating multiple tasks and methods with unified evaluation protocols for comparison. Furthermore, we propose MonoLAA, a monocular 3D detection baseline, achieving robust 3D localization from zoom cameras with varying focal lengths. Models pretrained on synthetic images transfer effectively to real-world data with fine-tuning, demonstrating strong sim-to-real generalization. Our LAA3D provides a comprehensive foundation for future research in low-altitude 3D object perception.",
    "authors": [
      "Hai Wu",
      "Shuai Tang",
      "Jiale Wang",
      "Longkun Zou",
      "Mingyue Guo",
      "Rongqin Liang",
      "Ke Chen",
      "Yaowei Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19057v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19057v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.19394v1",
    "title": "BackSplit: The Importance of Sub-dividing the Background in Biomedical Lesion Segmentation",
    "summary": "Segmenting small lesions in medical images remains notoriously difficult. Most prior work tackles this challenge by either designing better architectures, loss functions, or data augmentation schemes; and collecting more labeled data. We take a different view, arguing that part of the problem lies in how the background is modeled. Common lesion segmentation collapses all non-lesion pixels into a single \"background\" class, ignoring the rich anatomical context in which lesions appear. In reality, the background is highly heterogeneous-composed of tissues, organs, and other structures that can now be labeled manually or inferred automatically using existing segmentation models.   In this paper, we argue that training with fine-grained labels that sub-divide the background class, which we call BackSplit, is a simple yet powerful paradigm that can offer a significant performance boost without increasing inference costs. From an information theoretic standpoint, we prove that BackSplit increases the expected Fisher Information relative to conventional binary training, leading to tighter asymptotic bounds and more stable optimization. With extensive experiments across multiple datasets and architectures, we empirically show that BackSplit consistently boosts small-lesion segmentation performance, even when auxiliary labels are generated automatically using pretrained segmentation models. Additionally, we demonstrate that auxiliary labels derived from interactive segmentation frameworks exhibit the same beneficial effect, demonstrating its robustness, simplicity, and broad applicability.",
    "authors": [
      "Rachit Saluja",
      "Asli Cihangir",
      "Ruining Deng",
      "Johannes C. Paetzold",
      "Fengbei Liu",
      "Mert R. Sabuncu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19394v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19394v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.19119v1",
    "title": "MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images",
    "summary": "Spatial reasoning (SR), the ability to infer 3D spatial information from 2D inputs, is essential for real-world applications such as embodied AI and autonomous driving. However, existing research primarily focuses on indoor environments and typically relies on multi-view observations, which limits their generalizability to outdoor scenarios and constrains their applicability to monocular images, the most common real-world setting. In this work, we propose MonoSR, a large-scale monocular spatial reasoning dataset that spans diverse scenarios including indoor, outdoor, and object-centric settings, and supports multiple question types. MonoSR provides a path toward open-world monocular spatial reasoning. Beyond introducing the dataset, we evaluate advanced vision-language models to reveal their limitations on this challenging task. We further analyze whether auxiliary information is crucial for monocular spatial reasoning and offer practical guidance for designing future models. These contributions collectively establish a foundation for advancing monocular spatial reasoning in real-world, open-world environments.",
    "authors": [
      "Qirui Wang",
      "Jingyi He",
      "Yining Pan",
      "Si Yong Yeo",
      "Xulei Yang",
      "Shijie Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19119v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19119v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.18946v1",
    "title": "Leveraging Adversarial Learning for Pathological Fidelity in Virtual Staining",
    "summary": "In addition to evaluating tumor morphology using H&E staining, immunohistochemistry is used to assess the presence of specific proteins within the tissue. However, this is a costly and labor-intensive technique, for which virtual staining, as an image-to-image translation task, offers a promising alternative. Although recent, this is an emerging field of research with 64% of published studies just in 2024. Most studies use publicly available datasets of H&E-IHC pairs from consecutive tissue sections. Recognizing the training challenges, many authors develop complex virtual staining models based on conditional Generative Adversarial Networks, but ignore the impact of adversarial loss on the quality of virtual staining. Furthermore, overlooking the issues of model evaluation, they claim improved performance based on metrics such as SSIM and PSNR, which are not sufficiently robust to evaluate the quality of virtually stained images. In this paper, we developed CSSP2P GAN, which we demonstrate to achieve heightened pathological fidelity through a blind pathological expert evaluation. Furthermore, while iteratively developing our model, we study the impact of the adversarial loss and demonstrate its crucial role in the quality of virtually stained images. Finally, while comparing our model with reference works in the field, we underscore the limitations of the currently used evaluation metrics and demonstrate the superior performance of CSSP2P GAN.",
    "authors": [
      "José Teixeira",
      "Pascal Klöckner",
      "Diana Montezuma",
      "Melis Erdal Cesur",
      "João Fraga",
      "Hugo M. Horlings",
      "Jaime S. Cardoso",
      "Sara P. Oliveira"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.18946v1",
    "pdf_url": "https://arxiv.org/pdf/2511.18946v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.19426v1",
    "title": "Ref-SAM3D: Bridging SAM3D with Text for Reference 3D Reconstruction",
    "summary": "SAM3D has garnered widespread attention for its strong 3D object reconstruction capabilities. However, a key limitation remains: SAM3D cannot reconstruct specific objects referred to by textual descriptions, a capability that is essential for practical applications such as 3D editing, game development, and virtual environments. To address this gap, we introduce Ref-SAM3D, a simple yet effective extension to SAM3D that incorporates textual descriptions as a high-level prior, enabling text-guided 3D reconstruction from a single RGB image. Through extensive qualitative experiments, we show that Ref-SAM3D, guided only by natural language and a single 2D view, delivers competitive and high-fidelity zero-shot reconstruction performance. Our results demonstrate that Ref-SAM3D effectively bridges the gap between 2D visual cues and 3D geometric understanding, offering a more flexible and accessible paradigm for reference-guided 3D reconstruction. Code is available at: https://github.com/FudanCVL/Ref-SAM3D.",
    "authors": [
      "Yun Zhou",
      "Yaoting Wang",
      "Guangquan Jie",
      "Jinyu Liu",
      "Henghui Ding"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19426v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19426v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.42
  },
  {
    "arxiv_id": "2511.19089v1",
    "title": "Theoretical and Empirical Analysis of Lehmer Codes to Search Permutation Spaces with Evolutionary Algorithms",
    "summary": "A suitable choice of the representation of candidate solutions is crucial for the efficiency of evolutionary algorithms and related metaheuristics. We focus on problems in permutation spaces, which are at the core of numerous practical applications of such algorithms, e.g. in scheduling and transportation. Inversion vectors (also called Lehmer codes) are an alternative representation of the permutation space $S_n$ compared to the classical encoding as a vector of $n$ unique entries. In particular, they do not require any constraint handling. Using rigorous mathematical runtime analyses, we compare the efficiency of inversion vector encodings to the classical representation and give theory-guided advice on their choice. Moreover, we link the effect of local changes in the inversion code space to classical measures on permutations like the number of inversions. Finally, through experimental studies on linear ordering and quadratic assignment problems, we demonstrate the practical efficiency of inversion vector encodings.",
    "authors": [
      "Yuxuan Ma",
      "Valentino Santucci",
      "Carsten Witt"
    ],
    "categories": [
      "cs.NE"
    ],
    "published": "2025-11-24",
    "url": "https://arxiv.org/abs/2511.19089v1",
    "pdf_url": "https://arxiv.org/pdf/2511.19089v1.pdf",
    "date": "2025-11-25",
    "source": "arxiv",
    "research_score": 0.42
  }
]