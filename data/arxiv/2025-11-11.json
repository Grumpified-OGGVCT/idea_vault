[
  {
    "arxiv_id": "2511.07390v1",
    "title": "A Diffusion Model to Shrink Proteins While Maintaining Their Function",
    "summary": "Many proteins useful in modern medicine or bioengineering are challenging to make in the lab, fuse with other proteins in cells, or deliver to tissues in the body, because their sequences are too long. Shortening these sequences typically involves costly, time-consuming experimental campaigns. Ideally, we could instead use modern models of massive databases of sequences from nature to learn how to propose shrunken proteins that resemble sequences found in nature. Unfortunately, these models struggle to efficiently search the combinatorial space of all deletions, and are not trained with inductive biases to learn how to delete. To address this gap, we propose SCISOR, a novel discrete diffusion model that deletes letters from sequences to generate protein samples that resemble those found in nature. To do so, SCISOR trains a de-noiser to reverse a forward noising process that adds random insertions to natural sequences. As a generative model, SCISOR fits evolutionary sequence data competitively with previous large models. In evaluation, SCISOR achieves state-of-the-art predictions of the functional effects of deletions on ProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein sequences, and show that its suggested deletions result in significantly more realistic proteins and more often preserve functional motifs than previous models of evolutionary sequences.",
    "authors": [
      "Ethan Baron",
      "Alan N. Amin",
      "Ruben Weitzman",
      "Debora Marks",
      "Andrew Gordon Wilson"
    ],
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07390v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07390v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.94
  },
  {
    "arxiv_id": "2511.07343v1",
    "title": "TNT: Improving Chunkwise Training for Test-Time Memorization",
    "summary": "Recurrent neural networks (RNNs) with deep test-time memorization modules, such as Titans and TTT, represent a promising, linearly-scaling paradigm distinct from Transformers. While these expressive models do not yet match the peak performance of state-of-the-art Transformers, their potential has been largely untapped due to prohibitively slow training and low hardware utilization. Existing parallelization methods force a fundamental conflict governed by the chunksize hyperparameter: large chunks boost speed but degrade performance, necessitating a fixed, suboptimal compromise. To solve this challenge, we introduce TNT, a novel training paradigm that decouples training efficiency from inference performance through a two-stage process. Stage one is an efficiency-focused pre-training phase utilizing a hierarchical memory. A global module processes large, hardware-friendly chunks for long-range context, while multiple parallel local modules handle fine-grained details. Crucially, by periodically resetting local memory states, we break sequential dependencies to enable massive context parallelization. Stage two is a brief fine-tuning phase where only the local memory modules are adapted to a smaller, high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated on Titans and TTT models, TNT achieves a substantial acceleration in training speed-up to 17 times faster than the most accurate baseline configuration - while simultaneously improving model accuracy. This improvement removes a critical scalability barrier, establishing a practical foundation for developing expressive RNNs and facilitating future work to close the performance gap with Transformers.",
    "authors": [
      "Zeman Li",
      "Ali Behrouz",
      "Yuan Deng",
      "Peilin Zhong",
      "Praneeth Kacham",
      "Mahdi Karami",
      "Meisam Razaviyayn",
      "Vahab Mirrokni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07343v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07343v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.91
  },
  {
    "arxiv_id": "2511.07329v1",
    "title": "Preparation of Fractal-Inspired Computational Architectures for Advanced   Large Language Model Analysis",
    "summary": "It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.",
    "authors": [
      "Yash Mittal",
      "Dmitry Ignatov",
      "Radu Timofte"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07329v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07329v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2511.07213v1",
    "title": "DETECT: Data-Driven Evaluation of Treatments Enabled by Classification   Transformers",
    "summary": "Chronic pain is a global health challenge affecting millions of individuals, making it essential for physicians to have reliable and objective methods to measure the functional impact of clinical treatments. Traditionally used methods, like the numeric rating scale, while personalized and easy to use, are subjective due to their self-reported nature. Thus, this paper proposes DETECT (Data-Driven Evaluation of Treatments Enabled by Classification Transformers), a data-driven framework that assesses treatment success by comparing patient activities of daily life before and after treatment. We use DETECT on public benchmark datasets and simulated patient data from smartphone sensors. Our results demonstrate that DETECT is objective yet lightweight, making it a significant and novel contribution to clinical decision-making. By using DETECT, independently or together with other self-reported metrics, physicians can improve their understanding of their treatment impacts, ultimately leading to more personalized and responsive patient care.",
    "authors": [
      "Yuanheng Mao",
      "Lillian Yang",
      "Stephen Yang",
      "Ethan Shao",
      "Zihan Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07213v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07213v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.83
  },
  {
    "arxiv_id": "2511.07308v1",
    "title": "Can Training Dynamics of Scale-Invariant Neural Networks Be Explained by   the Thermodynamics of an Ideal Gas?",
    "summary": "Understanding the training dynamics of deep neural networks remains a major open problem, with physics-inspired approaches offering promising insights. Building on this perspective, we develop a thermodynamic framework to describe the stationary distributions of stochastic gradient descent (SGD) with weight decay for scale-invariant neural networks, a setting that both reflects practical architectures with normalization layers and permits theoretical analysis. We establish analogies between training hyperparameters (e.g., learning rate, weight decay) and thermodynamic variables such as temperature, pressure, and volume. Starting with a simplified isotropic noise model, we uncover a close correspondence between SGD dynamics and ideal gas behavior, validated through theory and simulation. Extending to training of neural networks, we show that key predictions of the framework, including the behavior of stationary entropy, align closely with experimental observations. This framework provides a principled foundation for interpreting training dynamics and may guide future work on hyperparameter tuning and the design of learning rate schedulers.",
    "authors": [
      "Ildus Sadrtdinov",
      "Ekaterina Lobacheva",
      "Ivan Klimov",
      "Mikhail I. Katsnelson",
      "Dmitry Vetrov"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07308v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07308v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.81
  },
  {
    "arxiv_id": "2511.07068v1",
    "title": "ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via   Concept Mining from Text Corpora",
    "summary": "Large-scale visual out-of-distribution (OOD) detection has witnessed remarkable progress by leveraging vision-language models such as CLIP. However, a significant limitation of current methods is their reliance on a pre-defined set of in-distribution (ID) ground-truth label names (positives). These fixed label names can be unavailable, unreliable at scale, or become less relevant due to in-distribution shifts after deployment. Towards truly unsupervised OOD detection, we utilize widely available text corpora for positive label mining, bypassing the need for positives. In this paper, we utilize widely available text corpora for positive label mining under a general concept mining paradigm. Within this framework, we propose ClusterMine, a novel positive label mining method. ClusterMine is the first method to achieve state-of-the-art OOD detection performance without access to positive labels. It extracts positive concepts from a large text corpus by combining visual-only sample consistency (via clustering) and zero-shot image-text consistency. Our experimental study reveals that ClusterMine is scalable across a plethora of CLIP models and achieves state-of-the-art robustness to covariate in-distribution shifts. The code is available at https://github.com/HHU-MMBS/clustermine_wacv_official.",
    "authors": [
      "Nikolas Adaloglou",
      "Diana Petrusheva",
      "Mohamed Asker",
      "Felix Michels",
      "Markus Kollmann"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07068v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07068v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.07061v1",
    "title": "Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and   Curriculum Learning",
    "summary": "Emotion Recognition in Conversation (ERC) is a crucial task for understanding human emotions and enabling natural human-computer interaction. Although Large Language Models (LLMs) have recently shown great potential in this field, their ability to capture the intrinsic connections between explicit and implicit emotions remains limited. We propose a novel ERC training framework, PRC-Emo, which integrates Prompt engineering, demonstration Retrieval, and Curriculum learning, with the goal of exploring whether LLMs can effectively perceive emotions in conversational contexts. Specifically, we design emotion-sensitive prompt templates based on both explicit and implicit emotional cues to better guide the model in understanding the speaker's psychological states. We construct the first dedicated demonstration retrieval repository for ERC, which includes training samples from widely used datasets, as well as high-quality dialogue examples generated by LLMs and manually verified. Moreover, we introduce a curriculum learning strategy into the LoRA fine-tuning process, incorporating weighted emotional shifts between same-speaker and different-speaker utterances to assign difficulty levels to dialogue samples, which are then organized in an easy-to-hard training sequence. Experimental results on two benchmark datasets-- IEMOCAP and MELD --show that our method achieves new state-of-the-art (SOTA) performance, demonstrating the effectiveness and generalizability of our approach in improving LLM-based emotional understanding.",
    "authors": [
      "Xinran Li",
      "Xiujuan Xu",
      "Jiaqi Qiao",
      "Yu Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07061v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07061v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.07418v1",
    "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with   Contact Fields",
    "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.",
    "authors": [
      "Zhao-Heng Yin",
      "Pieter Abbeel"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.DC",
      "cs.GR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07418v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07418v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2511.07205v1",
    "title": "Twenty-Five Years of MIR Research: Achievements, Practices, Evaluations,   and Future Challenges",
    "summary": "In this paper, we trace the evolution of Music Information Retrieval (MIR) over the past 25 years. While MIR gathers all kinds of research related to music informatics, a large part of it focuses on signal processing techniques for music data, fostering a close relationship with the IEEE Audio and Acoustic Signal Processing Technical Commitee. In this paper, we reflect the main research achievements of MIR along the three EDICS related to music analysis, processing and generation. We then review a set of successful practices that fuel the rapid development of MIR research. One practice is the annual research benchmark, the Music Information Retrieval Evaluation eXchange, where participants compete on a set of research tasks. Another practice is the pursuit of reproducible and open research. The active engagement with industry research and products is another key factor for achieving large societal impacts and motivating younger generations of students to join the field. Last but not the least, the commitment to diversity, equity and inclusion ensures MIR to be a vibrant and open community where various ideas, methodologies, and career pathways collide. We finish by providing future challenges MIR will have to face.",
    "authors": [
      "Geoffroy Peeters",
      "Zafar Rafii",
      "Magdalena Fuentes",
      "Zhiyao Duan",
      "Emmanouil Benetos",
      "Juhan Nam",
      "Yuki Mitsufuji"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07205v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07205v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2511.07379v1",
    "title": "LoReTTA: A Low Resource Framework To Poison Continuous Time Dynamic   Graphs",
    "summary": "Temporal Graph Neural Networks (TGNNs) are increasingly used in high-stakes domains, such as financial forecasting, recommendation systems, and fraud detection. However, their susceptibility to poisoning attacks poses a critical security risk. We introduce LoReTTA (Low Resource Two-phase Temporal Attack), a novel adversarial framework on Continuous-Time Dynamic Graphs, which degrades TGNN performance by an average of 29.47% across 4 widely benchmark datasets and 4 State-of-the-Art (SotA) models. LoReTTA operates through a two-stage approach: (1) sparsify the graph by removing high-impact edges using any of the 16 tested temporal importance metrics, (2) strategically replace removed edges with adversarial negatives via LoReTTA's novel degree-preserving negative sampling algorithm. Our plug-and-play design eliminates the need for expensive surrogate models while adhering to realistic unnoticeability constraints. LoReTTA degrades performance by upto 42.0% on MOOC, 31.5% on Wikipedia, 28.8% on UCI, and 15.6% on Enron. LoReTTA outperforms 11 attack baselines, remains undetectable to 4 leading anomaly detection systems, and is robust to 4 SotA adversarial defense training methods, establishing its effectiveness, unnoticeability, and robustness.",
    "authors": [
      "Himanshu Pal",
      "Venkata Sai Pranav Bachina",
      "Ankit Gangwal",
      "Charu Sharma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07379v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07379v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.07233v1",
    "title": "Noise & pattern: identity-anchored Tikhonov regularization for robust   structural anomaly detection",
    "summary": "Anomaly detection plays a pivotal role in automated industrial inspection, aiming to identify subtle or rare defects in otherwise uniform visual patterns. As collecting representative examples of all possible anomalies is infeasible, we tackle structural anomaly detection using a self-supervised autoencoder that learns to repair corrupted inputs. To this end, we introduce a corruption model that injects artificial disruptions into training images to mimic structural defects. While reminiscent of denoising autoencoders, our approach differs in two key aspects. First, instead of unstructured i.i.d.\\ noise, we apply structured, spatially coherent perturbations that make the task a hybrid of segmentation and inpainting. Second, and counterintuitively, we add and preserve Gaussian noise on top of the occlusions, which acts as a Tikhonov regularizer anchoring the Jacobian of the reconstruction function toward identity. This identity-anchored regularization stabilizes reconstruction and further improves both detection and segmentation accuracy. On the MVTec AD benchmark, our method achieves state-of-the-art results (I/P-AUROC: 99.9/99.4), supporting our theoretical framework and demonstrating its practical relevance for automatic inspection.",
    "authors": [
      "Alexander Bauer",
      "Klaus-Robert Müller"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07233v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07233v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.06898v1",
    "title": "A Hybrid Autoencoder-Transformer Model for Robust Day-Ahead Electricity   Price Forecasting under Extreme Conditions",
    "summary": "Accurate day-ahead electricity price forecasting (DAEPF) is critical for the efficient operation of power systems, but extreme condition and market anomalies pose significant challenges to existing forecasting methods. To overcome these challenges, this paper proposes a novel hybrid deep learning framework that integrates a Distilled Attention Transformer (DAT) model and an Autoencoder Self-regression Model (ASM). The DAT leverages a self-attention mechanism to dynamically assign higher weights to critical segments of historical data, effectively capturing both long-term trends and short-term fluctuations. Concurrently, the ASM employs unsupervised learning to detect and isolate anomalous patterns induced by extreme conditions, such as heavy rain, heat waves, or human festivals. Experiments on datasets sampled from California and Shandong Province demonstrate that our framework significantly outperforms state-of-the-art methods in prediction accuracy, robustness, and computational efficiency. Our framework thus holds promise for enhancing grid resilience and optimizing market operations in future power systems.",
    "authors": [
      "Boyan Tang",
      "Xuanhao Ren",
      "Peng Xiao",
      "Shunbo Lei",
      "Xiaorong Sun",
      "Jianghua Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06898v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06898v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2511.07328v1",
    "title": "Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder   Training",
    "summary": "Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.",
    "authors": [
      "Artyom Sorokin",
      "Nazar Buzun",
      "Alexander Anokhin",
      "Oleg Inozemcev",
      "Egor Vedernikov",
      "Petr Anokhin",
      "Mikhail Burtsev",
      "Trushkov Alexey",
      "Yin Wenshuai",
      "Evgeny Burnaev"
    ],
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07328v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07328v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.07198v1",
    "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM   Fine-Tuning",
    "summary": "Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.",
    "authors": [
      "Hua Ye",
      "Siyuan Chen",
      "Haoliang Zhang",
      "Weihao Luo",
      "Yanbin Li",
      "Xuan Zhang"
    ],
    "categories": [
      "cs.LG",
      "I.2.6"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07198v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07198v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.07171v1",
    "title": "Federated Learning for Video Violence Detection: Complementary Roles of   Lightweight CNNs and Vision-Language Models for Energy-Efficient Use",
    "summary": "Deep learning-based video surveillance increasingly demands privacy-preserving architectures with low computational and environmental overhead. Federated learning preserves privacy but deploying large vision-language models (VLMs) introduces major energy and sustainability challenges. We compare three strategies for federated violence detection under realistic non-IID splits on the RWF-2000 and RLVS datasets: zero-shot inference with pretrained VLMs, LoRA-based fine-tuning of LLaVA-NeXT-Video-7B, and personalized federated learning of a 65.8M-parameter 3D CNN. All methods exceed 90% accuracy in binary violence detection. The 3D CNN achieves superior calibration (ROC AUC 92.59%) at roughly half the energy cost (240 Wh vs. 570 Wh) of federated LoRA, while VLMs provide richer multimodal reasoning. Hierarchical category grouping (based on semantic similarity and class exclusion) boosts VLM multiclass accuracy from 65.31% to 81% on the UCF-Crime dataset. To our knowledge, this is the first comparative simulation study of LoRA-tuned VLMs and personalized CNNs for federated violence detection, with explicit energy and CO2e quantification. Our results inform hybrid deployment strategies that default to efficient CNNs for routine inference and selectively engage VLMs for complex contextual reasoning.",
    "authors": [
      "Sébastien Thuau",
      "Siba Haidar",
      "Rachid Chelouah"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07171v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07171v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.07098v1",
    "title": "Boosting Fine-Grained Urban Flow Inference via Lightweight Architecture   and Focalized Optimization",
    "summary": "Fine-grained urban flow inference is crucial for urban planning and intelligent transportation systems, enabling precise traffic management and resource allocation. However, the practical deployment of existing methods is hindered by two key challenges: the prohibitive computational cost of over-parameterized models and the suboptimal performance of conventional loss functions on the highly skewed distribution of urban flows. To address these challenges, we propose a unified solution that synergizes architectural efficiency with adaptive optimization. Specifically, we first introduce PLGF, a lightweight yet powerful architecture that employs a Progressive Local-Global Fusion strategy to effectively capture both fine-grained details and global contextual dependencies. Second, we propose DualFocal Loss, a novel function that integrates dual-space supervision with a difficulty-aware focusing mechanism, enabling the model to adaptively concentrate on hard-to-predict regions. Extensive experiments on 4 real-world scenarios validate the effectiveness and scalability of our method. Notably, while achieving state-of-the-art performance, PLGF reduces the model size by up to 97% compared to current high-performing methods. Furthermore, under comparable parameter budgets, our model yields an accuracy improvement of over 10% against strong baselines. The implementation is included in the https://github.com/Yasoz/PLGF.",
    "authors": [
      "Yuanshao Zhu",
      "Xiangyu Zhao",
      "Zijian Zhang",
      "Xuetao Wei",
      "James Jianqiao Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07098v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07098v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.07377v1",
    "title": "Real-Time LiDAR Super-Resolution via Frequency-Aware Multi-Scale Fusion",
    "summary": "LiDAR super-resolution addresses the challenge of achieving high-quality 3D perception from cost-effective, low-resolution sensors. While recent transformer-based approaches like TULIP show promise, they remain limited to spatial-domain processing with restricted receptive fields. We introduce FLASH (Frequency-aware LiDAR Adaptive Super-resolution with Hierarchical fusion), a novel framework that overcomes these limitations through dual-domain processing. FLASH integrates two key innovations: (i) Frequency-Aware Window Attention that combines local spatial attention with global frequency-domain analysis via FFT, capturing both fine-grained geometry and periodic scanning patterns at log-linear complexity. (ii) Adaptive Multi-Scale Fusion that replaces conventional skip connections with learned position-specific feature aggregation, enhanced by CBAM attention for dynamic feature selection. Extensive experiments on KITTI demonstrate that FLASH achieves state-of-the-art performance across all evaluation metrics, surpassing even uncertainty-enhanced baselines that require multiple forward passes. Notably, FLASH outperforms TULIP with Monte Carlo Dropout while maintaining single-pass efficiency, which enables real-time deployment. The consistent superiority across all distance ranges validates that our dual-domain approach effectively handles uncertainty through architectural design rather than computationally expensive stochastic inference, making it practical for autonomous systems.",
    "authors": [
      "June Moh Goo",
      "Zichao Zeng",
      "Jan Boehm"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07377v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07377v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.07311v1",
    "title": "ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding",
    "summary": "Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.",
    "authors": [
      "Tuan-Dung Le",
      "Shohreh Haddadan",
      "Thanh Q. Thieu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07311v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07311v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.07301v1",
    "title": "Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free   Object Detection",
    "summary": "Source-Free Object Detection (SFOD) aims to adapt a source-pretrained object detector to a target domain without access to source data. However, existing SFOD methods predominantly rely on internal knowledge from the source model, which limits their capacity to generalize across domains and often results in biased pseudo-labels, thereby hindering both transferability and discriminability. In contrast, Vision Foundation Models (VFMs), pretrained on massive and diverse data, exhibit strong perception capabilities and broad generalization, yet their potential remains largely untapped in the SFOD setting. In this paper, we propose a novel SFOD framework that leverages VFMs as external knowledge sources to jointly enhance feature alignment and label quality. Specifically, we design three VFM-based modules: (1) Patch-weighted Global Feature Alignment (PGFA) distills global features from VFMs using patch-similarity-based weighting to enhance global feature transferability; (2) Prototype-based Instance Feature Alignment (PIFA) performs instance-level contrastive learning guided by momentum-updated VFM prototypes; and (3) Dual-source Enhanced Pseudo-label Fusion (DEPF) fuses predictions from detection VFMs and teacher models via an entropy-aware strategy to yield more reliable supervision. Extensive experiments on six benchmarks demonstrate that our method achieves state-of-the-art SFOD performance, validating the effectiveness of integrating VFMs to simultaneously improve transferability and discriminability.",
    "authors": [
      "Huizai Yao",
      "Sicheng Zhao",
      "Pengteng Li",
      "Yi Cui",
      "Shuo Lu",
      "Weiyu Guo",
      "Yunfan Lu",
      "Yijie Xu",
      "Hui Xiong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07301v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07301v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.07006v1",
    "title": "S$^2$Drug: Bridging Protein Sequence and 3D Structure in Contrastive   Representation Learning for Virtual Screening",
    "summary": "Virtual screening (VS) is an essential task in drug discovery, focusing on the identification of small-molecule ligands that bind to specific protein pockets. Existing deep learning methods, from early regression models to recent contrastive learning approaches, primarily rely on structural data while overlooking protein sequences, which are more accessible and can enhance generalizability. However, directly integrating protein sequences poses challenges due to the redundancy and noise in large-scale protein-ligand datasets. To address these limitations, we propose \\textbf{S$^2$Drug}, a two-stage framework that explicitly incorporates protein \\textbf{S}equence information and 3D \\textbf{S}tructure context in protein-ligand contrastive representation learning. In the first stage, we perform protein sequence pretraining on ChemBL using an ESM2-based backbone, combined with a tailored data sampling strategy to reduce redundancy and noise on both protein and ligand sides. In the second stage, we fine-tune on PDBBind by fusing sequence and structure information through a residue-level gating module, while introducing an auxiliary binding site prediction task. This auxiliary task guides the model to accurately localize binding residues within the protein sequence and capture their 3D spatial arrangement, thereby refining protein-ligand matching. Across multiple benchmarks, S$^2$Drug consistently improves virtual screening performance and achieves strong results on binding site prediction, demonstrating the value of bridging sequence and structure in contrastive learning.",
    "authors": [
      "Bowei He",
      "Bowen Gao",
      "Yankai Chen",
      "Yanyan Lan",
      "Chen Ma",
      "Philip S. Yu",
      "Ya-Qin Zhang",
      "Wei-Ying Ma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07006v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07006v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.06991v1",
    "title": "CoLM: Collaborative Large Models via A Client-Server Paradigm",
    "summary": "Large models have achieved remarkable performance across a range of reasoning and understanding tasks. Prior work often utilizes model ensembles or multi-agent systems to collaboratively generate responses, effectively operating in a server-to-server paradigm. However, such approaches do not align well with practical deployment settings, where a limited number of server-side models are shared by many clients under modern internet architectures. In this paper, we introduce \\textbf{CoLM} (\\textbf{Co}llaboration in \\textbf{L}arge-\\textbf{M}odels), a novel framework for collaborative reasoning that redefines cooperation among large models from a client-server perspective. Unlike traditional ensemble methods that rely on simultaneous inference from multiple models to produce a single output, CoLM allows the outputs of multiple models to be aggregated or shared, enabling each client model to independently refine and update its own generation based on these high-quality outputs. This design enables collaborative benefits by fully leveraging both client-side and shared server-side models. We further extend CoLM to vision-language models (VLMs), demonstrating its applicability beyond language tasks. Experimental results across multiple benchmarks show that CoLM consistently improves model performance on previously failed queries, highlighting the effectiveness of collaborative guidance in enhancing single-model capabilities.",
    "authors": [
      "Siqi Huang",
      "Sida Huang",
      "Hongyuan Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06991v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06991v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.06943v1",
    "title": "PlantTraitNet: An Uncertainty-Aware Multimodal Framework for   Global-Scale Plant Trait Inference from Citizen Science Data",
    "summary": "Global plant maps of plant traits, such as leaf nitrogen or plant height, are essential for understanding ecosystem processes, including the carbon and energy cycles of the Earth system. However, existing trait maps remain limited by the high cost and sparse geographic coverage of field-based measurements. Citizen science initiatives offer a largely untapped resource to overcome these limitations, with over 50 million geotagged plant photographs worldwide capturing valuable visual information on plant morphology and physiology. In this study, we introduce PlantTraitNet, a multi-modal, multi-task uncertainty-aware deep learning framework that predictsfour key plant traits (plant height, leaf area, specific leaf area, and nitrogen content) from citizen science photos using weak supervision. By aggregating individual trait predictions across space, we generate global maps of trait distributions. We validate these maps against independent vegetation survey data (sPlotOpen) and benchmark them against leading global trait products. Our results show that PlantTraitNet consistently outperforms existing trait maps across all evaluated traits, demonstrating that citizen science imagery, when integrated with computer vision and geospatial AI, enables not only scalable but also more accurate global trait mapping. This approach offers a powerful new pathway for ecological research and Earth system modeling.",
    "authors": [
      "Ayushi Sharma",
      "Johanna Trost",
      "Daniel Lusk",
      "Johannes Dollinger",
      "Julian Schrader",
      "Christian Rossi",
      "Javier Lopatin",
      "Etienne Laliberté",
      "Simon Haberstroh",
      "Jana Eichel",
      "Daniel Mederer",
      "Jose Miguel Cerda-Paredes",
      "Shyam S. Phartyal",
      "Lisa-Maricia Schwarz",
      "Anja Linstädter",
      "Maria Conceição Caldeira",
      "Teja Kattenborn"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06943v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06943v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.07399v1",
    "title": "StreamDiffusionV2: A Streaming System for Dynamic and Interactive Video   Generation",
    "summary": "Generative models are reshaping the live-streaming industry by redefining how content is created, styled, and delivered. Previous image-based streaming diffusion models have powered efficient and creative live streaming products but have hit limits on temporal consistency due to the foundation of image-based designs. Recent advances in video diffusion have markedly improved temporal consistency and sampling efficiency for offline generation. However, offline generation systems primarily optimize throughput by batching large workloads. In contrast, live online streaming operates under strict service-level objectives (SLOs): time-to-first-frame must be minimal, and every frame must meet a per-frame deadline with low jitter. Besides, scalable multi-GPU serving for real-time streams remains largely unresolved so far. To address this, we present StreamDiffusionV2, a training-free pipeline for interactive live streaming with video diffusion models. StreamDiffusionV2 integrates an SLO-aware batching scheduler and a block scheduler, together with a sink-token--guided rolling KV cache, a motion-aware noise controller, and other system-level optimizations. Moreover, we introduce a scalable pipeline orchestration that parallelizes the diffusion process across denoising steps and network layers, achieving near-linear FPS scaling without violating latency guarantees. The system scales seamlessly across heterogeneous GPU environments and supports flexible denoising steps (e.g., 1--4), enabling both ultra-low-latency and higher-quality modes. Without TensorRT or quantization, StreamDiffusionV2 renders the first frame within 0.5s and attains 58.28 FPS with a 14B-parameter model and 64.52 FPS with a 1.3B-parameter model on four H100 GPUs, making state-of-the-art generative live streaming practical and accessible--from individual creators to enterprise-scale platforms.",
    "authors": [
      "Tianrui Feng",
      "Zhi Li",
      "Shuo Yang",
      "Haocheng Xi",
      "Muyang Li",
      "Xiuyu Li",
      "Lvmin Zhang",
      "Keting Yang",
      "Kelly Peng",
      "Song Han",
      "Maneesh Agrawala",
      "Kurt Keutzer",
      "Akio Kodaira",
      "Chenfeng Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07399v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07399v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07396v1",
    "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost   Constraints for Reasoning",
    "summary": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost. We introduce C3PO (Cost Controlled Cascaded Prediction Optimization), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget. We provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment.",
    "authors": [
      "Antonios Valkanas",
      "Soumyasundar Pal",
      "Pavel Rumiantsev",
      "Yingxue Zhang",
      "Mark Coates"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07396v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07396v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07327v1",
    "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State   Reconstruction",
    "summary": "Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.",
    "authors": [
      "Guoxin Chen",
      "Zile Qiao",
      "Xuanzhong Chen",
      "Donglei Yu",
      "Haotian Xu",
      "Wayne Xin Zhao",
      "Ruihua Song",
      "Wenbiao Yin",
      "Huifeng Yin",
      "Liwen Zhang",
      "Kuan Li",
      "Minpeng Liao",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Jingren Zhou"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07327v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07327v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07293v1",
    "title": "Verifying rich robustness properties for neural networks",
    "summary": "Robustness is a important problem in AI alignment and safety, with models such as neural networks being increasingly used in safety-critical systems. In the last decade, a large body of work has emerged on local robustness, i.e., checking if the decision of a neural network remains unchanged when the input is slightly perturbed. However, many of these approaches require specialized encoding and often ignore the confidence of a neural network on its output. In this paper, our goal is to build a generalized framework to specify and verify variants of robustness in neural network verification. We propose a specification framework using a simple grammar, which is flexible enough to capture most existing variants. This allows us to introduce new variants of robustness that take into account the confidence of the neural network in its outputs. Next, we develop a novel and powerful unified technique to verify all such variants in a homogeneous way, viz., by adding a few additional layers to the neural network. This enables us to use any state-of-the-art neural network verification tool, without having to tinker with the encoding within, while incurring an approximation error that we show is bounded. We perform an extensive experimental evaluation over a large suite of 8870 benchmarks having 138M parameters in a largest network, and show that we are able to capture a wide set of robustness variants and outperform direct encoding approaches by a significant margin.",
    "authors": [
      "Mohammad Afzal",
      "S. Akshay",
      "Ashutosh Gupta"
    ],
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07293v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07293v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07290v1",
    "title": "CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference   Quality Assessment of Compressed Video",
    "summary": "The prevalence of user-generated content (UGC) on platforms such as YouTube and TikTok has rendered no-reference (NR) perceptual video quality assessment (VQA) vital for optimizing video delivery. Nonetheless, the characteristics of non-professional acquisition and the subsequent transcoding of UGC video on sharing platforms present significant challenges for NR-VQA. Although NR-VQA models attempt to infer mean opinion scores (MOS), their modeling of subjective scores for compressed content remains limited due to the absence of fine-grained perceptual annotations of artifact types. To address these challenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the semantic understanding capabilities of large vision-language models. Our approach introduces a quality-aware prompting mechanism that integrates video metadata (e.g., resolution, frame rate, bitrate) with key fragments extracted from inter-frame variations to guide the BLIP-2 pretraining approach in generating fine-grained quality captions. A unified architecture has been designed to model perceptual quality across three dimensions: semantic alignment, temporal characteristics, and spatial characteristics. These multimodal features are extracted and fused, then regressed to video quality scores. Extensive experiments on a wide variety of UGC datasets demonstrate that our model consistently outperforms existing NR-VQA methods, achieving improved accuracy without the need for costly manual fine-grained annotations. Our method achieves the best performance in terms of average rank and linear correlation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods. The source code and trained models, along with a user-friendly demo, are available at: https://github.com/xinyiW915/CAMP-VQA.",
    "authors": [
      "Xinyi Wang",
      "Angeliki Katsenou",
      "Junxiao Shen",
      "David Bull"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07290v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07290v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07262v1",
    "title": "AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery   in Scientific Machine Learning",
    "summary": "Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.",
    "authors": [
      "Qile Jiang",
      "George Karniadakis"
    ],
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07262v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07262v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07238v1",
    "title": "Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation",
    "summary": "In autonomous driving and robotics, ensuring road safety and reliable decision-making critically depends on out-of-distribution (OOD) segmentation. While numerous methods have been proposed to detect anomalous objects on the road, leveraging the vision-language space-which provides rich linguistic knowledge-remains an underexplored field. We hypothesize that incorporating these linguistic cues can be especially beneficial in the complex contexts found in real-world autonomous driving scenarios.   To this end, we present a novel approach that trains a Text-Driven OOD Segmentation model to learn a semantically diverse set of objects in the vision-language space. Concretely, our approach combines a vision-language model's encoder with a transformer decoder, employs Distance-Based OOD prompts located at varying semantic distances from in-distribution (ID) classes, and utilizes OOD Semantic Augmentation for OOD representations. By aligning visual and textual information, our approach effectively generalizes to unseen objects and provides robust OOD segmentation in diverse driving environments.   We conduct extensive experiments on publicly available OOD segmentation datasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets, demonstrating that our approach achieves state-of-the-art performance across both pixel-level and object-level evaluations. This result underscores the potential of vision-language-based OOD segmentation to bolster the safety and reliability of future autonomous driving systems.",
    "authors": [
      "Seungheon Song",
      "Jaekoo Lee"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07238v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07238v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07085v1",
    "title": "Achieving Effective Virtual Reality Interactions via Acoustic Gesture   Recognition based on Large Language Models",
    "summary": "Natural and efficient interaction remains a critical challenge for virtual reality and augmented reality (VR/AR) systems. Vision-based gesture recognition suffers from high computational cost, sensitivity to lighting conditions, and privacy leakage concerns. Acoustic sensing provides an attractive alternative: by emitting inaudible high-frequency signals and capturing their reflections, channel impulse response (CIR) encodes how gestures perturb the acoustic field in a low-cost and user-transparent manner. However, existing CIR-based gesture recognition methods often rely on extensive training of models on large labeled datasets, making them unsuitable for few-shot VR scenarios. In this work, we propose the first framework that leverages large language models (LLMs) for CIR-based gesture recognition in VR/AR systems. Despite LLMs' strengths, it is non-trivial to achieve few-shot and zero-shot learning of CIR gestures due to their inconspicuous features. To tackle this challenge, we collect differential CIR rather than original CIR data. Moreover, we construct a real-world dataset collected from 10 participants performing 15 gestures across three categories (digits, letters, and shapes), with 10 repetitions each. We then conduct extensive experiments on this dataset using an LLM-adopted classifier. Results show that our LLM-based framework achieves accuracy comparable to classical machine learning baselines, while requiring no domain-specific retraining.",
    "authors": [
      "Xijie Zhang",
      "Fengliang He",
      "Hong-Ning Dai"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07085v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07085v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.06944v1",
    "title": "From Attribution to Action: Jointly ALIGNing Predictions and   Explanations",
    "summary": "Explanation-guided learning (EGL) has shown promise in aligning model predictions with interpretable reasoning, particularly in computer vision tasks. However, most approaches rely on external annotations or heuristic-based segmentation to supervise model explanations, which can be noisy, imprecise and difficult to scale. In this work, we provide both empirical and theoretical evidence that low-quality supervision signals can degrade model performance rather than improve it. In response, we propose ALIGN, a novel framework that jointly trains a classifier and a masker in an iterative manner. The masker learns to produce soft, task-relevant masks that highlight informative regions, while the classifier is optimized for both prediction accuracy and alignment between its saliency maps and the learned masks. By leveraging high-quality masks as guidance, ALIGN improves both interpretability and generalizability, showing its superiority across various settings. Experiments on the two domain generalization benchmarks, VLCS and Terra Incognita, show that ALIGN consistently outperforms six strong baselines in both in-distribution and out-of-distribution settings. Besides, ALIGN also yields superior explanation quality concerning sufficiency and comprehensiveness, highlighting its effectiveness in producing accurate and interpretable models.",
    "authors": [
      "Dongsheng Hong",
      "Chao Chen",
      "Yanhui Chen",
      "Shanshan Lin",
      "Zhihao Chen",
      "Xiangwen Liao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06944v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06944v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.06908v1",
    "title": "Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text   Encoding for Monocular 3D Visual Grounding",
    "summary": "Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D objects in RGB images using text descriptions with geometric cues. However, existing methods face two key limitations. Firstly, they often over-rely on high-certainty keywords that explicitly identify the target object while neglecting critical spatial descriptions. Secondly, generalized textual features contain both 2D and 3D descriptive information, thereby capturing an additional dimension of details compared to singular 2D or 3D visual features. This characteristic leads to cross-dimensional interference when refining visual features under text guidance. To overcome these challenges, we propose Mono3DVG-EnSD, a novel framework that integrates two key components: the CLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled Module (D2M). The CLIP-LCA dynamically masks high-certainty keywords while retaining low-certainty implicit spatial descriptions, thereby forcing the model to develop a deeper understanding of spatial relationships in captions for object localization. Meanwhile, the D2M decouples dimension-specific (2D/3D) textual features from generalized textual features to guide corresponding visual features at same dimension, which mitigates cross-dimensional interference by ensuring dimensionally-consistent cross-modal interactions. Through comprehensive comparisons and ablation studies on the Mono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance across all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario by a significant +13.54%.",
    "authors": [
      "Yuzhen Li",
      "Min Liu",
      "Zhaoyang Li",
      "Yuan Bian",
      "Xueping Wang",
      "Erbo Zhai",
      "Yaonan Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06908v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06908v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.07338v1",
    "title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
    "summary": "Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.",
    "authors": [
      "Zhen Wang",
      "Yufan Zhou",
      "Zhongyan Luo",
      "Lyumanshan Ye",
      "Adam Wood",
      "Man Yao",
      "Luoshang Pan"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "68T07, 68T20",
      "I.2.7; I.2.6; I.2.11"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07338v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07338v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.07025v1",
    "title": "Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for   Multilingual and Cross-Lingual Tasks",
    "summary": "We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.",
    "authors": [
      "Yauhen Babakhin",
      "Radek Osmulski",
      "Ronay Ak",
      "Gabriel Moreira",
      "Mengyao Xu",
      "Benedikt Schifferer",
      "Bo Liu",
      "Even Oldridge"
    ],
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07025v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07025v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.07003v1",
    "title": "Beyond English: Toward Inclusive and Scalable Multilingual Machine   Translation with LLMs",
    "summary": "Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \\textbf{LMT}, a suite of \\textbf{L}arge-scale \\textbf{M}ultilingual \\textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \\textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \\textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \\textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \\footnote{\\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.",
    "authors": [
      "Yingfeng Luo",
      "Ziqiang Xu",
      "Yuxuan Ouyang",
      "Murun Yang",
      "Dingyang Lin",
      "Kaiyan Chang",
      "Tong Zheng",
      "Bei Li",
      "Peinan Feng",
      "Quan Du",
      "Tong Xiao",
      "Jingbo Zhu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07003v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07003v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.07372v1",
    "title": "Provable Benefit of Curriculum in Transformer Tree-Reasoning   Post-Training",
    "summary": "Recent curriculum techniques in the post-training stage of LLMs have been widely observed to outperform non-curriculum approaches in enhancing reasoning performance, yet a principled understanding of why and to what extent they work remains elusive. To address this gap, we develop a theoretical framework grounded in the intuition that progressively learning through manageable steps is more efficient than directly tackling a hard reasoning task, provided each stage stays within the model's effective competence. Under mild complexity conditions linking consecutive curriculum stages, we show that curriculum post-training avoids the exponential complexity bottleneck.   To substantiate this result, drawing insights from the Chain-of-Thoughts (CoTs) solving mathematical problems such as Countdown and parity, we model CoT generation as a states-conditioned autoregressive reasoning tree, define a uniform-branching base model to capture pretrained behavior, and formalize curriculum stages as either depth-increasing (longer reasoning chains) or hint-decreasing (shorter prefixes) subtasks. Our analysis shows that, under outcome-only reward signals, reinforcement learning finetuning achieves high accuracy with polynomial sample complexity, whereas direct learning suffers from an exponential bottleneck. We further establish analogous guarantees for test-time scaling, where curriculum-aware querying reduces both reward oracle calls and sampling cost from exponential to polynomial order.",
    "authors": [
      "Dake Bu",
      "Wei Huang",
      "Andi Han",
      "Atsushi Nitanda",
      "Hau-San Wong",
      "Qingfu Zhang",
      "Taiji Suzuki"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07372v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07372v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.07296v1",
    "title": "Who Is the Story About? Protagonist Entity Recognition in News",
    "summary": "News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.",
    "authors": [
      "Jorge Gabín",
      "M. Eduardo Ares",
      "Javier Parapar"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07296v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07296v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.07282v1",
    "title": "MG-HGNN: A Heterogeneous GNN Framework for Indoor Wi-Fi   Fingerprint-Based Localization",
    "summary": "Received signal strength indicator (RSSI) is the primary representation of Wi-Fi fingerprints and serves as a crucial tool for indoor localization. However, existing RSSI-based positioning methods often suffer from reduced accuracy due to environmental complexity and challenges in processing multi-source information. To address these issues, we propose a novel multi-graph heterogeneous GNN framework (MG-HGNN) to enhance spatial awareness and improve positioning performance. In this framework, two graph construction branches perform node and edge embedding, respectively, to generate informative graphs. Subsequently, a heterogeneous graph neural network is employed for graph representation learning, enabling accurate positioning. The MG-HGNN framework introduces the following key innovations: 1) multi-type task-directed graph construction that combines label estimation and feature encoding for richer graph information; 2) a heterogeneous GNN structure that enhances the performance of conventional GNN models. Evaluations on the UJIIndoorLoc and UTSIndoorLoc public datasets demonstrate that MG-HGNN not only achieves superior performance compared to several state-of-the-art methods, but also provides a novel perspective for enhancing GNN-based localization methods. Ablation studies further confirm the rationality and effectiveness of the proposed framework.",
    "authors": [
      "Yibu Wang",
      "Zhaoxin Zhang",
      "Ning Li",
      "Xinlong Zhao",
      "Dong Zhao",
      "Tianzi Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07282v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07282v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.07166v1",
    "title": "AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and   Dual-Channel Reasoning",
    "summary": "We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.",
    "authors": [
      "Meiyun Wang",
      "Charin Polpanumas"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07166v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07166v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.07156v1",
    "title": "Conditional Diffusion as Latent Constraints for Controllable Symbolic   Music Generation",
    "summary": "Recent advances in latent diffusion models have demonstrated state-of-the-art performance in high-dimensional time-series data synthesis while providing flexible control through conditioning and guidance. However, existing methodologies primarily rely on musical context or natural language as the main modality of interacting with the generative process, which may not be ideal for expert users who seek precise fader-like control over specific musical attributes. In this work, we explore the application of denoising diffusion processes as plug-and-play latent constraints for unconditional symbolic music generation models. We focus on a framework that leverages a library of small conditional diffusion models operating as implicit probabilistic priors on the latents of a frozen unconditional backbone. While previous studies have explored domain-specific use cases, this work, to the best of our knowledge, is the first to demonstrate the versatility of such an approach across a diverse array of musical attributes, such as note density, pitch range, contour, and rhythm complexity. Our experiments show that diffusion-driven constraints outperform traditional attribute regularization and other latent constraints architectures, achieving significantly stronger correlations between target and generated attributes while maintaining high perceptual quality and diversity.",
    "authors": [
      "Matteo Pettenó",
      "Alessandro Ilic Mezza",
      "Alberto Bernardini"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07156v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07156v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.07103v1",
    "title": "GEWDiff: Geometric Enhanced Wavelet-based Diffusion Model for   Hyperspectral Image Super-resolution",
    "summary": "Improving the quality of hyperspectral images (HSIs), such as through super-resolution, is a crucial research area. However, generative modeling for HSIs presents several challenges. Due to their high spectral dimensionality, HSIs are too memory-intensive for direct input into conventional diffusion models. Furthermore, general generative models lack an understanding of the topological and geometric structures of ground objects in remote sensing imagery. In addition, most diffusion models optimize loss functions at the noise level, leading to a non-intuitive convergence behavior and suboptimal generation quality for complex data. To address these challenges, we propose a Geometric Enhanced Wavelet-based Diffusion Model (GEWDiff), a novel framework for reconstructing hyperspectral images at 4-times super-resolution. A wavelet-based encoder-decoder is introduced that efficiently compresses HSIs into a latent space while preserving spectral-spatial information. To avoid distortion during generation, we incorporate a geometry-enhanced diffusion process that preserves the geometric features. Furthermore, a multi-level loss function was designed to guide the diffusion process, promoting stable convergence and improved reconstruction fidelity. Our model demonstrated state-of-the-art results across multiple dimensions, including fidelity, spectral accuracy, visual realism, and clarity.",
    "authors": [
      "Sirui Wang",
      "Jiang He",
      "Natàlia Blasco Andreo",
      "Xiao Xiang Zhu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07103v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07103v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.06942v1",
    "title": "HLPD: Aligning LLMs to Human Language Preference for Machine-Revised   Text Detection",
    "summary": "To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.",
    "authors": [
      "Fangqi Dai",
      "Xingjian Jiang",
      "Zizhuang Deng"
    ],
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06942v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06942v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.07362v1",
    "title": "Inference-Time Scaling of Diffusion Models for Infrared Data Generation",
    "summary": "Infrared imagery enables temperature-based scene understanding using passive sensors, particularly under conditions of low visibility where traditional RGB imaging fails. Yet, developing downstream vision models for infrared applications is hindered by the scarcity of high-quality annotated data, due to the specialized expertise required for infrared annotation. While synthetic infrared image generation has the potential to accelerate model development by providing large-scale, diverse training data, training foundation-level generative diffusion models in the infrared domain has remained elusive due to limited datasets. In light of such data constraints, we explore an inference-time scaling approach using a domain-adapted CLIP-based verifier for enhanced infrared image generation quality. We adapt FLUX.1-dev, a state-of-the-art text-to-image diffusion model, to the infrared domain by finetuning it on a small sample of infrared images using parameter-efficient techniques. The trained verifier is then employed during inference to guide the diffusion sampling process toward higher quality infrared generations that better align with input text prompts. Empirically, we find that our approach leads to consistent improvements in generation quality, reducing FID scores on the KAIST Multispectral Pedestrian Detection Benchmark dataset by 10% compared to unguided baseline samples. Our results suggest that inference-time guidance offers a promising direction for bridging the domain gap in low-data infrared settings.",
    "authors": [
      "Kai A. Horstmann",
      "Maxim Clouser",
      "Kia Khezeli"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07362v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07362v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.07318v1",
    "title": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine   Hallucination Detection in LLMs",
    "summary": "Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.",
    "authors": [
      "Shaowen Wang",
      "Yiqi Dong",
      "Ruinian Chang",
      "Tansheng Zhu",
      "Yuebo Sun",
      "Kaifeng Lyu",
      "Jian Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07318v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07318v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.07092v1",
    "title": "Sample-efficient quantum error mitigation via classical learning   surrogates",
    "summary": "The pursuit of practical quantum utility on near-term quantum processors is critically challenged by their inherent noise. Quantum error mitigation (QEM) techniques are leading solutions to improve computation fidelity with relatively low qubit-overhead, while full-scale quantum error correction remains a distant goal. However, QEM techniques incur substantial measurement overheads, especially when applied to families of quantum circuits parameterized by classical inputs. Focusing on zero-noise extrapolation (ZNE), a widely adopted QEM technique, here we devise the surrogate-enabled ZNE (S-ZNE), which leverages classical learning surrogates to perform ZNE entirely on the classical side. Unlike conventional ZNE, whose measurement cost scales linearly with the number of circuits, S-ZNE requires only constant measurement overhead for an entire family of quantum circuits, offering superior scalability. Theoretical analysis indicates that S-ZNE achieves accuracy comparable to conventional ZNE in many practical scenarios, and numerical experiments on up to 100-qubit ground-state energy and quantum metrology tasks confirm its effectiveness. Our approach provides a template that can be effectively extended to other quantum error mitigation protocols, opening a promising path toward scalable error mitigation.",
    "authors": [
      "Wei-You Liao",
      "Ge Yan",
      "Yujin Song",
      "Tian-Ci Tian",
      "Wei-Ming Zhu",
      "De-Tao Jiang",
      "Yuxuan Du",
      "He-Liang Huang"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07092v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07092v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.07062v1",
    "title": "Improving Region Representation Learning from Urban Imagery with Noisy   Long-Caption Supervision",
    "summary": "Region representation learning plays a pivotal role in urban computing by extracting meaningful features from unlabeled urban data. Analogous to how perceived facial age reflects an individual's health, the visual appearance of a city serves as its ``portrait\", encapsulating latent socio-economic and environmental characteristics. Recent studies have explored leveraging Large Language Models (LLMs) to incorporate textual knowledge into imagery-based urban region representation learning. However, two major challenges remain: i)~difficulty in aligning fine-grained visual features with long captions, and ii) suboptimal knowledge incorporation due to noise in LLM-generated captions. To address these issues, we propose a novel pre-training framework called UrbanLN that improves Urban region representation learning through Long-text awareness and Noise suppression. Specifically, we introduce an information-preserved stretching interpolation strategy that aligns long captions with fine-grained visual semantics in complex urban scenes. To effectively mine knowledge from LLM-generated captions and filter out noise, we propose a dual-level optimization strategy. At the data level, a multi-model collaboration pipeline automatically generates diverse and reliable captions without human intervention. At the model level, we employ a momentum-based self-distillation mechanism to generate stable pseudo-targets, facilitating robust cross-modal learning under noisy conditions. Extensive experiments across four real-world cities and various downstream tasks demonstrate the superior performance of our UrbanLN.",
    "authors": [
      "Yimei Zhang",
      "Guojiang Shen",
      "Kaili Ning",
      "Tongwei Ren",
      "Xuebo Qiu",
      "Mengmeng Wang",
      "Xiangjie Kong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07062v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07062v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.06979v1",
    "title": "Breaking the Gradient Barrier: Unveiling Large Language Models for   Strategic Classification",
    "summary": "Strategic classification~(SC) explores how individuals or entities modify their features strategically to achieve favorable classification outcomes. However, existing SC methods, which are largely based on linear models or shallow neural networks, face significant limitations in terms of scalability and capacity when applied to real-world datasets with significantly increasing scale, especially in financial services and the internet sector. In this paper, we investigate how to leverage large language models to design a more scalable and efficient SC framework, especially in the case of growing individuals engaged with decision-making processes. Specifically, we introduce GLIM, a gradient-free SC method grounded in in-context learning. During the feed-forward process of self-attention, GLIM implicitly simulates the typical bi-level optimization process of SC, including both the feature manipulation and decision rule optimization. Without fine-tuning the LLMs, our proposed GLIM enjoys the advantage of cost-effective adaptation in dynamic strategic environments. Theoretically, we prove GLIM can support pre-trained LLMs to adapt to a broad range of strategic manipulations. We validate our approach through experiments with a collection of pre-trained LLMs on real-world and synthetic datasets in financial and internet domains, demonstrating that our GLIM exhibits both robustness and efficiency, and offering an effective solution for large-scale SC tasks.",
    "authors": [
      "Xinpeng Lv",
      "Yunxin Mao",
      "Haoxuan Li",
      "Ke Liang",
      "Jinxuan Yang",
      "Wanrong Huang",
      "Haoang Chi",
      "Huan Chen",
      "Long Lan",
      "Yuanlong Chen",
      "Wenjing Yang",
      "Haotian Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06979v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06979v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.06946v1",
    "title": "Learning to Focus: Prioritizing Informative Histories with Structured   Attention Mechanisms in Partially Observable Reinforcement Learning",
    "summary": "Transformers have shown strong ability to model long-term dependencies and are increasingly adopted as world models in model-based reinforcement learning (RL) under partial observability. However, unlike natural language corpora, RL trajectories are sparse and reward-driven, making standard self-attention inefficient because it distributes weight uniformly across all past tokens rather than emphasizing the few transitions critical for control. To address this, we introduce structured inductive priors into the self-attention mechanism of the dynamics head: (i) per-head memory-length priors that constrain attention to task-specific windows, and (ii) distributional priors that learn smooth Gaussian weightings over past state-action pairs. We integrate these mechanisms into UniZero, a model-based RL agent with a Transformer-based world model that supports planning under partial observability. Experiments on the Atari 100k benchmark show that most efficiency gains arise from the Gaussian prior, which smoothly allocates attention to informative transitions, while memory-length priors often truncate useful signals with overly restrictive cut-offs. In particular, Gaussian Attention achieves a 77% relative improvement in mean human-normalized scores over UniZero. These findings suggest that in partially observable RL domains with non-stationary temporal dependencies, discrete memory windows are difficult to learn reliably, whereas smooth distributional priors flexibly adapt across horizons and yield more robust data efficiency. Overall, our results demonstrate that encoding structured temporal priors directly into self-attention improves the prioritization of informative histories for dynamics modeling under partial observability.",
    "authors": [
      "Daniel De Dios Allegue",
      "Jinke He",
      "Frans A. Oliehoek"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06946v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06946v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.07332v1",
    "title": "Grounding Computer Use Agents on Human Demonstrations",
    "summary": "Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.",
    "authors": [
      "Aarash Feizi",
      "Shravan Nayak",
      "Xiangru Jian",
      "Kevin Qinghong Lin",
      "Kaixin Li",
      "Rabiul Awal",
      "Xing Han Lù",
      "Johan Obando-Ceron",
      "Juan A. Rodriguez",
      "Nicolas Chapados",
      "David Vazquez",
      "Adriana Romero-Soriano",
      "Reihaneh Rabbany",
      "Perouz Taslakian",
      "Christopher Pal",
      "Spandana Gella",
      "Sai Rajeswar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07332v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07332v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.07110v1",
    "title": "Two Heads are Better than One: Distilling Large Language Model Features   Into Small Models with Feature Decomposition and Mixture",
    "summary": "Market making (MM) through Reinforcement Learning (RL) has attracted significant attention in financial trading. With the development of Large Language Models (LLMs), more and more attempts are being made to apply LLMs to financial areas. A simple, direct application of LLM as an agent shows significant performance. Such methods are hindered by their slow inference speed, while most of the current research has not studied LLM distillation for this specific task. To address this, we first propose the normalized fluorescent probe to study the mechanism of the LLM's feature. Based on the observation found by our investigation, we propose Cooperative Market Making (CMM), a novel framework that decouples LLM features across three orthogonal dimensions: layer, task, and data. Various student models collaboratively learn simple LLM features along with different dimensions, with each model responsible for a distinct feature to achieve knowledge distillation. Furthermore, CMM introduces an H\\'{a}jek-MoE to integrate the output of the student models by investigating the contribution of different models in a kernel function-generated common feature space. Extensive experimental results on four real-world market datasets demonstrate the superiority of CMM over the current distillation method and RL-based market-making strategies.",
    "authors": [
      "Tianhao Fu",
      "Xinxin Xu",
      "Weichen Xu",
      "Jue Chen",
      "Ruilong Ren",
      "Bowen Deng",
      "Xinyu Zhao",
      "Jian Cao",
      "Xixin Cao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07110v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07110v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.07099v1",
    "title": "E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End   Speech Synthesis",
    "summary": "Recent advancements in speech synthesis technology have enriched our daily lives, with high-quality and human-like audio widely adopted across real-world applications. However, malicious exploitation like voice-cloning fraud poses severe security risks. Existing defense techniques struggle to address the production large language model (LLM)-based speech synthesis. While previous studies have considered the protection for fine-tuning synthesizers, they assume manually annotated transcripts. Given the labor intensity of manual annotation, end-to-end (E2E) systems leveraging automatic speech recognition (ASR) to generate transcripts are becoming increasingly prevalent, e.g., voice cloning via commercial APIs. Therefore, this E2E speech synthesis also requires new security mechanisms. To tackle these challenges, we propose E2E-VGuard, a proactive defense framework for two emerging threats: (1) production LLM-based speech synthesis, and (2) the novel attack arising from ASR-driven E2E scenarios. Specifically, we employ the encoder ensemble with a feature extractor to protect timbre, while ASR-targeted adversarial examples disrupt pronunciation. Moreover, we incorporate the psychoacoustic model to ensure perturbative imperceptibility. For a comprehensive evaluation, we test 16 open-source synthesizers and 3 commercial APIs across Chinese and English datasets, confirming E2E-VGuard's effectiveness in timbre and pronunciation protection. Real-world deployment validation is also conducted. Our code and demo page are available at https://wxzyd123.github.io/e2e-vguard/.",
    "authors": [
      "Zhisheng Zhang",
      "Derui Wang",
      "Yifan Mi",
      "Zhiyong Wu",
      "Jie Gao",
      "Yuxin Cao",
      "Kai Ye",
      "Minhui Xue",
      "Jie Hao"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07099v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07099v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.07078v1",
    "title": "LeCoT: revisiting network architecture for two-view correspondence   pruning",
    "summary": "Two-view correspondence pruning aims to accurately remove incorrect correspondences (outliers) from initial ones and is widely applied to various computer vision tasks. Current popular strategies adopt multilayer perceptron (MLP) as the backbone, supplemented by additional modules to enhance the network ability to handle context information, which is a known limitation of MLPs. In contrast, we introduce a novel perspective for capturing correspondence context information without extra design modules. To this end, we design a two-view correspondence pruning network called LeCoT, which can naturally leverage global context information at different stages. Specifically, the core design of LeCoT is the Spatial-Channel Fusion Transformer block, a newly proposed component that efficiently utilizes both spatial and channel global context information among sparse correspondences. In addition, we integrate the proposed prediction block that utilizes correspondence features from intermediate stages to generate a probability set, which acts as guiding information for subsequent learning phases, allowing the network to more effectively capture robust global context information. Notably, this prediction block progressively refines the probability set, thereby mitigating the issue of information loss that is common in the traditional one. Extensive experiments prove that the proposed LeCoT outperforms state-of-the-art methods in correspondence pruning, relative pose estimation, homography estimation, visual localization, and $3$D~reconstruction tasks. The code is provided in https://github.com/Dailuanyuan2024/LeCoT-Revisiting-Network-Architecture-for-Two-View-Correspondence-Pruning.",
    "authors": [
      "Luanyuan Dai",
      "Xiaoyu Du",
      "Jinhui Tang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07078v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07078v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.07051v1",
    "title": "Improving Deepfake Detection with Reinforcement Learning-Based Adaptive   Data Augmentation",
    "summary": "The generalization capability of deepfake detectors is critical for real-world use. Data augmentation via synthetic fake face generation effectively enhances generalization, yet current SoTA methods rely on fixed strategies-raising a key question: Is a single static augmentation sufficient, or does the diversity of forgery features demand dynamic approaches? We argue existing methods overlook the evolving complexity of real-world forgeries (e.g., facial warping, expression manipulation), which fixed policies cannot fully simulate. To address this, we propose CRDA (Curriculum Reinforcement-Learning Data Augmentation), a novel framework guiding detectors to progressively master multi-domain forgery features from simple to complex. CRDA synthesizes augmented samples via a configurable pool of forgery operations and dynamically generates adversarial samples tailored to the detector's current learning state. Central to our approach is integrating reinforcement learning (RL) and causal inference. An RL agent dynamically selects augmentation actions based on detector performance to efficiently explore the vast augmentation space, adapting to increasingly challenging forgeries. Simultaneously, the agent introduces action space variations to generate heterogeneous forgery patterns, guided by causal inference to mitigate spurious correlations-suppressing task-irrelevant biases and focusing on causally invariant features. This integration ensures robust generalization by decoupling synthetic augmentation patterns from the model's learned representations. Extensive experiments show our method significantly improves detector generalizability, outperforming SOTA methods across multiple cross-domain datasets.",
    "authors": [
      "Yuxuan Zhou",
      "Tao Yu",
      "Wen Huang",
      "Yuheng Zhang",
      "Tao Dai",
      "Shu-Tao Xia"
    ],
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07051v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07051v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.06978v1",
    "title": "Fast Bayesian Updates via Harmonic Representations",
    "summary": "Bayesian inference, while foundational to probabilistic reasoning, is often hampered by the computational intractability of posterior distributions, particularly through the challenging evidence integral. Conventional approaches like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face significant scalability and efficiency limitations. This paper introduces a novel, unifying framework for fast Bayesian updates by leveraging harmonic analysis. We demonstrate that representing the prior and likelihood in a suitable orthogonal basis transforms the Bayesian update rule into a spectral convolution. Specifically, the Fourier coefficients of the posterior are shown to be the normalized convolution of the prior and likelihood coefficients. To achieve computational feasibility, we introduce a spectral truncation scheme, which, for smooth functions, yields an exceptionally accurate finite-dimensional approximation and reduces the update to a circular convolution. This formulation allows us to exploit the Fast Fourier Transform (FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a substantial improvement over the O(N^2) cost of naive methods. We establish rigorous mathematical criteria for the applicability of our method, linking its efficiency to the smoothness and spectral decay of the involved distributions. The presented work offers a paradigm shift, connecting Bayesian computation to signal processing and opening avenues for real-time, sequential inference in a wide class of problems.",
    "authors": [
      "Di Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "cs.NA",
      "math.IT",
      "math.NA",
      "math.ST",
      "stat.TH",
      "65T50, 62F15, 65C60, 42A85",
      "G.3; I.2.6; G.1.2; E.4"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06978v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06978v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.07419v1",
    "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts   LLMs",
    "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large language models since it can efficiently scale up the model capability without increasing the inference cost. However, evaluations on broad downstream tasks reveal a consistent suboptimality of the routers in existing MoE LLMs, which results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal routing. In this paper, we show that aligning the manifold of routing weights with that of task embedding can effectively reduce the gap and improve MoE LLMs' generalization performance. Our method, \"Routing Manifold Alignment (RoMA)\", introduces an additional manifold regularization term in the post-training objective and only requires lightweight finetuning of routers (with other parameters frozen). Specifically, the regularization encourages the routing weights of each sample to be close to those of its successful neighbors (whose routing weights lead to correct answers) in a task embedding space. Consequently, samples targeting similar tasks will share similar expert choices across layers. Building such bindings between tasks and experts over different samples is essential to achieve better generalization. Moreover, RoMA demonstrates the advantage of unifying the task understanding (by embedding models) with solution generation (by MoE LLMs). In experiments, we finetune routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse benchmarks and extensive comparisons with baselines show the substantial improvement brought by RoMA.",
    "authors": [
      "Zhongyang Li",
      "Ziyue Li",
      "Tianyi Zhou"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07419v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07419v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07380v1",
    "title": "Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource   Domains",
    "summary": "Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \\textbf{\\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \\textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \\textbf{gains of +8.7 and +5.1 points}, which corresponds to a \\textbf{10.9x and 5.7x improvement} over the domain-only setting.",
    "authors": [
      "Pingjie Wang",
      "Hongcheng Liu",
      "Yusheng Liao",
      "Ziqing Fan",
      "Yaxin Du",
      "Shuo Tang",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07380v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07380v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07367v1",
    "title": "Machine-Learning Accelerated Calculations of Reduced Density Matrices",
    "summary": "$n$-particle reduced density matrices ($n$-RDMs) play a central role in understanding correlated phases of matter. Yet the calculation of $n$-RDMs is often computationally inefficient for strongly-correlated states, particularly when the system sizes are large. In this work, we propose to use neural network (NN) architectures to accelerate the calculation of, and even predict, the $n$-RDMs for large-size systems. The underlying intuition is that $n$-RDMs are often smooth functions over the Brillouin zone (BZ) (certainly true for gapped states) and are thus interpolable, allowing NNs trained on small-size $n$-RDMs to predict large-size ones. Building on this intuition, we devise two NNs: (i) a self-attention NN that maps random RDMs to physical ones, and (ii) a Sinusoidal Representation Network (SIREN) that directly maps momentum-space coordinates to RDM values. We test the NNs in three 2D models: the pair-pair correlation functions of the Richardson model of superconductivity, the translationally-invariant 1-RDM in a four-band model with short-range repulsion, and the translation-breaking 1-RDM in the half-filled Hubbard model. We find that a SIREN trained on a $6\\times 6$ momentum mesh can predict the $18\\times 18$ pair-pair correlation function with a relative accuracy of $0.839$. The NNs trained on $6\\times 6 \\sim 8\\times 8$ meshes can provide high-quality initial guesses for $50\\times 50$ translation-invariant Hartree-Fock (HF) and $30\\times 30$ fully translation-breaking-allowed HF, reducing the number of iterations required for convergence by up to $91.63\\%$ and $92.78\\%$, respectively, compared to random initializations. Our results illustrate the potential of using NN-based methods for interpolable $n$-RDMs, which might open a new avenue for future research on strongly correlated phases.",
    "authors": [
      "Awwab A. Azam",
      "Lexu Zhao",
      "Jiabin Yu"
    ],
    "categories": [
      "cond-mat.str-el",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07367v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07367v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07364v1",
    "title": "Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence   Estimation for Failure Detection",
    "summary": "Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.",
    "authors": [
      "Vaibhav Mavi",
      "Shubh Jaroria",
      "Weiqi Sun"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07364v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07364v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07286v1",
    "title": "Glioma C6: A Novel Dataset for Training and Benchmarking Cell   Segmentation",
    "summary": "We present Glioma C6, a new open dataset for instance segmentation of glioma C6 cells, designed as both a benchmark and a training resource for deep learning models. The dataset comprises 75 high-resolution phase-contrast microscopy images with over 12,000 annotated cells, providing a realistic testbed for biomedical image analysis. It includes soma annotations and morphological cell categorization provided by biologists. Additional categorization of cells, based on morphology, aims to enhance the utilization of image data for cancer cell research. Glioma C6 consists of two parts: the first is curated with controlled parameters for benchmarking, while the second supports generalization testing under varying conditions. We evaluate the performance of several generalist segmentation models, highlighting their limitations on our dataset. Our experiments demonstrate that training on Glioma C6 significantly enhances segmentation performance, reinforcing its value for developing robust and generalizable models. The dataset is publicly available for researchers.",
    "authors": [
      "Roman Malashin",
      "Svetlana Pashkevich",
      "Daniil Ilyukhin",
      "Arseniy Volkov",
      "Valeria Yachnaya",
      "Andrey Denisov",
      "Maria Mikhalkova"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07286v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07286v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07244v1",
    "title": "A Fully Polynomial-Time Algorithm for Robustly Learning Halfspaces over   the Hypercube",
    "summary": "We give the first fully polynomial-time algorithm for learning halfspaces with respect to the uniform distribution on the hypercube in the presence of contamination, where an adversary may corrupt some fraction of examples and labels arbitrarily. We achieve an error guarantee of $\\eta^{O(1)}+\\epsilon$ where $\\eta$ is the noise rate. Such a result was not known even in the agnostic setting, where only labels can be adversarially corrupted. All prior work over the last two decades has a superpolynomial dependence in $1/\\epsilon$ or succeeds only with respect to continuous marginals (such as log-concave densities).   Previous analyses rely heavily on various structural properties of continuous distributions such as anti-concentration. Our approach avoids these requirements and makes use of a new algorithm for learning Generalized Linear Models (GLMs) with only a polylogarithmic dependence on the activation function's Lipschitz constant. More generally, our framework shows that supervised learning with respect to discrete distributions is not as difficult as previously thought.",
    "authors": [
      "Gautam Chandrasekaran",
      "Adam R. Klivans",
      "Konstantinos Stavropoulos",
      "Arsen Vasilyan"
    ],
    "categories": [
      "cs.DS",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07244v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07244v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07210v1",
    "title": "Breaking the Stealth-Potency Trade-off in Clean-Image Backdoors with   Generative Trigger Optimization",
    "summary": "Clean-image backdoor attacks, which use only label manipulation in training datasets to compromise deep neural networks, pose a significant threat to security-critical applications. A critical flaw in existing methods is that the poison rate required for a successful attack induces a proportional, and thus noticeable, drop in Clean Accuracy (CA), undermining their stealthiness. This paper presents a new paradigm for clean-image attacks that minimizes this accuracy degradation by optimizing the trigger itself. We introduce Generative Clean-Image Backdoors (GCB), a framework that uses a conditional InfoGAN to identify naturally occurring image features that can serve as potent and stealthy triggers. By ensuring these triggers are easily separable from benign task-related features, GCB enables a victim model to learn the backdoor from an extremely small set of poisoned examples, resulting in a CA drop of less than 1%. Our experiments demonstrate GCB's remarkable versatility, successfully adapting to six datasets, five architectures, and four tasks, including the first demonstration of clean-image backdoors in regression and segmentation. GCB also exhibits resilience against most of the existing backdoor defenses.",
    "authors": [
      "Binyan Xu",
      "Fan Yang",
      "Di Tang",
      "Xilin Dai",
      "Kehuan Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG",
      "68T07",
      "I.2.6"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07210v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07210v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07087v1",
    "title": "Direct Molecular Polarizability Prediction with SO(3) Equivariant Local   Frame GNNs",
    "summary": "We introduce a novel equivariant graph neural network (GNN) architecture designed to predict the tensorial response properties of molecules. Unlike traditional frameworks that focus on regressing scalar quantities and derive tensorial properties from their derivatives, our approach maintains $SO(3)$-equivariance through the use of local coordinate frames. Our GNN effectively captures geometric information by integrating scalar, vector, and tensor channels within a local message-passing framework. To assess the accuracy of our model, we apply it to predict the polarizabilities of molecules in the QM7-X dataset and show that tensorial message passing outperforms scalar message passing models. This work marks an advancement towards developing structured, geometry-aware neural models for molecular property prediction.",
    "authors": [
      "Jean Philip Filling",
      "Felix Post",
      "Michael Wand",
      "Denis Andrienko"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07087v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07087v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07077v1",
    "title": "EmoBang: Detecting Emotion From Bengali Texts",
    "summary": "Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.",
    "authors": [
      "Abdullah Al Maruf",
      "Aditi Golder",
      "Zakaria Masud Jiyad",
      "Abdullah Al Numan",
      "Tarannum Shaila Zaman"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07077v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07077v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07047v1",
    "title": "Anatomy-Aware Lymphoma Lesion Detection in Whole-Body PET/CT",
    "summary": "Early cancer detection is crucial for improving patient outcomes, and 18F FDG PET/CT imaging plays a vital role by combining metabolic and anatomical information. Accurate lesion detection remains challenging due to the need to identify multiple lesions of varying sizes. In this study, we investigate the effect of adding anatomy prior information to deep learning-based lesion detection models. In particular, we add organ segmentation masks from the TotalSegmentator tool as auxiliary inputs to provide anatomical context to nnDetection, which is the state-of-the-art for lesion detection, and Swin Transformer. The latter is trained in two stages that combine self-supervised pre-training and supervised fine-tuning. The method is tested in the AutoPET and Karolinska lymphoma datasets. The results indicate that the inclusion of anatomical priors substantially improves the detection performance within the nnDetection framework, while it has almost no impact on the performance of the vision transformer. Moreover, we observe that Swin Transformer does not offer clear advantages over conventional convolutional neural network (CNN) encoders used in nnDetection. These findings highlight the critical role of the anatomical context in cancer lesion detection, especially in CNN-based models.",
    "authors": [
      "Simone Bendazzoli",
      "Antonios Tzortzakakis",
      "Andreas Abrahamsson",
      "Björn Engelbrekt Wahlin",
      "Örjan Smedby",
      "Maria Holstensson",
      "Rodrigo Moreno"
    ],
    "categories": [
      "eess.IV",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07047v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07047v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07032v1",
    "title": "Fair Bayesian Data Selection via Generalized Discrepancy Measures",
    "summary": "Fairness concerns are increasingly critical as machine learning models are deployed in high-stakes applications. While existing fairness-aware methods typically intervene at the model level, they often suffer from high computational costs, limited scalability, and poor generalization. To address these challenges, we propose a Bayesian data selection framework that ensures fairness by aligning group-specific posterior distributions of model parameters and sample weights with a shared central distribution. Our framework supports flexible alignment via various distributional discrepancy measures, including Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing geometry-aware control without imposing explicit fairness constraints. This data-centric approach mitigates group-specific biases in training data and improves fairness in downstream tasks, with theoretical guarantees. Experiments on benchmark datasets show that our method consistently outperforms existing data selection and model-based fairness methods in both fairness and accuracy.",
    "authors": [
      "Yixuan Zhang",
      "Jiabin Luo",
      "Zhenggang Wang",
      "Feng Zhou",
      "Quyu Kong"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07032v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07032v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.07417v1",
    "title": "Language Generation with Infinite Contamination",
    "summary": "We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.   Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).   We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.   Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.",
    "authors": [
      "Anay Mehrotra",
      "Grigoris Velegkas",
      "Xifan Yu",
      "Felix Zhou"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.DS",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07417v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07417v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07274v1",
    "title": "Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering",
    "summary": "Multiple clustering aims to discover diverse latent structures from different perspectives, yet existing methods generate exhaustive clusterings without discerning user interest, necessitating laborious manual screening. Current multi-modal solutions suffer from static semantic rigidity: predefined candidate words fail to adapt to dataset-specific concepts, and fixed fusion strategies ignore evolving feature interactions. To overcome these limitations, we propose Multi-DProxy, a novel multi-modal dynamic proxy learning framework that leverages cross-modal alignment through learnable textual proxies. Multi-DProxy introduces 1) gated cross-modal fusion that synthesizes discriminative joint representations by adaptively modeling feature interactions. 2) dual-constraint proxy optimization where user interest constraints enforce semantic consistency with domain concepts while concept constraints employ hard example mining to enhance cluster discrimination. 3) dynamic candidate management that refines textual proxies through iterative clustering feedback. Therefore, Multi-DProxy not only effectively captures a user's interest through proxies but also enables the identification of relevant clusterings with greater precision. Extensive experiments demonstrate state-of-the-art performance with significant improvements over existing methods across a broad set of multi-clustering benchmarks.",
    "authors": [
      "Jinfeng Xu",
      "Zheyu Chen",
      "Shuo Yang",
      "Jinze Li",
      "Ziyue Peng",
      "Zewei Liu",
      "Hewei Wang",
      "Jiayi Zhang",
      "Edith C. H. Ngai"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07274v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07274v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07260v1",
    "title": "PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork",
    "summary": "Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen teammates, which is crucial for many real-world applications. The core challenge of AHT is to develop an ego agent that can predict and adapt to unknown teammates on the fly. Conventional RL-based approaches optimize a single expected return, which often causes policies to collapse into a single dominant behavior, thus failing to capture the multimodal cooperation patterns inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach that captures agent's multimodal behaviors, unlocking its diverse cooperation modes with teammates. However, standard diffusion models lack the ability to predict and adapt in highly non-stationary AHT scenarios. To address this limitation, we propose a novel diffusion-based policy that integrates critical predictive information about teammates into the denoising process. Extensive experiments across three cooperation environments demonstrate that PADiff outperforms existing AHT methods significantly.",
    "authors": [
      "Hohei Chan",
      "Xinzhi Zhang",
      "Antao Xiang",
      "Weinan Zhang",
      "Mengchen Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07260v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07260v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07250v1",
    "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal   LLMs",
    "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI capabilities to visual modalities, yet existing evaluation benchmarks remain limited to single-video understanding, overlooking the critical need for multi-video understanding in real-world scenarios (e.g., sports analytics and autonomous driving). To address this significant gap, we introduce MVU-Eval, the first comprehensive benchmark for evaluating Multi-Video Understanding for MLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies through 1,824 meticulously curated question-answer pairs spanning 4,959 videos from diverse domains, addressing both fundamental perception tasks and high-order reasoning tasks. These capabilities are rigorously aligned with real-world applications such as multi-sensor synthesis in autonomous systems and cross-angle sports analytics. Through extensive evaluation of state-of-the-art open-source and closed-source models, we reveal significant performance discrepancies and limitations in current MLLMs' ability to perform understanding across multiple videos. The benchmark will be made publicly available to foster future research.",
    "authors": [
      "Tianhao Peng",
      "Haochen Wang",
      "Yuanxing Zhang",
      "Zekun Wang",
      "Zili Wang",
      "Ge Zhang",
      "Jian Yang",
      "Shihao Li",
      "Yanghai Wang",
      "Xintao Wang",
      "Houyi Li",
      "Wei Ji",
      "Pengfei Wan",
      "Wenhao Huang",
      "Zhaoxiang Zhang",
      "Jiaheng Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07250v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07250v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07237v1",
    "title": "The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series   Models",
    "summary": "Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \\textit{\\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \\emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \\textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\\% of the parameters achieves up to a 12\\% accuracy improvement and a 2.7$\\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\\% of layers achieves comparable or superior accuracy in over 95\\% of tasks.",
    "authors": [
      "Xin Qiu",
      "Junlong Tong",
      "Yirong Sun",
      "Yunpu Ma",
      "Xiaoyu Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07237v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07237v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07229v1",
    "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and   Serving Techniques in LLM Infrastructure",
    "summary": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.",
    "authors": [
      "Jaehong Cho",
      "Hyunmin Choi",
      "Jongse Park"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07229v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07129v1",
    "title": "LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging",
    "summary": "Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.",
    "authors": [
      "Seungeon Lee",
      "Soumi Das",
      "Manish Gupta",
      "Krishna P. Gummadi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07129v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07129v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07124v1",
    "title": "Think Consistently, Reason Efficiently: Energy-Based Calibration for   Implicit Chain-of-Thought",
    "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \\emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.",
    "authors": [
      "Zhikang Chen",
      "Sen Cui",
      "Deheng Ye",
      "Yu Zhang",
      "Yatao Bian",
      "Tingting Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07124v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07124v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07107v1",
    "title": "MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering   and Mitigating Implicit Risks in LLMs on Domain Tasks",
    "summary": "Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.",
    "authors": [
      "Liang Shan",
      "Kaicheng Shen",
      "Wen Wu",
      "Zhenyu Ying",
      "Chaochao Lu",
      "Guangze Ye",
      "Liang He"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07107v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07107v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07084v1",
    "title": "Pandar128 dataset for lane line detection",
    "summary": "We present Pandar128, the largest public dataset for lane line detection using a 128-beam LiDAR. It contains over 52,000 camera frames and 34,000 LiDAR scans, captured in diverse real-world conditions in Germany. The dataset includes full sensor calibration (intrinsics, extrinsics) and synchronized odometry, supporting tasks such as projection, fusion, and temporal modeling.   To complement the dataset, we also introduce SimpleLidarLane, a light-weight baseline method for lane line reconstruction that combines BEV segmentation, clustering, and polyline fitting. Despite its simplicity, our method achieves strong performance under challenging various conditions (e.g., rain, sparse returns), showing that modular pipelines paired with high-quality data and principled evaluation can compete with more complex approaches.   Furthermore, to address the lack of standardized evaluation, we propose a novel polyline-based metric - Interpolation-Aware Matching F1 (IAM-F1) - that employs interpolation-aware lateral matching in BEV space.   All data and code are publicly released to support reproducibility in LiDAR-based lane detection.",
    "authors": [
      "Filip Beránek",
      "Václav Diviš",
      "Ivan Gruber"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07084v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07084v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07074v1",
    "title": "Importance-Aware Data Selection for Efficient LLM Instruction Tuning",
    "summary": "Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.",
    "authors": [
      "Tingyu Jiang",
      "Shen Li",
      "Yiyao Song",
      "Lan Zhang",
      "Hualei Zhu",
      "Yuan Zhao",
      "Xiaohang Xu",
      "Kenjiro Taura",
      "Hao Henry Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07074v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07074v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07057v1",
    "title": "TauFlow: Dynamic Causal Constraint for Complexity-Adaptive Lightweight   Segmentation",
    "summary": "Deploying lightweight medical image segmentation models on edge devices presents two major challenges: 1) efficiently handling the stark contrast between lesion boundaries and background regions, and 2) the sharp drop in accuracy that occurs when pursuing extremely lightweight designs (e.g., <0.5M parameters). To address these problems, this paper proposes TauFlow, a novel lightweight segmentation model. The core of TauFlow is a dynamic feature response strategy inspired by brain-like mechanisms. This is achieved through two key innovations: the Convolutional Long-Time Constant Cell (ConvLTC), which dynamically regulates the feature update rate to \"slowly\" process low-frequency backgrounds and \"quickly\" respond to high-frequency boundaries; and the STDP Self-Organizing Module, which significantly mitigates feature conflicts between the encoder and decoder, reducing the conflict rate from approximately 35%-40% to 8%-10%.",
    "authors": [
      "Zidong Chen",
      "Fadratul Hafinaz Hassan"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "68U10, 68T45, 92C55, 68T07",
      "I.4.6; I.2.10; J.3; I.2.6"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07057v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07057v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07010v1",
    "title": "A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided   Judge-Corrector System for Multimodal Machine Translation",
    "summary": "In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.   Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.   We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).",
    "authors": [
      "Siddharth Betala",
      "Kushan Raj",
      "Vipul Betala",
      "Rohan Saswade"
    ],
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07010v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07010v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.06947v1",
    "title": "FoCLIP: A Feature-Space Misalignment Framework for CLIP-Based Image   Manipulation and Detection",
    "summary": "The well-aligned attribute of CLIP-based models enables its effective application like CLIPscore as a widely adopted image quality assessment metric. However, such a CLIP-based metric is vulnerable for its delicate multimodal alignment. In this work, we propose \\textbf{FoCLIP}, a feature-space misalignment framework for fooling CLIP-based image quality metric. Based on the stochastic gradient descent technique, FoCLIP integrates three key components to construct fooling examples: feature alignment as the core module to reduce image-text modality gaps, the score distribution balance module and pixel-guard regularization, which collectively optimize multimodal output equilibrium between CLIPscore performance and image quality. Such a design can be engineered to maximize the CLIPscore predictions across diverse input prompts, despite exhibiting either visual unrecognizability or semantic incongruence with the corresponding adversarial prompts from human perceptual perspectives. Experiments on ten artistic masterpiece prompts and ImageNet subsets demonstrate that optimized images can achieve significant improvement in CLIPscore while preserving high visual fidelity. In addition, we found that grayscale conversion induces significant feature degradation in fooling images, exhibiting noticeable CLIPscore reduction while preserving statistical consistency with original images. Inspired by this phenomenon, we propose a color channel sensitivity-driven tampering detection mechanism that achieves 91% accuracy on standard benchmarks. In conclusion, this work establishes a practical pathway for feature misalignment in CLIP-based multimodal systems and the corresponding defense method.",
    "authors": [
      "Yulin Chen",
      "Zeyuan Wang",
      "Tianyuan Yu",
      "Yingmei Wei",
      "Liang Bai"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06947v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06947v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.06937v1",
    "title": "Fine-Tuning Diffusion-Based Recommender Systems via Reinforcement   Learning with Reward Function Optimization",
    "summary": "Diffusion models recently emerged as a powerful paradigm for recommender systems, offering state-of-the-art performance by modeling the generative process of user-item interactions. However, training such models from scratch is both computationally expensive and yields diminishing returns once convergence is reached. To remedy these challenges, we propose ReFiT, a new framework that integrates Reinforcement learning (RL)-based Fine-Tuning into diffusion-based recommender systems. In contrast to prior RL approaches for diffusion models depending on external reward models, ReFiT adopts a task-aligned design: it formulates the denoising trajectory as a Markov decision process (MDP) and incorporates a collaborative signal-aware reward function that directly reflects recommendation quality. By tightly coupling the MDP structure with this reward signal, ReFiT empowers the RL agent to exploit high-order connectivity for fine-grained optimization, while avoiding the noisy or uninformative feedback common in naive reward designs. Leveraging policy gradient optimization, ReFiT maximizes exact log-likelihood of observed interactions, thereby enabling effective post hoc fine-tuning of diffusion recommenders. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed ReFiT framework (a) exhibits substantial performance gains over strong competitors (up to 36.3% on sequential recommendation), (b) demonstrates strong efficiency with linear complexity in the number of users or items, and (c) generalizes well across multiple diffusion-based recommendation scenarios. The source code and datasets are publicly available at https://anonymous.4open.science/r/ReFiT-4C60.",
    "authors": [
      "Yu Hou",
      "Hua Li",
      "Ha Young Kim",
      "Won-Yong Shin"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG",
      "cs.NI",
      "cs.SI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06937v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06937v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.07405v1",
    "title": "SPOT: An Annotated French Corpus and Benchmark for Detecting Critical   Interventions in Online Conversations",
    "summary": "We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.",
    "authors": [
      "Manon Berriche",
      "Célia Nouri",
      "Chloé Clavel",
      "Jean-Philippe Cointet"
    ],
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07405v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07405v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.07155v1",
    "title": "Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in   Reinforcement Learning for Autonomous Driving",
    "summary": "Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.",
    "authors": [
      "Thomas Steinecker",
      "Alexander Bienemann",
      "Denis Trescher",
      "Thorsten Luettel",
      "Mirko Maehlisch"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07155v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07155v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.06899v1",
    "title": "RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal   Evaluation",
    "summary": "Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.",
    "authors": [
      "Haofeng Wang",
      "Yu Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06899v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06899v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.07416v1",
    "title": "Robot Learning from a Physical World Model",
    "summary": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.",
    "authors": [
      "Jiageng Mao",
      "Sicheng He",
      "Hao-Ning Wu",
      "Yang You",
      "Shuyang Sun",
      "Zhicheng Wang",
      "Yanan Bao",
      "Huizhong Chen",
      "Leonidas Guibas",
      "Vitor Guizilini",
      "Howard Zhou",
      "Yue Wang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07416v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07416v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07403v1",
    "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial   Rewards",
    "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.",
    "authors": [
      "Hunar Batra",
      "Haoqin Tu",
      "Hardy Chen",
      "Yuanze Lin",
      "Cihang Xie",
      "Ronald Clark"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07403v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07403v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07281v1",
    "title": "Segmentation of Ischemic Stroke Lesions using Transfer Learning on   Multi-sequence MRI",
    "summary": "The accurate understanding of ischemic stroke lesions is critical for efficient therapy and prognosis of stroke patients. Magnetic resonance imaging (MRI) is sensitive to acute ischemic stroke and is a common diagnostic method for stroke. However, manual lesion segmentation performed by experts is tedious, time-consuming, and prone to observer inconsistency. Automatic medical image analysis methods have been proposed to overcome this challenge. However, previous approaches have relied on hand-crafted features that may not capture the irregular and physiologically complex shapes of ischemic stroke lesions. In this study, we present a novel framework for quickly and automatically segmenting ischemic stroke lesions on various MRI sequences, including T1-weighted, T2-weighted, DWI, and FLAIR. The proposed methodology is validated on the ISLES 2015 Brain Stroke sequence dataset, where we trained our model using the Res-Unet architecture twice: first, with pre-existing weights, and then without, to explore the benefits of transfer learning. Evaluation metrics, including the Dice score and sensitivity, were computed across 3D volumes. Finally, a Majority Voting Classifier was integrated to amalgamate the outcomes from each axis, resulting in a comprehensive segmentation method. Our efforts culminated in achieving a Dice score of 80.5\\% and an accuracy of 74.03\\%, showcasing the efficacy of our segmentation approach.",
    "authors": [
      "R. P. Chowdhury",
      "T. Rahman"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07281v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07281v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07148v1",
    "title": "TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for   Traditional Chinese Medicine",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.",
    "authors": [
      "Zihao Cheng",
      "Yuheng Lu",
      "Huaiqian Ye",
      "Zeming Liu",
      "Minqi Wang",
      "Jingjing Liu",
      "Zihan Li",
      "Wei Fan",
      "Yuanfang Guo",
      "Ruiji Fu",
      "Shifeng She",
      "Gang Wang",
      "Yunhong Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07148v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07148v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07104v1",
    "title": "A Theoretical Analysis of Detecting Large Model-Generated Time Series",
    "summary": "Motivated by the increasing risks of data misuse and fabrication, we investigate the problem of identifying synthetic time series generated by Time-Series Large Models (TSLMs) in this work. While there are extensive researches on detecting model generated text, we find that these existing methods are not applicable to time series data due to the fundamental modality difference, as time series usually have lower information density and smoother probability distributions than text data, which limit the discriminative power of token-based detectors. To address this issue, we examine the subtle distributional differences between real and model-generated time series and propose the contraction hypothesis, which states that model-generated time series, unlike real ones, exhibit progressively decreasing uncertainty under recursive forecasting. We formally prove this hypothesis under theoretical assumptions on model behavior and time series structure. Model-generated time series exhibit progressively concentrated distributions under recursive forecasting, leading to uncertainty contraction. We provide empirical validation of the hypothesis across diverse datasets. Building on this insight, we introduce the Uncertainty Contraction Estimator (UCE), a white-box detector that aggregates uncertainty metrics over successive prefixes to identify TSLM-generated time series. Extensive experiments on 32 datasets show that UCE consistently outperforms state-of-the-art baselines, offering a reliable and generalizable solution for detecting model-generated time series.",
    "authors": [
      "Junji Hou",
      "Junzhou Zhao",
      "Shuo Zhang",
      "Pinghui Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07104v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07104v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07073v1",
    "title": "Breaking Privacy in Federated Clustering: Perfect Input Reconstruction   via Temporal Correlations",
    "summary": "Federated clustering allows multiple parties to discover patterns in distributed data without sharing raw samples. To reduce overhead, many protocols disclose intermediate centroids during training. While often treated as harmless for efficiency, whether such disclosure compromises privacy remains an open question. Prior analyses modeled the problem as a so-called Hidden Subset Sum Problem (HSSP) and argued that centroid release may be safe, since classical HSSP attacks fail to recover inputs.   We revisit this question and uncover a new leakage mechanism: temporal regularities in $k$-means iterations create exploitable structure that enables perfect input reconstruction. Building on this insight, we propose Trajectory-Aware Reconstruction (TAR), an attack that combines temporal assignment information with algebraic analysis to recover exact original inputs. Our findings provide the first rigorous evidence, supported by a practical attack, that centroid disclosure in federated clustering significantly compromises privacy, exposing a fundamental tension between privacy and efficiency.",
    "authors": [
      "Guang Yang",
      "Lixia Luo",
      "Qiongxiu Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07073v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07073v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07023v1",
    "title": "Correcting False Alarms from Unseen: Adapting Graph Anomaly Detectors at   Test Time",
    "summary": "Graph anomaly detection (GAD), which aims to detect outliers in graph-structured data, has received increasing research attention recently. However, existing GAD methods assume identical training and testing distributions, which is rarely valid in practice. In real-world scenarios, unseen but normal samples may emerge during deployment, leading to a normality shift that degrades the performance of GAD models trained on the original data. Through empirical analysis, we reveal that the degradation arises from (1) semantic confusion, where unseen normal samples are misinterpreted as anomalies due to their novel patterns, and (2) aggregation contamination, where the representations of seen normal nodes are distorted by unseen normals through message aggregation. While retraining or fine-tuning GAD models could be a potential solution to the above challenges, the high cost of model retraining and the difficulty of obtaining labeled data often render this approach impractical in real-world applications. To bridge the gap, we proposed a lightweight and plug-and-play Test-time adaptation framework for correcting Unseen Normal pattErns (TUNE) in GAD. To address semantic confusion, a graph aligner is employed to align the shifted data to the original one at the graph attribute level. Moreover, we utilize the minimization of representation-level shift as a supervision signal to train the aligner, which leverages the estimated aggregation contamination as a key indicator of normality shift. Extensive experiments on 10 real-world datasets demonstrate that TUNE significantly enhances the generalizability of pre-trained GAD models to both synthetic and real unseen normal patterns.",
    "authors": [
      "Junjun Pan",
      "Yixin Liu",
      "Chuan Zhou",
      "Fei Xiong",
      "Alan Wee-Chung Liew",
      "Shirui Pan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07023v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07023v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07014v1",
    "title": "Diffolio: A Diffusion Model for Multivariate Probabilistic Financial   Time-Series Forecasting and Portfolio Construction",
    "summary": "Probabilistic forecasting is crucial in multivariate financial time-series for constructing efficient portfolios that account for complex cross-sectional dependencies. In this paper, we propose Diffolio, a diffusion model designed for multivariate financial time-series forecasting and portfolio construction. Diffolio employs a denoising network with a hierarchical attention architecture, comprising both asset-level and market-level layers. Furthermore, to better reflect cross-sectional correlations, we introduce a correlation-guided regularizer informed by a stable estimate of the target correlation matrix. This structure effectively extracts salient features not only from historical returns but also from asset-specific and systematic covariates, significantly enhancing the performance of forecasts and portfolios. Experimental results on the daily excess returns of 12 industry portfolios show that Diffolio outperforms various probabilistic forecasting baselines in multivariate forecasting accuracy and portfolio performance. Moreover, in portfolio experiments, portfolios constructed from Diffolio's forecasts show consistently robust performance, thereby outperforming those from benchmarks by achieving higher Sharpe ratios for the mean-variance tangency portfolio and higher certainty equivalents for the growth-optimal portfolio. These results demonstrate the superiority of our proposed Diffolio in terms of not only statistical accuracy but also economic significance.",
    "authors": [
      "So-Yoon Cho",
      "Jin-Young Kim",
      "Kayoung Ban",
      "Hyeng Keun Koo",
      "Hyun-Gyoon Kim"
    ],
    "categories": [
      "cs.CE",
      "cs.AI",
      "econ.EM",
      "q-fin.PM"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07014v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07014v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07007v1",
    "title": "TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene   Understanding",
    "summary": "3D semantic scene understanding remains a long-standing challenge in the 3D computer vision community. One of the key issues pertains to limited real-world annotated data to facilitate generalizable models. The common practice to tackle this issue is to simulate new data. Although synthetic datasets offer scalability and perfect labels, their designer-crafted scenes fail to capture real-world complexity and sensor noise, resulting in a synthetic-to-real domain gap. Moreover, no benchmark provides synchronized real and simulated point clouds for segmentation-oriented domain shift analysis. We introduce TrueCity, the first urban semantic segmentation benchmark with cm-accurate annotated real-world point clouds, semantic 3D city models, and annotated simulated point clouds representing the same city. TrueCity proposes segmentation classes aligned with international 3D city modeling standards, enabling consistent evaluation of synthetic-to-real gap. Our extensive experiments on common baselines quantify domain shift and highlight strategies for exploiting synthetic data to enhance real-world 3D scene understanding. We are convinced that the TrueCity dataset will foster further development of sim-to-real gap quantification and enable generalizable data-driven models. The data, code, and 3D models are available online: https://tum-gis.github.io/TrueCity/",
    "authors": [
      "Duc Nguyen",
      "Yan-Ling Lai",
      "Qilin Zhang",
      "Prabin Gyawali",
      "Benedikt Schwab",
      "Olaf Wysocki",
      "Thomas H. Kolbe"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07007v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07007v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.07317v1",
    "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with   Adaptive Verifiable Environments",
    "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.",
    "authors": [
      "Zhiyuan Zeng",
      "Hamish Ivison",
      "Yiping Wang",
      "Lifan Yuan",
      "Shuyue Stella Li",
      "Zhuorui Ye",
      "Siting Li",
      "Jacqueline He",
      "Runlong Zhou",
      "Tong Chen",
      "Chenyang Zhao",
      "Yulia Tsvetkov",
      "Simon Shaolei Du",
      "Natasha Jaques",
      "Hao Peng",
      "Pang Wei Koh",
      "Hannaneh Hajishirzi"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07317v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07317v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.07137v1",
    "title": "MPJudge: Towards Perceptual Assessment of Music-Induced Paintings",
    "summary": "Music induced painting is a unique artistic practice, where visual artworks are created under the influence of music. Evaluating whether a painting faithfully reflects the music that inspired it poses a challenging perceptual assessment task. Existing methods primarily rely on emotion recognition models to assess the similarity between music and painting, but such models introduce considerable noise and overlook broader perceptual cues beyond emotion. To address these limitations, we propose a novel framework for music induced painting assessment that directly models perceptual coherence between music and visual art. We introduce MPD, the first large scale dataset of music painting pairs annotated by domain experts based on perceptual coherence. To better handle ambiguous cases, we further collect pairwise preference annotations. Building on this dataset, we present MPJudge, a model that integrates music features into a visual encoder via a modulation based fusion mechanism. To effectively learn from ambiguous cases, we adopt Direct Preference Optimization for training. Extensive experiments demonstrate that our method outperforms existing approaches. Qualitative results further show that our model more accurately identifies music relevant regions in paintings.",
    "authors": [
      "Shiqi Jiang",
      "Tianyi Liang",
      "Changbo Wang",
      "Chenhui Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07137v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07137v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.07065v1",
    "title": "Aligning Attention with Human Rationales for Self-Explaining Hate Speech   Detection",
    "summary": "The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.",
    "authors": [
      "Brage Eilertsen",
      "Røskva Bjørgfinsdóttir",
      "Francielle Vargas",
      "Ali Ramezani-Kebrya"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07065v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07065v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.06973v1",
    "title": "Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet   Template Discovery",
    "summary": "Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.",
    "authors": [
      "Ananad Krishnakumar",
      "Vengadesh Ravikumaran"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06973v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06973v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.07398v1",
    "title": "Solving bilevel optimization via sequential minimax optimization",
    "summary": "In this paper we propose a sequential minimax optimization (SMO) method for solving a class of constrained bilevel optimization problems in which the lower-level part is a possibly nonsmooth convex optimization problem, while the upper-level part is a possibly nonconvex optimization problem. Specifically, SMO applies a first-order method to solve a sequence of minimax subproblems, which are obtained by employing a hybrid of modified augmented Lagrangian and penalty schemes on the bilevel optimization problems. Under suitable assumptions, we establish an operation complexity of $O(\\varepsilon^{-7}\\log\\varepsilon^{-1})$ and $O(\\varepsilon^{-6}\\log\\varepsilon^{-1})$, measured in terms of fundamental operations, for SMO in finding an $\\varepsilon$-KKT solution of the bilevel optimization problems with merely convex and strongly convex lower-level objective functions, respectively. The latter result improves the previous best-known operation complexity by a factor of $\\varepsilon^{-1}$. Preliminary numerical results demonstrate significantly superior computational performance compared to the recently developed first-order penalty method.",
    "authors": [
      "Zhaosong Lu",
      "Sanyou Mei"
    ],
    "categories": [
      "math.OC",
      "cs.LG",
      "cs.NA",
      "math.NA",
      "stat.ML",
      "90C26, 90C30, 90C47, 90C99, 65K05"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07398v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07398v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07295v1",
    "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender   Systems via Large Language Models",
    "summary": "Implicit feedback, employed in training recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to identify noisy samples through their diverged data patterns, such as higher loss values, and mitigate their influence through sample dropping or reweighting. However, we observed that noisy samples and hard samples display similar patterns, leading to hard-noisy confusion issue. Such confusion is problematic as hard samples are vital for modeling user preferences. To solve this problem, we propose LLMHNI framework, leveraging two auxiliary user-item relevance signals generated by Large Language Models (LLMs) to differentiate hard and noisy samples. LLMHNI obtains user-item semantic relevance from LLM-encoded embeddings, which is used in negative sampling to select hard negatives while filtering out noisy false negatives. An objective alignment strategy is proposed to project LLM-encoded embeddings, originally for general language tasks, into a representation space optimized for user-item relevance modeling. LLMHNI also exploits LLM-inferred logical relevance within user-item interactions to identify hard and noisy samples. These LLM-inferred interactions are integrated into the interaction graph and guide denoising with cross-graph contrastive alignment. To eliminate the impact of unreliable interactions induced by LLM hallucination, we propose a graph contrastive learning strategy that aligns representations from randomly edge-dropped views to suppress unreliable edges. Empirical results demonstrate that LLMHNI significantly improves denoising and recommendation performance.",
    "authors": [
      "Tianrui Song",
      "Wen-Shuo Chao",
      "Hao Liu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07295v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07295v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07288v1",
    "title": "Enabling Off-Policy Imitation Learning with Deep Actor Critic   Stabilization",
    "summary": "Learning complex policies with Reinforcement Learning (RL) is often hindered by instability and slow convergence, a problem exacerbated by the difficulty of reward engineering. Imitation Learning (IL) from expert demonstrations bypasses this reliance on rewards. However, state-of-the-art IL methods, exemplified by Generative Adversarial Imitation Learning (GAIL)Ho et. al, suffer from severe sample inefficiency. This is a direct consequence of their foundational on-policy algorithms, such as TRPO Schulman et.al. In this work, we introduce an adversarial imitation learning algorithm that incorporates off-policy learning to improve sample efficiency. By combining an off-policy framework with auxiliary techniques specifically, double Q network based stabilization and value learning without reward function inference we demonstrate a reduction in the samples required to robustly match expert behavior.",
    "authors": [
      "Sayambhu Sen",
      "Shalabh Bhatnagar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07288v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07288v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07253v1",
    "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large   Language Models",
    "summary": "Large language models (LLMs) have recently achieved impressive results in speech recognition across multiple modalities, including Auditory Speech Recognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech Recognition (AVSR). Despite this progress, current LLM-based approaches typically address each task independently, training separate models that raise computational and deployment resource use while missing potential cross-task synergies. They also rely on fixed-rate token compression, which restricts flexibility in balancing accuracy with efficiency. These limitations highlight the need for a unified framework that can support ASR, VSR, and AVSR while enabling elastic inference. To this end, we present Omni-AVSR, a unified audio-visual LLM that combines efficient multi-granularity training with parameter-efficient adaptation. Specifically, we adapt the matryoshka representation learning paradigm to efficiently train across multiple audio and visual granularities, reducing its inherent training resource use. Furthermore, we explore three LoRA-based strategies for adapting the backbone LLM, balancing shared and task-specific specialization. Experiments on LRS2 and LRS3 show that Omni-AVSR achieves comparable or superior accuracy to state-of-the-art baselines while training a single model at substantially lower training and deployment resource use. The model also remains robust under acoustic noise, and we analyze its scaling behavior as LLM size increases, providing insights into the trade-off between performance and efficiency.",
    "authors": [
      "Umberto Cappellazzo",
      "Xubo Liu",
      "Pingchuan Ma",
      "Stavros Petridis",
      "Maja Pantic"
    ],
    "categories": [
      "eess.AS",
      "cs.CV",
      "cs.SD"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07253v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07253v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07165v1",
    "title": "Fuzzy Label: From Concept to Its Application in Label Learning",
    "summary": "Label learning is a fundamental task in machine learning that aims to construct intelligent models using labeled data, encompassing traditional single-label and multi-label classification models. Traditional methods typically rely on logical labels, such as binary indicators (e.g., \"yes/no\") that specify whether an instance belongs to a given category. However, in practical applications, label annotations often involve significant uncertainty due to factors such as data noise, inherent ambiguity in the observed entities, and the subjectivity of human annotators. Therefore, representing labels using simplistic binary logic can obscure valuable information and limit the expressiveness of label learning models. To overcome this limitation, this paper introduces the concept of fuzzy labels, grounded in fuzzy set theory, to better capture and represent label uncertainty. We further propose an efficient fuzzy labeling method that mines and generates fuzzy labels from the original data, thereby enriching the label space with more informative and nuanced representations. Based on this foundation, we present fuzzy-label-enhanced algorithms for both single-label and multi-label learning, using the classical K-Nearest Neighbors (KNN) and multi-label KNN algorithms as illustrative examples. Experimental results indicate that fuzzy labels can more effectively characterize the real-world labeling information and significantly enhance the performance of label learning models.",
    "authors": [
      "Chenxi Luoa",
      "Zhuangzhuang Zhaoa",
      "Zhaohong Denga",
      "Te Zhangb"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07165v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07165v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07158v1",
    "title": "Guiding Generative Models to Uncover Diverse and Novel Crystals via   Reinforcement Learning",
    "summary": "Discovering functional crystalline materials entails navigating an immense combinatorial design space. While recent advances in generative artificial intelligence have enabled the sampling of chemically plausible compositions and structures, a fundamental challenge remains: the objective misalignment between likelihood-based sampling in generative modelling and targeted focus on underexplored regions where novel compounds reside. Here, we introduce a reinforcement learning framework that guides latent denoising diffusion models toward diverse and novel, yet thermodynamically viable crystalline compounds. Our approach integrates group relative policy optimisation with verifiable, multi-objective rewards that jointly balance creativity, stability, and diversity. Beyond de novo generation, we demonstrate enhanced property-guided design that preserves chemical validity, while targeting desired functional properties. This approach establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off across scientific discovery applications of generative models.",
    "authors": [
      "Hyunsoo Park",
      "Aron Walsh"
    ],
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07158v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07158v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07142v1",
    "title": "ProcGen3D: Learning Neural Procedural Graph Representations for   Image-to-3D Reconstruction",
    "summary": "We introduce ProcGen3D, a new approach for 3D content creation by generating procedural graph abstractions of 3D objects, which can then be decoded into rich, complex 3D assets. Inspired by the prevalent use of procedural generators in production 3D applications, we propose a sequentialized, graph-based procedural graph representation for 3D assets. We use this to learn to approximate the landscape of a procedural generator for image-based 3D reconstruction. We employ edge-based tokenization to encode the procedural graphs, and train a transformer prior to predict the next token conditioned on an input RGB image. Crucially, to enable better alignment of our generated outputs to an input image, we incorporate Monte Carlo Tree Search (MCTS) guided sampling into our generation process, steering output procedural graphs towards more image-faithful reconstructions. Our approach is applicable across a variety of objects that can be synthesized with procedural generators. Extensive experiments on cacti, trees, and bridges show that our neural procedural graph generation outperforms both state-of-the-art generative 3D methods and domain-specific modeling techniques. Furthermore, this enables improved generalization on real-world input images, despite training only on synthetic data.",
    "authors": [
      "Xinyi Zhang",
      "Daoyi Gao",
      "Naiqi Li",
      "Angela Dai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07142v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07142v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07091v1",
    "title": "How Bias Binds: Measuring Hidden Associations for Bias Control in   Text-to-Image Compositions",
    "summary": "Text-to-image generative models often exhibit bias related to sensitive attributes. However, current research tends to focus narrowly on single-object prompts with limited contextual diversity. In reality, each object or attribute within a prompt can contribute to bias. For example, the prompt \"an assistant wearing a pink hat\" may reflect female-inclined biases associated with a pink hat. The neglected joint effects of the semantic binding in the prompts cause significant failures in current debiasing approaches. This work initiates a preliminary investigation on how bias manifests under semantic binding, where contextual associations between objects and attributes influence generative outcomes. We demonstrate that the underlying bias distribution can be amplified based on these associations. Therefore, we introduce a bias adherence score that quantifies how specific object-attribute bindings activate bias. To delve deeper, we develop a training-free context-bias control framework to explore how token decoupling can facilitate the debiasing of semantic bindings. This framework achieves over 10% debiasing improvement in compositional generation tasks. Our analysis of bias scores across various attribute-object bindings and token decorrelation highlights a fundamental challenge: reducing bias without disrupting essential semantic relationships. These findings expose critical limitations in current debiasing approaches when applied to semantically bound contexts, underscoring the need to reassess prevailing bias mitigation strategies.",
    "authors": [
      "Jeng-Lin Li",
      "Ming-Ching Chang",
      "Wei-Chao Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07091v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07091v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07040v1",
    "title": "3D-ANC: Adaptive Neural Collapse for Robust 3D Point Cloud Recognition",
    "summary": "Deep neural networks have recently achieved notable progress in 3D point cloud recognition, yet their vulnerability to adversarial perturbations poses critical security challenges in practical deployments. Conventional defense mechanisms struggle to address the evolving landscape of multifaceted attack patterns. Through systematic analysis of existing defenses, we identify that their unsatisfactory performance primarily originates from an entangled feature space, where adversarial attacks can be performed easily. To this end, we present 3D-ANC, a novel approach that capitalizes on the Neural Collapse (NC) mechanism to orchestrate discriminative feature learning. In particular, NC depicts where last-layer features and classifier weights jointly evolve into a simplex equiangular tight frame (ETF) arrangement, establishing maximally separable class prototypes. However, leveraging this advantage in 3D recognition confronts two substantial challenges: (1) prevalent class imbalance in point cloud datasets, and (2) complex geometric similarities between object categories. To tackle these obstacles, our solution combines an ETF-aligned classification module with an adaptive training framework consisting of representation-balanced learning (RBL) and dynamic feature direction loss (FDL). 3D-ANC seamlessly empowers existing models to develop disentangled feature spaces despite the complexity in 3D data distribution. Comprehensive evaluations state that 3D-ANC significantly improves the robustness of models with various structures on two datasets. For instance, DGCNN's classification accuracy is elevated from 27.2% to 80.9% on ModelNet40 -- a 53.7% absolute gain that surpasses leading baselines by 34.0%.",
    "authors": [
      "Yuanmin Huang",
      "Wenxuan Li",
      "Mi Zhang",
      "Xiaohan Zhang",
      "Xiaoyu You",
      "Min Yang"
    ],
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07040v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07040v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.06988v1",
    "title": "HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety   Detection",
    "summary": "Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSL's potential for anxiety classification.",
    "authors": [
      "Aditya Sneh",
      "Nilesh Kumar Sahu",
      "Anushka Sanjay Shelke",
      "Arya Adyasha",
      "Haroon R. Lone"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06988v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06988v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.06976v1",
    "title": "Rethinking Crystal Symmetry Prediction: A Decoupled Perspective",
    "summary": "Efficiently and accurately determining the symmetry is a crucial step in the structural analysis of crystalline materials. Existing methods usually mindlessly apply deep learning models while ignoring the underlying chemical rules. More importantly, experiments show that they face a serious sub-property confusion SPC problem. To address the above challenges, from a decoupled perspective, we introduce the XRDecoupler framework, a problem-solving arsenal specifically designed to tackle the SPC problem. Imitating the thinking process of chemists, we innovatively incorporate multidimensional crystal symmetry information as superclass guidance to ensure that the model's prediction process aligns with chemical intuition. We further design a hierarchical PXRD pattern learning model and a multi-objective optimization approach to achieve high-quality representation and balanced optimization. Comprehensive evaluations on three mainstream databases (e.g., CCDC, CoREMOF, and InorganicData) demonstrate that XRDecoupler excels in performance, interpretability, and generalization.",
    "authors": [
      "Liheng Yu",
      "Zhe Zhao",
      "Xucong Wang",
      "Di Wu",
      "Pengkun Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06976v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06976v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.06961v1",
    "title": "Hybrid Autoencoders for Tabular Data: Leveraging Model-Based   Augmentation in Low-Label Settings",
    "summary": "Deep neural networks often under-perform on tabular data due to their sensitivity to irrelevant features and a spectral bias toward smooth, low-frequency functions. These limitations hinder their ability to capture the sharp, high-frequency signals that often define tabular structure, especially under limited labeled samples. While self-supervised learning (SSL) offers promise in such settings, it remains challenging in tabular domains due to the lack of effective data augmentations. We propose a hybrid autoencoder that combines a neural encoder with an oblivious soft decision tree (OSDT) encoder, each guided by its own stochastic gating network that performs sample-specific feature selection. Together, these structurally different encoders and model-specific gating networks implement model-based augmentation, producing complementary input views tailored to each architecture. The two encoders, trained with a shared decoder and cross-reconstruction loss, learn distinct yet aligned representations that reflect their respective inductive biases. During training, the OSDT encoder (robust to noise and effective at modeling localized, high-frequency structure) guides the neural encoder toward representations more aligned with tabular data. At inference, only the neural encoder is used, preserving flexibility and SSL compatibility. Spectral analysis highlights the distinct inductive biases of each encoder. Our method achieves consistent gains in low-label classification and regression across diverse tabular datasets, outperforming deep and tree-based supervised baselines.",
    "authors": [
      "Erel Naor",
      "Ofir Lindenbaum"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06961v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06961v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.06948v1",
    "title": "PADM: A Physics-aware Diffusion Model for Attenuation Correction",
    "summary": "Attenuation artifacts remain a significant challenge in cardiac Myocardial Perfusion Imaging (MPI) using Single-Photon Emission Computed Tomography (SPECT), often compromising diagnostic accuracy and reducing clinical interpretability. While hybrid SPECT/CT systems mitigate these artifacts through CT-derived attenuation maps, their high cost, limited accessibility, and added radiation exposure hinder widespread clinical adoption. In this study, we propose a novel CT-free solution to attenuation correction in cardiac SPECT. Specifically, we introduce Physics-aware Attenuation Correction Diffusion Model (PADM), a diffusion-based generative method that incorporates explicit physics priors via a teacher--student distillation mechanism. This approach enables attenuation artifact correction using only Non-Attenuation-Corrected (NAC) input, while still benefiting from physics-informed supervision during training. To support this work, we also introduce CardiAC, a comprehensive dataset comprising 424 patient studies with paired NAC and Attenuation-Corrected (AC) reconstructions, alongside high-resolution CT-based attenuation maps. Extensive experiments demonstrate that PADM outperforms state-of-the-art generative models, delivering superior reconstruction fidelity across both quantitative metrics and visual assessment.",
    "authors": [
      "Trung Kien Pham",
      "Hoang Minh Vu",
      "Anh Duc Chu",
      "Dac Thai Nguyen",
      "Trung Thanh Nguyen",
      "Thao Nguyen Truong",
      "Mai Hong Son",
      "Thanh Trung Nguyen",
      "Phi Le Nguyen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06948v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06948v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.06902v1",
    "title": "A Closer Look at Knowledge Distillation in Spiking Neural Network   Training",
    "summary": "Spiking Neural Networks (SNNs) become popular due to excellent energy efficiency, yet facing challenges for effective model training. Recent works improve this by introducing knowledge distillation (KD) techniques, with the pre-trained artificial neural networks (ANNs) used as teachers and the target SNNs as students. This is commonly accomplished through a straightforward element-wise alignment of intermediate features and prediction logits from ANNs and SNNs, often neglecting the intrinsic differences between their architectures. Specifically, ANN's outputs exhibit a continuous distribution, whereas SNN's outputs are characterized by sparsity and discreteness. To mitigate this issue, we introduce two innovative KD strategies. Firstly, we propose the Saliency-scaled Activation Map Distillation (SAMD), which aligns the spike activation map of the student SNN with the class-aware activation map of the teacher ANN. Rather than performing KD directly on the raw %and distinct features of ANN and SNN, our SAMD directs the student to learn from saliency activation maps that exhibit greater semantic and distribution consistency. Additionally, we propose a Noise-smoothed Logits Distillation (NLD), which utilizes Gaussian noise to smooth the sparse logits of student SNN, facilitating the alignment with continuous logits from teacher ANN. Extensive experiments on multiple datasets demonstrate the effectiveness of our methods. Code is available~\\footnote{https://github.com/SinoLeu/CKDSNN.git}.",
    "authors": [
      "Xu Liu",
      "Na Xia",
      "Jinxing Zhou",
      "Jingyuan Xu",
      "Dan Guo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06902v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06902v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.07406v1",
    "title": "Entangled Schrödinger Bridge Matching",
    "summary": "Simulating trajectories of multi-particle systems on complex energy landscapes is a central task in molecular dynamics (MD) and drug discovery, but remains challenging at scale due to computationally expensive and long simulations. Previous approaches leverage techniques such as flow or Schr\\\"odinger bridge matching to implicitly learn joint trajectories through data snapshots. However, many systems, including biomolecular systems and heterogeneous cell populations, undergo dynamic interactions that evolve over their trajectory and cannot be captured through static snapshots. To close this gap, we introduce Entangled Schr\\\"odinger Bridge Matching (EntangledSBM), a framework that learns the first- and second-order stochastic dynamics of interacting, multi-particle systems where the direction and magnitude of each particle's path depend dynamically on the paths of the other particles. We define the Entangled Schr\\\"odinger Bridge (EntangledSB) problem as solving a coupled system of bias forces that entangle particle velocities. We show that our framework accurately simulates heterogeneous cell populations under perturbations and rare transitions in high-dimensional biomolecular systems.",
    "authors": [
      "Sophia Tang",
      "Yinuo Zhang",
      "Pranam Chatterjee"
    ],
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07406v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07406v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07392v1",
    "title": "Surgical Agent Orchestration Platform for Voice-directed Patient Data   Interaction",
    "summary": "In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.",
    "authors": [
      "Hyeryun Park",
      "Byung Mo Gu",
      "Jun Hee Lee",
      "Byeong Hyeon Choi",
      "Sekeun Kim",
      "Hyun Koo Kim",
      "Kyungsang Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07392v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07392v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07382v1",
    "title": "Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for   Bangla-to-Python Code Generation",
    "summary": "Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on \"Code Generation in Bangla\". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team \"Retriv\" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.",
    "authors": [
      "K M Nafi Asib",
      "Sourav Saha",
      "Mohammed Moshiul Hoque"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07382v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07382v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07366v1",
    "title": "UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A   Multi-Agent DRL Approach",
    "summary": "This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience perspective in the 6G network energy saving (NES) scenario. More specifically, we consider multiple ground base stations (GBSs) and each GBS has three different sectors/cells in the terrestrial networks, and multiple cells are turned off due to NES or incidents, e.g., disasters, hardware failures, or outages. To address this, we propose a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) framework to enable UAV-assisted communication by jointly optimizing UAV trajectories, transmission power, and user-UAV association under a sleeping ground base station (GBS) strategy. This framework aims to ensure the resilience of active users in the network and the long-term operability of UAVs. Specifically, it maximizes service coverage for users during power outages or NES zones, while minimizing the energy consumption of UAVs. Simulation results demonstrate that the proposed MADDPG policy consistently achieves high coverage ratio across different testing episodes, outperforming other baselines. Moreover, the MADDPG framework attains the lowest total energy consumption, with a reduction of approximately 24\\% compared to the conventional all GBS ON configuration, while maintaining a comparable user service rate. These results confirm the effectiveness of the proposed approach in achieving a superior trade-off between energy efficiency and service performance, supporting the development of sustainable and resilient UAV-assisted cellular networks.",
    "authors": [
      "Dao Lan Vy Dinh",
      "Anh Nguyen Thi Mai",
      "Hung Tran",
      "Giang Quynh Le Vu",
      "Tu Dac Ho",
      "Zhenni Pan",
      "Vo Nhan Van",
      "Symeon Chatzinotas",
      "Dinh-Hieu Tran"
    ],
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07366v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07366v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07299v1",
    "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware   Large Language Models",
    "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and semantic comprehension of anomalous events within videos, addressing limitations of traditional methods that focus solely on detecting and localizing anomalies. However, existing approaches often neglect the deeper causal relationships and interactions between objects, which are critical for understanding anomalous behaviors. In this paper, we propose VADER, an LLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe object Relation features with visual cues to enhance anomaly comprehension from video. Specifically, VADER first applies an Anomaly Scorer to assign per-frame anomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture the causal context of each anomalous event. A Relation Feature Extractor and a COntrastive Relation Encoder (CORE) jointly model dynamic object interactions, producing compact relational representations for downstream reasoning. These visual and relational cues are integrated with LLMs to generate detailed, causally grounded descriptions and support robust anomaly-related question answering. Experiments on multiple real-world VAU benchmarks demonstrate that VADER achieves strong results across anomaly description, explanation, and causal reasoning tasks, advancing the frontier of explainable video anomaly analysis.",
    "authors": [
      "Ying Cheng",
      "Yu-Ho Lin",
      "Min-Hung Chen",
      "Fu-En Yang",
      "Shang-Hong Lai"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07299v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07299v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07276v1",
    "title": "RobustA: Robust Anomaly Detection in Multimodal Data",
    "summary": "In recent years, multimodal anomaly detection methods have demonstrated remarkable performance improvements over video-only models. However, real-world multimodal data is often corrupted due to unforeseen environmental distortions. In this paper, we present the first-of-its-kind work that comprehensively investigates the adverse effects of corrupted modalities on multimodal anomaly detection task. To streamline this work, we propose RobustA, a carefully curated evaluation dataset to systematically observe the impacts of audio and visual corruptions on the overall effectiveness of anomaly detection systems. Furthermore, we propose a multimodal anomaly detection method, which shows notable resilience against corrupted modalities. The proposed method learns a shared representation space for different modalities and employs a dynamic weighting scheme during inference based on the estimated level of corruption. Our work represents a significant step forward in enabling the real-world application of multimodal anomaly detection, addressing situations where the likely events of modality corruptions occur. The proposed evaluation dataset with corrupted modalities and respective extracted features will be made publicly available.",
    "authors": [
      "Salem AlMarri",
      "Muhammad Irzam Liaqat",
      "Muhammad Zaigham Zaheer",
      "Shah Nawaz",
      "Karthik Nandakumar",
      "Markus Schedl"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07276v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07276v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07267v1",
    "title": "Beyond Detection: Exploring Evidence-based Multi-Agent Debate for   Misinformation Intervention and Persuasion",
    "summary": "Multi-agent debate (MAD) frameworks have emerged as promising approaches for misinformation detection by simulating adversarial reasoning. While prior work has focused on detection accuracy, it overlooks the importance of helping users understand the reasoning behind factual judgments and develop future resilience. The debate transcripts generated during MAD offer a rich but underutilized resource for transparent reasoning. In this study, we introduce ED2D, an evidence-based MAD framework that extends previous approach by incorporating factual evidence retrieval. More importantly, ED2D is designed not only as a detection framework but also as a persuasive multi-agent system aimed at correcting user beliefs and discouraging misinformation sharing. We compare the persuasive effects of ED2D-generated debunking transcripts with those authored by human experts. Results demonstrate that ED2D outperforms existing baselines across three misinformation detection benchmarks. When ED2D generates correct predictions, its debunking transcripts exhibit persuasive effects comparable to those of human experts; However, when ED2D misclassifies, its accompanying explanations may inadvertently reinforce users'misconceptions, even when presented alongside accurate human explanations. Our findings highlight both the promise and the potential risks of deploying MAD systems for misinformation intervention. We further develop a public community website to help users explore ED2D, fostering transparency, critical thinking, and collaborative fact-checking.",
    "authors": [
      "Chen Han",
      "Yijia Ma",
      "Jin Tan",
      "Wenzhen Zheng",
      "Xijin Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07267v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07267v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07241v1",
    "title": "4DSTR: Advancing Generative 4D Gaussians with Spatial-Temporal   Rectification for High-Quality and Consistent 4D Generation",
    "summary": "Remarkable advances in recent 2D image and 3D shape generation have induced a significant focus on dynamic 4D content generation. However, previous 4D generation methods commonly struggle to maintain spatial-temporal consistency and adapt poorly to rapid temporal variations, due to the lack of effective spatial-temporal modeling. To address these problems, we propose a novel 4D generation network called 4DSTR, which modulates generative 4D Gaussian Splatting with spatial-temporal rectification. Specifically, temporal correlation across generated 4D sequences is designed to rectify deformable scales and rotations and guarantee temporal consistency. Furthermore, an adaptive spatial densification and pruning strategy is proposed to address significant temporal variations by dynamically adding or deleting Gaussian points with the awareness of their pre-frame movements. Extensive experiments demonstrate that our 4DSTR achieves state-of-the-art performance in video-to-4D generation, excelling in reconstruction quality, spatial-temporal consistency, and adaptation to rapid temporal movements.",
    "authors": [
      "Mengmeng Liu",
      "Jiuming Liu",
      "Yunpeng Zhang",
      "Jiangtao Li",
      "Michael Ying Yang",
      "Francesco Nex",
      "Hao Cheng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07241v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07241v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07236v1",
    "title": "Does TabPFN Understand Causal Structures?",
    "summary": "Causal discovery is fundamental for multiple scientific domains, yet extracting causal information from real world data remains a significant challenge. Given the recent success on real data, we investigate whether TabPFN, a transformer-based tabular foundation model pre-trained on synthetic datasets generated from structural causal models, encodes causal information in its internal representations. We develop an adapter framework using a learnable decoder and causal tokens that extract causal signals from TabPFN's frozen embeddings and decode them into adjacency matrices for causal discovery. Our evaluations demonstrate that TabPFN's embeddings contain causal information, outperforming several traditional causal discovery algorithms, with such causal information being concentrated in mid-range layers. These findings establish a new direction for interpretable and adaptable foundation models and demonstrate the potential for leveraging pre-trained tabular models for causal discovery.",
    "authors": [
      "Omar Swelam",
      "Lennart Purucker",
      "Jake Robertson",
      "Hanne Raum",
      "Joschka Boedecker",
      "Frank Hutter"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07236v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07236v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07235v1",
    "title": "Deep Neural Operator Learning for Probabilistic Models",
    "summary": "We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean space-and for a broad class of probabilistic models-we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forward-backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.",
    "authors": [
      "Erhan Bayraktar",
      "Qi Feng",
      "Zecheng Zhang",
      "Zhaoyu Zhang"
    ],
    "categories": [
      "cs.LG",
      "q-fin.CP"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07235v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07235v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07230v1",
    "title": "Discourse Graph Guided Document Translation with Large Language Models",
    "summary": "Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.",
    "authors": [
      "Viet-Thanh Pham",
      "Minghan Wang",
      "Hao-Han Liao",
      "Thuy-Trang Vu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07230v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07230v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07222v1",
    "title": "Omni-View: Unlocking How Generation Facilitates Understanding in Unified   3D Model based on Multiview images",
    "summary": "This paper presents Omni-View, which extends the unified multimodal understanding and generation to 3D scenes based on multiview images, exploring the principle that \"generation facilitates understanding\". Consisting of understanding model, texture module, and geometry module, Omni-View jointly models scene understanding, novel view synthesis, and geometry estimation, enabling synergistic interaction between 3D scene understanding and generation tasks. By design, it leverages the spatiotemporal modeling capabilities of its texture module responsible for appearance synthesis, alongside the explicit geometric constraints provided by its dedicated geometry module, thereby enriching the model's holistic understanding of 3D scenes. Trained with a two-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the VSI-Bench benchmark, outperforming existing specialized 3D understanding models, while simultaneously delivering strong performance in both novel view synthesis and 3D scene generation.",
    "authors": [
      "JiaKui Hu",
      "Shanshan Zhao",
      "Qing-Guo Chen",
      "Xuerui Qiu",
      "Jialun Liu",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Yanye Lu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07222v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07222v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07197v1",
    "title": "Simulation-based Methods for Optimal Sampling Design in Systems Biology",
    "summary": "In many areas of systems biology, including virology, pharmacokinetics, and population biology, dynamical systems are commonly used to describe biological processes. These systems can be characterized by estimating their parameters from sampled data. The key problem is how to optimally select sampling points to achieve accurate parameter estimation. Classical approaches often rely on Fisher information matrix-based criteria such as A-, D-, and E-optimality, which require an initial parameter estimate and may yield suboptimal results when the estimate is inaccurate. This study proposes two simulation-based methods for optimal sampling design that do not depend on initial parameter estimates. The first method, E-optimal-ranking (EOR), employs the E-optimal criterion, while the second utilizes a Long Short-Term Memory (LSTM) neural network. Simulation studies based on the Lotka-Volterra and three-compartment models demonstrate that the proposed methods outperform both random selection and classical E-optimal design.",
    "authors": [
      "Tuan Minh Ha",
      "Binh Thanh Nguyen",
      "Lam Si Tung Ho"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07197v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07197v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07170v1",
    "title": "On Stealing Graph Neural Network Models",
    "summary": "Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract the GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.",
    "authors": [
      "Marcin Podhajski",
      "Jan Dubiński",
      "Franziska Boenisch",
      "Adam Dziedzic",
      "Agnieszka Pręgowska",
      "Tomasz P. Michalak"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07170v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07170v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07109v1",
    "title": "A Provably-Correct and Robust Convex Model for Smooth Separable NMF",
    "summary": "Nonnegative matrix factorization (NMF) is a linear dimensionality reduction technique for nonnegative data, with applications such as hyperspectral unmixing and topic modeling. NMF is a difficult problem in general (NP-hard), and its solutions are typically not unique. To address these two issues, additional constraints or assumptions are often used. In particular, separability assumes that the basis vectors in the NMF are equal to some columns of the input matrix. In that case, the problem is referred to as separable NMF (SNMF) and can be solved in polynomial-time with robustness guarantees, while identifying a unique solution. However, in real-world scenarios, due to noise or variability, multiple data points may lie near the basis vectors, which SNMF does not leverage. In this work, we rely on the smooth separability assumption, which assumes that each basis vector is close to multiple data points. We explore the properties of the corresponding problem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF and orthogonal NMF. We then propose a convex model for SSNMF and show that it provably recovers the sought-after factors, even in the presence of noise. We finally adapt an existing fast gradient method to solve this convex model for SSNMF, and show that it compares favorably with state-of-the-art methods on both synthetic and hyperspectral datasets.",
    "authors": [
      "Junjun Pan",
      "Valentin Leplat",
      "Michael Ng",
      "Nicolas Gillis"
    ],
    "categories": [
      "math.NA",
      "cs.LG",
      "cs.NA",
      "eess.SP",
      "math.OC",
      "stat.ML"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07109v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07109v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07095v1",
    "title": "Data Complexity of Querying Description Logic Knowledge Bases under   Cost-Based Semantics",
    "summary": "In this paper, we study the data complexity of querying inconsistent weighted description logic (DL) knowledge bases under recently-introduced cost-based semantics. In a nutshell, the idea is to assign each interpretation a cost based upon the weights of the violated axioms and assertions, and certain and possible query answers are determined by considering all (resp. some) interpretations having optimal or bounded cost. Whereas the initial study of cost-based semantics focused on DLs between $\\mathcal{EL}_\\bot$ and $\\mathcal{ALCO}$, we consider DLs that may contain inverse roles and role inclusions, thus covering prominent DL-Lite dialects. Our data complexity analysis goes significantly beyond existing results by sharpening several lower bounds and pinpointing the precise complexity of optimal-cost certain answer semantics (no non-trivial upper bound was known). Moreover, while all existing results show the intractability of cost-based semantics, our most challenging and surprising result establishes that if we consider $\\text{DL-Lite}^\\mathcal{H}_\\mathsf{bool}$ ontologies and a fixed cost bound, certain answers for instance queries and possible answers for conjunctive queries can be computed using first-order rewriting and thus enjoy the lowest possible data complexity ($\\mathsf{TC}_0$).",
    "authors": [
      "Meghyn Bienvenu",
      "Quentin Manière"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07095v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07095v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07090v1",
    "title": "Green AI: A systematic review and meta-analysis of its definitions,   lifecycle models, hardware and measurement attempts",
    "summary": "Across the Artificial Intelligence (AI) lifecycle - from hardware to development, deployment, and reuse - burdens span energy, carbon, water, and embodied impacts. Cloud provider tools improve transparency but remain heterogeneous and often omit water and value chain effects, limiting comparability and reproducibility. Addressing these multi dimensional burdens requires a lifecycle approach linking phase explicit mapping with system levers (hardware, placement, energy mix, cooling, scheduling) and calibrated measurement across facility, system, device, and workload levels. This article (i) establishes a unified, operational definition of Green AI distinct from Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle Assessment (LCA) stages, making energy, carbon, water, and embodied impacts first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles with decision gateways; (iv) systematizes hardware and system level strategies across the edge cloud continuum to reduce embodied burdens; and (v) defines a calibrated measurement framework combining estimator models with direct metering to enable reproducible, provider agnostic comparisons. Combining definition, lifecycle processes, hardware strategies, and calibrated measurement, this article offers actionable, evidence based guidance for researchers, practitioners, and policymakers.",
    "authors": [
      "Marcel Rojahn",
      "Marcus Grum"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07090v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07090v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07083v1",
    "title": "Increasing AI Explainability by LLM Driven Standard Processes",
    "summary": "This paper introduces an approach to increasing the explainability of artificial intelligence (AI) systems by embedding Large Language Models (LLMs) within standardized analytical processes. While traditional explainable AI (XAI) methods focus on feature attribution or post-hoc interpretation, the proposed framework integrates LLMs into defined decision models such as Question-Option-Criteria (QOC), Sensitivity Analysis, Game Theory, and Risk Management. By situating LLM reasoning within these formal structures, the approach transforms opaque inference into transparent and auditable decision traces. A layered architecture is presented that separates the reasoning space of the LLM from the explainable process space above it. Empirical evaluations show that the system can reproduce human-level decision logic in decentralized governance, systems analysis, and strategic reasoning contexts. The results suggest that LLM-driven standard processes provide a foundation for reliable, interpretable, and verifiable AI-supported decision making.",
    "authors": [
      "Marc Jansen",
      "Marcel Pehlke"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07083v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07083v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07080v1",
    "title": "Wasm: A Pipeline for Constructing Structured Arabic Interleaved   Multimodal Corpora",
    "summary": "The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.",
    "authors": [
      "Khalil Hennara",
      "Ahmad Bastati",
      "Muhammad Hreden",
      "Mohamed Motasim Hamed",
      "Zeina Aldallal",
      "Sara Chrouf",
      "Safwan AlModhayan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07080v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07080v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.07378v1",
    "title": "Transformers Provably Learn Chain-of-Thought Reasoning with Length   Generalization",
    "summary": "The ability to reason lies at the core of artificial intelligence (AI), and challenging problems usually call for deeper and longer reasoning to tackle. A crucial question about AI reasoning is whether models can extrapolate learned reasoning patterns to solve harder tasks with longer chain-of-thought (CoT). In this work, we present a theoretical analysis of transformers learning on synthetic state-tracking tasks with gradient descent. We mathematically prove how the algebraic structure of state-tracking problems governs the degree of extrapolation of the learned CoT. Specifically, our theory characterizes the length generalization of transformers through the mechanism of attention concentration, linking the retrieval robustness of the attention layer to the state-tracking task structure of long-context reasoning. Moreover, for transformers with limited reasoning length, we prove that a recursive self-training scheme can progressively extend the range of solvable problem lengths. To our knowledge, we provide the first optimization guarantee that constant-depth transformers provably learn $\\mathsf{NC}^1$-complete problems with CoT, significantly going beyond prior art confined in $\\mathsf{TC}^0$, unless the widely held conjecture $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$ fails. Finally, we present a broad set of experiments supporting our theoretical results, confirming the length generalization behaviors and the mechanism of attention concentration.",
    "authors": [
      "Yu Huang",
      "Zixin Wen",
      "Aarti Singh",
      "Yuejie Chi",
      "Yuxin Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07378v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07378v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07368v1",
    "title": "Consistency Is Not Always Correct: Towards Understanding the Role of   Exploration in Post-Training Reasoning",
    "summary": "Foundation models exhibit broad knowledge but limited task-specific reasoning, motivating post-training strategies such as RLVR and inference scaling with outcome or process reward models (ORM/PRM). While recent work highlights the role of exploration and entropy stability in improving pass@K, empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce existing tree-like reasoning paths rather than expanding the reasoning scope, raising the question of why exploration helps at all if no new patterns emerge.   To reconcile this paradox, we adopt the perspective of Kim et al. (2025), viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a symmetry) reasoning steps as low- versus high-probability Markov transitions, and formalize post-training dynamics through Multi-task Tree-structured Markov Chains (TMC). In this tractable model, pretraining corresponds to tree expansion, while post-training corresponds to chain-of-thought reweighting. We show that several phenomena recently observed in empirical studies arise naturally in this setting: (1) RLVR induces a squeezing effect, reducing reasoning entropy and forgetting some correct paths; (2) population rewards of ORM/PRM encourage consistency rather than accuracy, thereby favoring common patterns; and (3) certain rare, high-uncertainty reasoning paths by the base model are responsible for solving hard problem instances.   Together, these explain why exploration -- even when confined to the base model's reasoning scope -- remains essential: it preserves access to rare but crucial reasoning traces needed for difficult cases, which are squeezed out by RLVR or unfavored by inference scaling. Building on this, we further show that exploration strategies such as rejecting easy instances and KL regularization help preserve rare reasoning traces. Empirical simulations corroborate our theoretical results.",
    "authors": [
      "Dake Bu",
      "Wei Huang",
      "Andi Han",
      "Atsushi Nitanda",
      "Bo Xue",
      "Qingfu Zhang",
      "Hau-San Wong",
      "Taiji Suzuki"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07368v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07368v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07322v1",
    "title": "FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework   for Equity Research Report Generation",
    "summary": "While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.",
    "authors": [
      "Song Jin",
      "Shuqi Li",
      "Shukun Zhang",
      "Rui Yan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07322v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07322v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07321v1",
    "title": "YoNoSplat: You Only Need One Model for Feedforward 3D Gaussian Splatting",
    "summary": "Fast and flexible 3D scene reconstruction from unstructured image collections remains a significant challenge. We present YoNoSplat, a feedforward model that reconstructs high-quality 3D Gaussian Splatting representations from an arbitrary number of images. Our model is highly versatile, operating effectively with both posed and unposed, calibrated and uncalibrated inputs. YoNoSplat predicts local Gaussians and camera poses for each view, which are aggregated into a global representation using either predicted or provided poses. To overcome the inherent difficulty of jointly learning 3D Gaussians and camera parameters, we introduce a novel mixing training strategy. This approach mitigates the entanglement between the two tasks by initially using ground-truth poses to aggregate local Gaussians and gradually transitioning to a mix of predicted and ground-truth poses, which prevents both training instability and exposure bias. We further resolve the scale ambiguity problem by a novel pairwise camera-distance normalization scheme and by embedding camera intrinsics into the network. Moreover, YoNoSplat also predicts intrinsic parameters, making it feasible for uncalibrated inputs. YoNoSplat demonstrates exceptional efficiency, reconstructing a scene from 100 views (at 280x518 resolution) in just 2.69 seconds on an NVIDIA GH200 GPU. It achieves state-of-the-art performance on standard benchmarks in both pose-free and pose-dependent settings. Our project page is at https://botaoye.github.io/yonosplat/.",
    "authors": [
      "Botao Ye",
      "Boqi Chen",
      "Haofei Xu",
      "Daniel Barath",
      "Marc Pollefeys"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07321v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07321v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07204v1",
    "title": "Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations",
    "summary": "Online Social Networks (OSNs) widely adopt content moderation to mitigate the spread of abusive and toxic discourse. Nonetheless, the real effectiveness of moderation interventions remains unclear due to the high cost of data collection and limited experimental control. The latest developments in Natural Language Processing pave the way for a new evaluation approach. Large Language Models (LLMs) can be successfully leveraged to enhance Agent-Based Modeling and simulate human-like social behavior with unprecedented degree of believability. Yet, existing tools do not support simulation-based evaluation of moderation strategies. We fill this gap by designing a LLM-powered simulator of OSN conversations enabling a parallel, counterfactual simulation where toxic behavior is influenced by moderation interventions, keeping all else equal. We conduct extensive experiments, unveiling the psychological realism of OSN agents, the emergence of social contagion phenomena and the superior effectiveness of personalized moderation strategies.",
    "authors": [
      "Giacomo Fidone",
      "Lucia Passaro",
      "Riccardo Guidotti"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07204v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07204v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07193v1",
    "title": "EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large   Language Models",
    "summary": "Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.",
    "authors": [
      "Jiacheng Huang",
      "Ning Yu",
      "Xiaoyin Yi"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07193v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07193v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07126v1",
    "title": "Saliency Map-Guided Knowledge Discovery for Subclass Identification with   LLM-Based Symbolic Approximations",
    "summary": "This paper proposes a novel neuro-symbolic approach for sensor signal-based knowledge discovery, focusing on identifying latent subclasses in time series classification tasks. The approach leverages gradient-based saliency maps derived from trained neural networks to guide the discovery process. Multiclass time series classification problems are transformed into binary classification problems through label subsumption, and classifiers are trained for each of these to yield saliency maps. The input signals, grouped by predicted class, are clustered under three distinct configurations. The centroids of the final set of clusters are provided as input to an LLM for symbolic approximation and fuzzy knowledge graph matching to discover the underlying subclasses of the original multiclass problem. Experimental results on well-established time series classification datasets demonstrate the effectiveness of our saliency map-driven method for knowledge discovery, outperforming signal-only baselines in both clustering and subclass identification.",
    "authors": [
      "Tim Bohne",
      "Anne-Kathrin Patricia Windler",
      "Martin Atzmueller"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07126v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07126v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07106v1",
    "title": "HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and   End-to-end Autonomous Driving",
    "summary": "Three-dimensional feature extraction is a critical component of autonomous driving systems, where perception tasks such as 3D object detection, bird's-eye-view (BEV) semantic segmentation, and occupancy prediction serve as important constraints on 3D features. While large image encoders, high-resolution images, and long-term temporal inputs can significantly enhance feature quality and deliver remarkable performance gains, these techniques are often incompatible in both training and inference due to computational resource constraints. Moreover, different tasks favor distinct feature representations, making it difficult for a single model to perform end-to-end inference across multiple tasks while maintaining accuracy comparable to that of single-task models. To alleviate these issues, we present the HENet and HENet++ framework for multi-task 3D perception and end-to-end autonomous driving. Specifically, we propose a hybrid image encoding network that uses a large image encoder for short-term frames and a small one for long-term frames. Furthermore, our framework simultaneously extracts both dense and sparse features, providing more suitable representations for different tasks, reducing cumulative errors, and delivering more comprehensive information to the planning module. The proposed architecture maintains compatibility with various existing 3D feature extraction methods and supports multimodal inputs. HENet++ achieves state-of-the-art end-to-end multi-task 3D perception results on the nuScenes benchmark, while also attaining the lowest collision rate on the nuScenes end-to-end autonomous driving benchmark.",
    "authors": [
      "Zhongyu Xia",
      "Zhiwei Lin",
      "Yongtao Wang",
      "Ming-Hsuan Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07106v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07106v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07070v1",
    "title": "RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social   Networking Services",
    "summary": "As a key medium for human interaction and information exchange, social networking services (SNS) pose unique challenges for large language models (LLMs): heterogeneous workloads, fast-shifting norms and slang, and multilingual, culturally diverse corpora that induce sharp distribution shift. Supervised fine-tuning (SFT) can specialize models but often triggers a ``seesaw'' between in-distribution gains and out-of-distribution robustness, especially for smaller models. To address these challenges, we introduce RedOne 2.0, an SNS-oriented LLM trained with a progressive, RL-prioritized post-training paradigm designed for rapid and stable adaptation. The pipeline consist in three stages: (1) Exploratory Learning on curated SNS corpora to establish initial alignment and identify systematic weaknesses; (2) Targeted Fine-Tuning that selectively applies SFT to the diagnosed gaps while mixing a small fraction of general data to mitigate forgetting; and (3) Refinement Learning that re-applies RL with SNS-centric signals to consolidate improvements and harmonize trade-offs across tasks. Across various tasks spanning three categories, our 4B scale model delivers an average improvements about 2.41 over the 7B sub-optimal baseline. Additionally, RedOne 2.0 achieves average performance lift about 8.74 from the base model with less than half the data required by SFT-centric method RedOne, evidencing superior data efficiency and stability at compact scales. Overall, RedOne 2.0 establishes a competitive, cost-effective baseline for domain-specific LLMs in SNS scenario, advancing capability without sacrificing robustness.",
    "authors": [
      "Fei Zhao",
      "Chonggang Lu",
      "Haofu Qian",
      "Fangcheng Shi",
      "Zijie Meng",
      "Jianzhao Huang",
      "Xu Tang",
      "Zheyong Xie",
      "Zheyu Ye",
      "Zhe Xu",
      "Yao Hu",
      "Shaosheng Cao"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07070v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07070v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07044v1",
    "title": "Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating   Large Language Models for Anxiety, Depression, and Stress Detection: Insights   into Prompting Strategies and Synthetic Data",
    "summary": "Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.",
    "authors": [
      "Mihael Arcan",
      "David-Paul Niland"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07044v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07044v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.07347v1",
    "title": "Walsh-Hadamard Neural Operators for Solving PDEs with Discontinuous   Coefficients",
    "summary": "Neural operators have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). However, standard spectral methods based on Fourier transforms struggle with problems involving discontinuous coefficients due to the Gibbs phenomenon and poor representation of sharp interfaces. We introduce the Walsh-Hadamard Neural Operator (WHNO), which leverages Walsh-Hadamard transforms-a spectral basis of rectangular wave functions naturally suited for piecewise constant fields-combined with learnable spectral weights that transform low-sequency Walsh coefficients to capture global dependencies efficiently. We validate WHNO on three problems: steady-state Darcy flow (preliminary validation), heat conduction with discontinuous thermal conductivity, and the 2D Burgers equation with discontinuous initial conditions. In controlled comparisons with Fourier Neural Operators (FNO) under identical conditions, WHNO demonstrates superior accuracy with better preservation of sharp solution features at material interfaces. Critically, we discover that weighted ensemble combinations of WHNO and FNO achieve substantial improvements over either model alone: for both heat conduction and Burgers equation, optimal ensembles reduce mean squared error by 35-40 percent and maximum error by up to 25 percent compared to individual models. This demonstrates that Walsh-Hadamard and Fourier representations capture complementary aspects of discontinuous PDE solutions, with WHNO excelling at sharp interfaces while FNO captures smooth features effectively.",
    "authors": [
      "Giorrgio M. Cavallazzi",
      "Miguel Perex Cuadrado",
      "Alfredo Pinelli"
    ],
    "categories": [
      "physics.comp-ph",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07347v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07347v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07280v1",
    "title": "The Value of Personalized Recommendations: Evidence from Netflix",
    "summary": "Personalized recommendation systems shape much of user choice online, yet their targeted nature makes separating out the value of recommendation and the underlying goods challenging. We build a discrete choice model that embeds recommendation-induced utility, low-rank heterogeneity, and flexible state dependence and apply the model to viewership data at Netflix. We exploit idiosyncratic variation introduced by the recommendation algorithm to identify and separately value these components as well as to recover model-free diversion ratios that we can use to validate our structural model. We use the model to evaluate counterfactuals that quantify the incremental engagement generated by personalized recommendations. First, we show that replacing the current recommender system with a matrix factorization or popularity-based algorithm would lead to 4% and 12% reduction in engagement, respectively, and decreased consumption diversity. Second, most of the consumption increase from recommendations comes from effective targeting, not mechanical exposure, with the largest gains for mid-popularity goods (as opposed to broadly appealing or very niche goods).",
    "authors": [
      "Kevin Zielnicki",
      "Guy Aridor",
      "Aurélien Bibaut",
      "Allen Tran",
      "Winston Chou",
      "Nathan Kallus"
    ],
    "categories": [
      "econ.GN",
      "cs.IR",
      "cs.LG",
      "q-fin.EC"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07280v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07280v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07208v1",
    "title": "SMiLE: Provably Enforcing Global Relational Properties in Neural   Networks",
    "summary": "Artificial Intelligence systems are increasingly deployed in settings where ensuring robustness, fairness, or domain-specific properties is essential for regulation compliance and alignment with human values. However, especially on Neural Networks, property enforcement is very challenging, and existing methods are limited to specific constraints or local properties (defined around datapoints), or fail to provide full guarantees. We tackle these limitations by extending SMiLE, a recently proposed enforcement framework for NNs, to support global relational properties (defined over the entire input space). The proposed approach scales well with model complexity, accommodates general properties and backbones, and provides full satisfaction guarantees. We evaluate SMiLE on monotonicity, global robustness, and individual fairness, on synthetic and real data, for regression and classification tasks. Our approach is competitive with property-specific baselines in terms of accuracy and runtime, and strictly superior in terms of generality and level of guarantees. Overall, our results emphasize the potential of the SMiLE framework as a platform for future research and applications.",
    "authors": [
      "Matteo Francobaldi",
      "Michele Lombardi",
      "Andrea Lodi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07208v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07208v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07118v1",
    "title": "On the Joint Minimization of Regularization Loss Functions in Deep   Variational Bayesian Methods for Attribute-Controlled Symbolic Music   Generation",
    "summary": "Explicit latent variable models provide a flexible yet powerful framework for data synthesis, enabling controlled manipulation of generative factors. With latent variables drawn from a tractable probability density function that can be further constrained, these models enable continuous and semantically rich exploration of the output space by navigating their latent spaces. Structured latent representations are typically obtained through the joint minimization of regularization loss functions. In variational information bottleneck models, reconstruction loss and Kullback-Leibler Divergence (KLD) are often linearly combined with an auxiliary Attribute-Regularization (AR) loss. However, balancing KLD and AR turns out to be a very delicate matter. When KLD dominates over AR, generative models tend to lack controllability; when AR dominates over KLD, the stochastic encoder is encouraged to violate the standard normal prior. We explore this trade-off in the context of symbolic music generation with explicit control over continuous musical attributes. We show that existing approaches struggle to jointly minimize both regularization objectives, whereas suitable attribute transformations can help achieve both controllability and regularization of the target latent dimensions.",
    "authors": [
      "Matteo Pettenó",
      "Alessandro Ilic Mezza",
      "Alberto Bernardini"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07118v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07118v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07112v1",
    "title": "More Agents Helps but Adversarial Robustness Gap Persists",
    "summary": "When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.",
    "authors": [
      "Khashayar Alavi",
      "Zhastay Yeltay",
      "Lucie Flek",
      "Akbar Karimi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07112v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07112v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07055v1",
    "title": "When Sufficient is not Enough: Utilizing the Rashomon Effect for   Complete Evidence Extraction",
    "summary": "Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\\sim$0.60 (single best model) to $\\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.",
    "authors": [
      "Katharina Beckh",
      "Stefan Rüping"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07055v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07055v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07049v1",
    "title": "From Pretrain to Pain: Adversarial Vulnerability of Video Foundation   Models Without Task Knowledge",
    "summary": "Large-scale Video Foundation Models (VFMs) has significantly advanced various video-related tasks, either through task-specific models or Multi-modal Large Language Models (MLLMs). However, the open accessibility of VFMs also introduces critical security risks, as adversaries can exploit full knowledge of the VFMs to launch potent attacks. This paper investigates a novel and practical adversarial threat scenario: attacking downstream models or MLLMs fine-tuned from open-source VFMs, without requiring access to the victim task, training data, model query, and architecture. In contrast to conventional transfer-based attacks that rely on task-aligned surrogate models, we demonstrate that adversarial vulnerabilities can be exploited directly from the VFMs. To this end, we propose the Transferable Video Attack (TVA), a temporal-aware adversarial attack method that leverages the temporal representation dynamics of VFMs to craft effective perturbations. TVA integrates a bidirectional contrastive learning mechanism to maximize the discrepancy between the clean and adversarial features, and introduces a temporal consistency loss that exploits motion cues to enhance the sequential impact of perturbations. TVA avoids the need to train expensive surrogate models or access to domain-specific data, thereby offering a more practical and efficient attack strategy. Extensive experiments across 24 video-related tasks demonstrate the efficacy of TVA against downstream models and MLLMs, revealing a previously underexplored security vulnerability in the deployment of video models.",
    "authors": [
      "Hui Lu",
      "Yi Yu",
      "Song Xia",
      "Yiming Yang",
      "Deepu Rajan",
      "Boon Poh Ng",
      "Alex Kot",
      "Xudong Jiang"
    ],
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07049v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07049v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07029v1",
    "title": "Certified L2-Norm Robustness of 3D Point Cloud Recognition in the   Frequency Domain",
    "summary": "3D point cloud classification is a fundamental task in safety-critical applications such as autonomous driving, robotics, and augmented reality. However, recent studies reveal that point cloud classifiers are vulnerable to structured adversarial perturbations and geometric corruptions, posing risks to their deployment in safety-critical scenarios. Existing certified defenses limit point-wise perturbations but overlook subtle geometric distortions that preserve individual points yet alter the overall structure, potentially leading to misclassification. In this work, we propose FreqCert, a novel certification framework that departs from conventional spatial domain defenses by shifting robustness analysis to the frequency domain, enabling structured certification against global L2-bounded perturbations. FreqCert first transforms the input point cloud via the graph Fourier transform (GFT), then applies structured frequency-aware subsampling to generate multiple sub-point clouds. Each sub-cloud is independently classified by a standard model, and the final prediction is obtained through majority voting, where sub-clouds are constructed based on spectral similarity rather than spatial proximity, making the partitioning more stable under L2 perturbations and better aligned with the object's intrinsic structure. We derive a closed-form lower bound on the certified L2 robustness radius and prove its tightness under minimal and interpretable assumptions, establishing a theoretical foundation for frequency domain certification. Extensive experiments on the ModelNet40 and ScanObjectNN datasets demonstrate that FreqCert consistently achieves higher certified accuracy and empirical accuracy under strong perturbations. Our results suggest that spectral representations provide an effective pathway toward certifiable robustness in 3D point cloud recognition.",
    "authors": [
      "Liang Zhou",
      "Qiming Wang",
      "Tianze Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07029v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07029v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.06958v1",
    "title": "Learning from the Right Patches: A Two-Stage Wavelet-Driven Masked   Autoencoder for Histopathology Representation Learning",
    "summary": "Whole-slide images are central to digital pathology, yet their extreme size and scarce annotations make self-supervised learning essential. Masked Autoencoders (MAEs) with Vision Transformer backbones have recently shown strong potential for histopathology representation learning. However, conventional random patch sampling during MAE pretraining often includes irrelevant or noisy regions, limiting the model's ability to capture meaningful tissue patterns. In this paper, we present a lightweight and domain-adapted framework that brings structure and biological relevance into MAE-based learning through a wavelet-informed patch selection strategy. WISE-MAE applies a two-step coarse-to-fine process: wavelet-based screening at low magnification to locate structurally rich regions, followed by high-resolution extraction for detailed modeling. This approach mirrors the diagnostic workflow of pathologists and improves the quality of learned representations. Evaluations across multiple cancer datasets, including lung, renal, and colorectal tissues, show that WISE-MAE achieves competitive representation quality and downstream classification performance while maintaining efficiency under weak supervision.",
    "authors": [
      "Raneen Younis",
      "Louay Hamdi",
      "Lukas Chavez",
      "Zahra Ahmadi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06958v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06958v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.06913v1",
    "title": "Sampling and Loss Weights in Multi-Domain Training",
    "summary": "In the training of large deep neural networks, there is a need for vast amounts of training data. To meet this need, data is collected from multiple domains, such as Wikipedia and GitHub. These domains are heterogeneous in both data quality and the diversity of information they provide. This raises the question of how much we should rely on each domain. Several methods have attempted to address this issue by assigning sampling weights to each data domain using heuristics or approximations. As a first step toward a deeper understanding of the role of data mixing, this work revisits the problem by studying two kinds of weights: sampling weights, which control how much each domain contributes in a batch, and loss weights, which scale the loss from each domain during training. Through a rigorous study of linear regression, we show that these two weights play complementary roles. First, they can reduce the variance of gradient estimates in iterative methods such as stochastic gradient descent (SGD). Second, they can improve generalization performance by reducing the generalization gap. We provide both theoretical and empirical support for these claims. We further study the joint dynamics of sampling weights and loss weights, examining how they can be combined to capture both contributions.",
    "authors": [
      "Mahdi Salmani",
      "Pratik Worah",
      "Meisam Razaviyayn",
      "Vahab Mirrokni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06913v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06913v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.07304v1",
    "title": "Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task   Learning Approach for Bangla Hate Speech Identification",
    "summary": "This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the \"Bangla Multi-task Hate Speech Identification\" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team \"Retriv\" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.",
    "authors": [
      "Sourav Saha",
      "K M Nafi Asib",
      "Mohammed Moshiul Hoque"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07304v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07304v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.07192v1",
    "title": "LiteUpdate: A Lightweight Framework for Updating AI-Generated Image   Detectors",
    "summary": "The rapid progress of generative AI has led to the emergence of new generative models, while existing detection methods struggle to keep pace, resulting in significant degradation in the detection performance. This highlights the urgent need for continuously updating AI-generated image detectors to adapt to new generators. To overcome low efficiency and catastrophic forgetting in detector updates, we propose LiteUpdate, a lightweight framework for updating AI-generated image detectors. LiteUpdate employs a representative sample selection module that leverages image confidence and gradient-based discriminative features to precisely select boundary samples. This approach improves learning and detection accuracy on new distributions with limited generated images, significantly enhancing detector update efficiency. Additionally, LiteUpdate incorporates a model merging module that fuses weights from multiple fine-tuning trajectories, including pre-trained, representative, and random updates. This balances the adaptability to new generators and mitigates the catastrophic forgetting of prior knowledge. Experiments demonstrate that LiteUpdate substantially boosts detection performance in various detectors. Specifically, on AIDE, the average detection accuracy on Midjourney improved from 87.63% to 93.03%, a 6.16% relative increase.",
    "authors": [
      "Jiajie Lu",
      "Zhenkan Fu",
      "Na Zhao",
      "Long Xing",
      "Kejiang Chen",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07192v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07192v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.07176v1",
    "title": "Graph Representation-based Model Poisoning on the Heterogeneous Internet   of Agents",
    "summary": "Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.",
    "authors": [
      "Hanlin Cai",
      "Houtianfu Wang",
      "Haofan Dong",
      "Kai Li",
      "Ozgur B. Akan"
    ],
    "categories": [
      "cs.NI",
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07176v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07176v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.07162v1",
    "title": "Categorical Emotions or Appraisals - Which Emotion Model Explains   Argument Convincingness Better?",
    "summary": "The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.",
    "authors": [
      "Lynn Greschner",
      "Meike Bauer",
      "Sabine Weber",
      "Roman Klinger"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07162v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07162v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.07127v1",
    "title": "REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal   Features in Clinical Prognostic Tasks",
    "summary": "Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.",
    "authors": [
      "Linna Wang",
      "Zhixuan You",
      "Qihui Zhang",
      "Jiunan Wen",
      "Ji Shi",
      "Yimin Chen",
      "Yusen Wang",
      "Fanqi Ding",
      "Ziliang Feng",
      "Li Lu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07127v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07127v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.07011v1",
    "title": "Multilingual Lexical Feature Analysis of Spoken Language for Predicting   Major Depression Symptom Severity",
    "summary": "Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.   Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.   Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.   Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.   Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.",
    "authors": [
      "Anastasiia Tokareva",
      "Judith Dineley",
      "Zoe Firth",
      "Pauline Conde",
      "Faith Matcham",
      "Sara Siddi",
      "Femke Lamers",
      "Ewan Carr",
      "Carolin Oetzmann",
      "Daniel Leightley",
      "Yuezhou Zhang",
      "Amos A. Folarin",
      "Josep Maria Haro",
      "Brenda W. J. H. Penninx",
      "Raquel Bailon",
      "Srinivasan Vairavan",
      "Til Wykes",
      "Richard J. B. Dobson",
      "Vaibhav A. Narayan",
      "Matthew Hotopf",
      "Nicholas Cummins",
      "The RADAR-CNS Consortium"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07011v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07011v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.07397v1",
    "title": "ConvFill: Model Collaboration for Responsive Conversational Voice Agents",
    "summary": "Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.",
    "authors": [
      "Vidya Srinivas",
      "Zachary Englhardt",
      "Maximus Powers",
      "Shwetak Patel",
      "Vikram Iyer"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07397v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07397v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07325v1",
    "title": "Garbage Vulnerable Point Monitoring using IoT and Computer Vision",
    "summary": "This paper proposes a smart way to manage municipal solid waste by using the Internet of Things (IoT) and computer vision (CV) to monitor illegal waste dumping at garbage vulnerable points (GVPs) in urban areas. The system can quickly detect and monitor dumped waste using a street-level camera and object detection algorithm. Data was collected from the Sangareddy district in Telangana, India. A series of comprehensive experiments was carried out using the proposed dataset to assess the accuracy and overall performance of various object detection models. Specifically, we performed an in-depth evaluation of YOLOv8, YOLOv10, YOLO11m, and RT-DETR on our dataset. Among these models, YOLO11m achieved the highest accuracy of 92.39\\% in waste detection, demonstrating its effectiveness in detecting waste. Additionally, it attains an mAP@50 of 0.91, highlighting its high precision. These findings confirm that the object detection model is well-suited for monitoring and tracking waste dumping events at GVP locations. Furthermore, the system effectively captures waste disposal patterns, including hourly, daily, and weekly dumping trends, ensuring comprehensive daily and nightly monitoring.",
    "authors": [
      "R. Kumar",
      "A. Lall",
      "S. Chaudhari",
      "M. Kale",
      "A. Vattem"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07325v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07325v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07313v1",
    "title": "De-Individualizing fMRI Signals via Mahalanobis Whitening and Bures   Geometry",
    "summary": "Functional connectivity has been widely investigated to understand brain disease in clinical studies and imaging-based neuroscience, and analyzing changes in functional connectivity has proven to be valuable for understanding and computationally evaluating the effects on brain function caused by diseases or experimental stimuli. By using Mahalanobis data whitening prior to the use of dimensionality reduction algorithms, we are able to distill meaningful information from fMRI signals about subjects and the experimental stimuli used to prompt them. Furthermore, we offer an interpretation of Mahalanobis whitening as a two-stage de-individualization of data which is motivated by similarity as captured by the Bures distance, which is connected to quantum mechanics. These methods have potential to aid discoveries about the mechanisms that link brain function with cognition and behavior and may improve the accuracy and consistency of Alzheimer's diagnosis, especially in the preclinical stage of disease progression.",
    "authors": [
      "Aaron Jacobson",
      "Tingting Dan",
      "Martin Styner",
      "Guorong Wu",
      "Shahar Kovalsky",
      "Caroline Moosmueller"
    ],
    "categories": [
      "q-bio.NC",
      "cs.LG",
      "q-bio.QM",
      "92-08, 68T10, 62H310",
      "I.5.3; J.3"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07313v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07313v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07097v1",
    "title": "Agentic AI Sustainability Assessment for Supply Chain Document Insights",
    "summary": "This paper presents a comprehensive sustainability assessment framework for document intelligence within supply chain operations, centered on agentic artificial intelligence (AI). We address the dual objective of improving automation efficiency while providing measurable environmental performance in document-intensive workflows. The research compares three scenarios: fully manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical results show that AI-assisted HITL and agentic AI scenarios achieve reductions of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and 89-98% in water usage compared to manual processes. Notably, full agentic configurations, combining advanced reasoning (thinking mode) and multi-agent validation, achieve substantial sustainability gains over human-only approaches, even when resource usage increases slightly versus simpler AI-assisted solutions. The framework integrates performance, energy, and emission indicators into a unified ESG-oriented methodology for assessing and governing AI-enabled supply chain solutions. The paper includes a complete replicability use case demonstrating the methodology's application to real-world document extraction tasks.",
    "authors": [
      "Diego Gosmar",
      "Anna Chiara Pallotta",
      "Giovanni Zenezini"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07097v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07097v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07086v1",
    "title": "LLM Driven Processes to Foster Explainable AI",
    "summary": "We present a modular, explainable LLM-agent pipeline for decision support that externalizes reasoning into auditable artifacts. The system instantiates three frameworks: Vester's Sensitivity Model (factor set, signed impact matrix, systemic roles, feedback loops); normal-form games (strategies, payoff matrix, equilibria); and sequential games (role-conditioned agents, tree construction, backward induction), with swappable modules at every step. LLM components (default: GPT-5) are paired with deterministic analyzers for equilibria and matrix-based role classification, yielding traceable intermediates rather than opaque outputs. In a real-world logistics case (100 runs), mean factor alignment with a human baseline was 55.5\\% over 26 factors and 62.9\\% on the transport-core subset; role agreement over matches was 57\\%. An LLM judge using an eight-criterion rubric (max 100) scored runs on par with a reconstructed human baseline. Configurable LLM pipelines can thus mimic expert workflows with transparent, inspectable steps.",
    "authors": [
      "Marcel Pehlke",
      "Marc Jansen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07086v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07086v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07017v1",
    "title": "Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in   Practice",
    "summary": "Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.   We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.   ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.",
    "authors": [
      "Ruida Hu",
      "Xinchen Wang",
      "Xin-Cheng Wen",
      "Zhao Zhang",
      "Bo Jiang",
      "Pengfei Gao",
      "Chao Peng",
      "Cuiyun Gao"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07017v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07017v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07009v1",
    "title": "Performance Decay in Deepfake Detection: The Limitations of Training on   Outdated Data",
    "summary": "The continually advancing quality of deepfake technology exacerbates the threats of disinformation, fraud, and harassment by making maliciously-generated synthetic content increasingly difficult to distinguish from reality. We introduce a simple yet effective two-stage detection method that achieves an AUROC of over 99.8% on contemporary deepfakes. However, this high performance is short-lived. We show that models trained on this data suffer a recall drop of over 30% when evaluated on deepfakes created with generation techniques from just six months later, demonstrating significant decay as threats evolve. Our analysis reveals two key insights for robust detection. Firstly, continued performance requires the ongoing curation of large, diverse datasets. Second, predictive power comes primarily from static, frame-level artifacts, not temporal inconsistencies. The future of effective deepfake detection therefore depends on rapid data collection and the development of advanced frame-level feature detectors.",
    "authors": [
      "Jack Richings",
      "Margaux Leblanc",
      "Ian Groves",
      "Victoria Nockles"
    ],
    "categories": [
      "cs.CV",
      "68T07, 68T45"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07009v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07009v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.07292v1",
    "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving",
    "summary": "Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.",
    "authors": [
      "Simon Gerstenecker",
      "Andreas Geiger",
      "Katrin Renz"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07292v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07292v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.07272v1",
    "title": "Understanding the role of depth in the neural tangent kernel for   overparameterized neural networks",
    "summary": "Overparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.",
    "authors": [
      "William St-Arnaud",
      "Margarida Carvalho",
      "Golnoosh Farnadi"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07272v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07272v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.07270v1",
    "title": "High-Dimensional Asymptotics of Differentially Private PCA",
    "summary": "In differential privacy, statistics of a sensitive dataset are privatized by introducing random noise. Most privacy analyses provide privacy bounds specifying a noise level sufficient to achieve a target privacy guarantee. Sometimes, these bounds are pessimistic and suggest adding excessive noise, which overwhelms the meaningful signal. It remains unclear if such high noise levels are truly necessary or a limitation of the proof techniques. This paper explores whether we can obtain sharp privacy characterizations that identify the smallest noise level required to achieve a target privacy level for a given mechanism. We study this problem in the context of differentially private principal component analysis, where the goal is to privatize the leading principal components (PCs) of a dataset with n samples and p features. We analyze the exponential mechanism for this problem in a model-free setting and provide sharp utility and privacy characterizations in the high-dimensional limit ($p\\rightarrow\\infty$). Our privacy result shows that, in high dimensions, detecting the presence of a target individual in the dataset using the privatized PCs is exactly as hard as distinguishing two Gaussians with slightly different means, where the mean difference depends on certain spectral properties of the dataset. Our privacy analysis combines the hypothesis-testing formulation of privacy guarantees proposed by Dong, Roth, and Su (2022) with classical contiguity arguments due to Le Cam to obtain sharp high-dimensional privacy characterizations.",
    "authors": [
      "Youngjoo Yun",
      "Rishabh Dudeja"
    ],
    "categories": [
      "math.ST",
      "cs.IT",
      "cs.LG",
      "math.IT",
      "math.PR",
      "stat.ML",
      "stat.TH"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07270v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07270v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.07001v1",
    "title": "SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright   Infringement in LLMs",
    "summary": "Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.",
    "authors": [
      "Zhenliang Zhang",
      "Xinyu Hu",
      "Xiaojun Wan"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07001v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07001v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.06906v1",
    "title": "Counterfactual Explanation for Multivariate Time Series Forecasting with   Exogenous Variables",
    "summary": "Currently, machine learning is widely used across various domains, including time series data analysis. However, some machine learning models function as black boxes, making interpretability a critical concern. One approach to address this issue is counterfactual explanation (CE), which aims to provide insights into model predictions. This study focuses on the relatively underexplored problem of generating counterfactual explanations for time series forecasting. We propose a method for extracting CEs in time series forecasting using exogenous variables, which are frequently encountered in fields such as business and marketing. In addition, we present methods for analyzing the influence of each variable over an entire time series, generating CEs by altering only specific variables, and evaluating the quality of the resulting CEs. We validate the proposed method through theoretical analysis and empirical experiments, showcasing its accuracy and practical applicability. These contributions are expected to support real-world decision-making based on time series data analysis.",
    "authors": [
      "Keita Kinjo"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 62M10, 62P20",
      "I.2.6; I.5.1; I.5.4"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06906v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06906v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.07412v1",
    "title": "TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for   Embodied AI Research",
    "summary": "Developing embodied AI for intelligent surgical systems requires safe, controllable environments for continual learning and evaluation. However, safety regulations and operational constraints in operating rooms (ORs) limit embodied agents from freely perceiving and interacting in realistic settings. Digital twins provide high-fidelity, risk-free environments for exploration and training. How we may create photorealistic and dynamic digital representations of ORs that capture relevant spatial, visual, and behavioral complexity remains unclear. We introduce TwinOR, a framework for constructing photorealistic, dynamic digital twins of ORs for embodied AI research. The system reconstructs static geometry from pre-scan videos and continuously models human and equipment motion through multi-view perception of OR activities. The static and dynamic components are fused into an immersive 3D environment that supports controllable simulation and embodied exploration. The proposed framework reconstructs complete OR geometry with centimeter level accuracy while preserving dynamic interaction across surgical workflows, enabling realistic renderings and a virtual playground for embodied AI systems. In our experiments, TwinOR simulates stereo and monocular sensor streams for geometry understanding and visual localization tasks. Models such as FoundationStereo and ORB-SLAM3 on TwinOR-synthesized data achieve performance within their reported accuracy on real indoor datasets, demonstrating that TwinOR provides sensor-level realism sufficient for perception and localization challenges. By establishing a real-to-sim pipeline for constructing dynamic, photorealistic digital twins of OR environments, TwinOR enables the safe, scalable, and data-efficient development and benchmarking of embodied AI, ultimately accelerating the deployment of embodied AI from sim-to-real.",
    "authors": [
      "Han Zhang",
      "Yiqing Shen",
      "Roger D. Soberanis-Mukul",
      "Ankita Ghosh",
      "Hao Ding",
      "Lalithkumar Seenivasan",
      "Jose L. Porras",
      "Zhekai Mao",
      "Chenjia Li",
      "Wenjie Xiao",
      "Lonny Yarmus",
      "Angela Christine Argento",
      "Masaru Ishii",
      "Mathias Unberath"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07412v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07412v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07410v1",
    "title": "Using Vision Language Models as Closed-Loop Symbolic Planners for   Robotic Applications: A Control-Theoretic Perspective",
    "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.",
    "authors": [
      "Hao Wang",
      "Sathwik Karnik",
      "Bea Lim",
      "Somil Bansal"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07410v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07410v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07365v1",
    "title": "Private Sketches for Linear Regression",
    "summary": "Linear regression is frequently applied in a variety of domains. In order to improve the efficiency of these methods, various methods have been developed that compute summaries or \\emph{sketches} of the datasets. Certain domains, however, contain sensitive data which necessitates that the application of these statistical methods does not reveal private information. Differentially private (DP) linear regression methods have been developed for mitigating this problem. These techniques typically involve estimating a noisy version of the parameter vector. Instead, we propose releasing private sketches of the datasets. We present differentially private sketches for the problems of least squares regression, as well as least absolute deviations regression. The availability of these private sketches facilitates the application of commonly available solvers for regression, without the risk of privacy leakage.",
    "authors": [
      "Shrutimoy Das",
      "Debanuj Nayak",
      "Anirban Dasgupta"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07365v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07365v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07298v1",
    "title": "LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging",
    "summary": "Low-dose computed tomography (CT) represents a significant improvement in patient safety through lower radiation doses, but increased noise, blur, and contrast loss can diminish diagnostic quality. Therefore, consistency and robustness in image quality assessment become essential for clinical applications. In this study, we propose an LLM-based quality assessment system that generates both numerical scores and textual descriptions of degradations such as noise, blur, and contrast loss. Furthermore, various inference strategies - from the zero-shot approach to metadata integration and error feedback - are systematically examined, demonstrating the progressive contribution of each method to overall performance. The resultant assessments yield not only highly correlated scores but also interpretable output, thereby adding value to clinical workflows. The source codes of our study are available at https://github.com/itu-biai/lmms_ldct_iqa.",
    "authors": [
      "Kagan Celik",
      "Mehmet Ozan Unal",
      "Metin Ertas",
      "Isa Yildirim"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07298v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07298v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07277v1",
    "title": "Designing Beyond Language: Sociotechnical Barriers in AI Health   Technologies for Limited English Proficiency",
    "summary": "Limited English proficiency (LEP) patients in the U.S. face systemic barriers to healthcare beyond language and interpreter access, encompassing procedural and institutional constraints. AI advances may support communication and care through on-demand translation and visit preparation, but also risk exacerbating existing inequalities. We conducted storyboard-driven interviews with 14 patient navigators to explore how AI could shape care experiences for Spanish-speaking LEP individuals. We identified tensions around linguistic and cultural misunderstandings, privacy concerns, and opportunities and risks for AI to augment care workflows. Participants highlighted structural factors that can undermine trust in AI systems, including sensitive information disclosure, unstable technology access, and low digital literacy. While AI tools can potentially alleviate social barriers and institutional constraints, there are risks of misinformation and uprooting human camaraderie. Our findings contribute design considerations for AI that support LEP patients and care teams via rapport-building, education, and language support, and minimizing disruptions to existing practices.",
    "authors": [
      "Michelle Huang",
      "Violeta J. Rodriguez",
      "Koustuv Saha",
      "Tal August"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07277v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07277v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07139v1",
    "title": "Trading Vector Data in Vector Databases",
    "summary": "Vector data trading is essential for cross-domain learning with vector databases, yet it remains largely unexplored. We study this problem under online learning, where sellers face uncertain retrieval costs and buyers provide stochastic feedback to posted prices. Three main challenges arise: (1) heterogeneous and partial feedback in configuration learning, (2) variable and complex feedback in pricing learning, and (3) inherent coupling between configuration and pricing decisions.   We propose a hierarchical bandit framework that jointly optimizes retrieval configurations and pricing. Stage I employs contextual clustering with confidence-based exploration to learn effective configurations with logarithmic regret. Stage II adopts interval-based price selection with local Taylor approximation to estimate buyer responses and achieve sublinear regret. We establish theoretical guarantees with polynomial time complexity and validate the framework on four real-world datasets, demonstrating consistent improvements in cumulative reward and regret reduction compared with existing methods.",
    "authors": [
      "Jin Cheng",
      "Xiangxiang Dai",
      "Ningning Ding",
      "John C. S. Lui",
      "Jianwei Huang"
    ],
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07139v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07139v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07125v1",
    "title": "Towards a Rigorous Understanding of the Population Dynamics of the   NSGA-III: Tight Runtime Bounds",
    "summary": "Evolutionary algorithms are widely used for solving multi-objective optimization problems. A prominent example is NSGA-III, which is particularly well suited for solving problems involving more than three objectives, distinguishing it from the classical NSGA-II. Despite its empirical success, the theoretical understanding of NSGA III remains very limited, especially with respect to runtime analysis. A central open problem concerns its population dynamics, which involve controlling the maximum number of individuals sharing the same fitness value during the exploration process. In this paper, we make a significant step towards such an understanding by proving tight runtime bounds for NSGA-III on the bi-objective OneMinMax ($2$-OMM) problem. Firstly, we prove that NSGA-III requires $\\Omega(n^2 \\log(n) / \\mu)$ generations in expectation to optimize $2$-OMM assuming the population size $\\mu$ satisfies $n+1 \\leq \\mu =O(\\log(n)^c(n+1))$ where $n$ denotes the problem size and $c<1$ is a constant. Apart from~\\cite{opris2025multimodal}, this is the first proven lower runtime bound for NSGA-III on a classical benchmark problem. Complementing this, we secondly improve the best known upper bound of NSGA-III on the $m$-objective OneMinMax problem ($m$-OMM) of $O(n \\log(n))$ generations by a factor of $\\mu /(2n/m + 1)^{m/2}$ for a constant number $m$ of objectives and population size $(2n/m + 1)^{m/2} \\leq \\mu \\in O(\\sqrt{\\log(n)} (2n/m + 1)^{m/2})$. This yields tight runtime bounds in the case $m = 2$, and the surprising result that NSGA-III beats NSGA-II by a factor of $\\mu/n$ in the expected runtime.",
    "authors": [
      "Andre Opris"
    ],
    "categories": [
      "cs.NE",
      "68W05, 68Q25, 68W20, 68W50, 68T20",
      "F.2.2"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07125v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07125v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07122v1",
    "title": "Sparse4DGS: 4D Gaussian Splatting for Sparse-Frame Dynamic Scene   Reconstruction",
    "summary": "Dynamic Gaussian Splatting approaches have achieved remarkable performance for 4D scene reconstruction. However, these approaches rely on dense-frame video sequences for photorealistic reconstruction. In real-world scenarios, due to equipment constraints, sometimes only sparse frames are accessible. In this paper, we propose Sparse4DGS, the first method for sparse-frame dynamic scene reconstruction. We observe that dynamic reconstruction methods fail in both canonical and deformed spaces under sparse-frame settings, especially in areas with high texture richness. Sparse4DGS tackles this challenge by focusing on texture-rich areas. For the deformation network, we propose Texture-Aware Deformation Regularization, which introduces a texture-based depth alignment loss to regulate Gaussian deformation. For the canonical Gaussian field, we introduce Texture-Aware Canonical Optimization, which incorporates texture-based noise into the gradient descent process of canonical Gaussians. Extensive experiments show that when taking sparse frames as inputs, our method outperforms existing dynamic or few-shot techniques on NeRF-Synthetic, HyperNeRF, NeRF-DS, and our iPhone-4D datasets.",
    "authors": [
      "Changyue Shi",
      "Chuxiao Yang",
      "Xinyuan Hu",
      "Minghao Chen",
      "Wenwen Pan",
      "Yan Yang",
      "Jiajun Ding",
      "Zhou Yu",
      "Jun Yu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07122v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07122v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.06967v1",
    "title": "Approximate Bayesian inference for cumulative probit regression models",
    "summary": "Ordinal categorical data are routinely encountered in a wide range of practical applications. When the primary goal is to construct a regression model for ordinal outcomes, cumulative link models represent one of the most popular choices to link the cumulative probabilities of the response with a set of covariates through a parsimonious linear predictor, shared across response categories. When the number of observations grows, standard sampling algorithms for Bayesian inference scale poorly, making posterior computation increasingly challenging in large datasets. In this article, we propose three scalable algorithms for approximating the posterior distribution of the regression coefficients in cumulative probit models relying on Variational Bayes and Expectation Propagation. We compare the proposed approaches with inference based on Markov Chain Monte Carlo, demonstrating superior computational performance and remarkable accuracy; finally, we illustrate the utility of the proposed algorithms on a challenging case study to investigate the structure of a criminal network.",
    "authors": [
      "Emanuele Aliverti"
    ],
    "categories": [
      "stat.ME",
      "stat.CO",
      "stat.ML"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06967v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06967v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.06925v1",
    "title": "DTTNet: Improving Video Shadow Detection via Dark-Aware Guidance and   Tokenized Temporal Modeling",
    "summary": "Video shadow detection confronts two entwined difficulties: distinguishing shadows from complex backgrounds and modeling dynamic shadow deformations under varying illumination. To address shadow-background ambiguity, we leverage linguistic priors through the proposed Vision-language Match Module (VMM) and a Dark-aware Semantic Block (DSB), extracting text-guided features to explicitly differentiate shadows from dark objects. Furthermore, we introduce adaptive mask reweighting to downweight penumbra regions during training and apply edge masks at the final decoder stage for better supervision. For temporal modeling of variable shadow shapes, we propose a Tokenized Temporal Block (TTB) that decouples spatiotemporal learning. TTB summarizes cross-frame shadow semantics into learnable temporal tokens, enabling efficient sequence encoding with minimal computation overhead. Comprehensive Experiments on multiple benchmark datasets demonstrate state-of-the-art accuracy and real-time inference efficiency. Codes are available at https://github.com/city-cheng/DTTNet.",
    "authors": [
      "Zhicheng Li",
      "Kunyang Sun",
      "Rui Yao",
      "Hancheng Zhu",
      "Fuyuan Hu",
      "Jiaqi Zhao",
      "Zhiwen Shao",
      "Yong Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06925v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06925v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.07413v1",
    "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
    "summary": "AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.",
    "authors": [
      "Yuxuan Sun",
      "Manchen Wang",
      "Shengyi Qian",
      "William R. Wong",
      "Eric Gan",
      "Pierluca D'Oro",
      "Alejandro Castillejo Munoz",
      "Sneha Silwal",
      "Pedro Matias",
      "Nitin Kamra",
      "Satwik Kottur",
      "Nick Raines",
      "Xuanyi Zhao",
      "Joy Chen",
      "Joseph Greer",
      "Andrea Madotto",
      "Allen Bolourchi",
      "James Valori",
      "Kevin Carlberg",
      "Karl Ridgeway",
      "Joseph Tighe"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07413v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07413v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.07384v1",
    "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted   Recurrence",
    "summary": "Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.",
    "authors": [
      "Sean McLeish",
      "Ang Li",
      "John Kirchenbauer",
      "Dayal Singh Kalra",
      "Brian R. Bartoldson",
      "Bhavya Kailkhura",
      "Avi Schwarzschild",
      "Jonas Geiping",
      "Tom Goldstein",
      "Micah Goldblum"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07384v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07384v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.07312v1",
    "title": "Superhuman AI for Stratego Using Self-Play Reinforcement Learning and   Test-Time Search",
    "summary": "Few classical games have been regarded as such significant benchmarks of artificial intelligence as to have justified training costs in the millions of dollars. Among these, Stratego -- a board wargame exemplifying the challenge of strategic decision making under massive amounts of hidden information -- stands apart as a case where such efforts failed to produce performance at the level of top humans. This work establishes a step change in both performance and cost for Stratego, showing that it is now possible not only to reach the level of top humans, but to achieve vastly superhuman level -- and that doing so requires not an industrial budget, but merely a few thousand dollars. We achieved this result by developing general approaches for self-play reinforcement learning and test-time search under imperfect information.",
    "authors": [
      "Samuel Sokota",
      "Eugene Vinitsky",
      "Hengyuan Hu",
      "J. Zico Kolter",
      "Gabriele Farina"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07312v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07312v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.07223v1",
    "title": "NoteEx: Interactive Visual Context Manipulation for LLM-Assisted   Exploratory Data Analysis in Computational Notebooks",
    "summary": "Computational notebooks have become popular for Exploratory Data Analysis (EDA), augmented by LLM-based code generation and result interpretation. Effective LLM assistance hinges on selecting informative context -- the minimal set of cells whose code, data, or outputs suffice to answer a prompt. As notebooks grow long and messy, users can lose track of the mental model of their analysis. They thus fail to curate appropriate contexts for LLM tasks, causing frustration and tedious prompt engineering. We conducted a formative study (n=6) that surfaced challenges in LLM context selection and mental model maintenance. Therefore, we introduce NoteEx, a JupyterLab extension that provides a semantic visualization of the EDA workflow, allowing analysts to externalize their mental model, specify analysis dependencies, and enable interactive selection of task-relevant contexts for LLMs. A user study (n=12) against a baseline shows that NoteEx improved mental model retention and context selection, leading to more accurate and relevant LLM responses.",
    "authors": [
      "Mohammad Hasan Payandeh",
      "Lin-Ping Yuan",
      "Jian Zhao"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07223v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07223v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.07202v1",
    "title": "Resilient by Design - Active Inference for Distributed Continuum   Intelligence",
    "summary": "Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free-energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework.",
    "authors": [
      "Praveen Kumar Donta",
      "Alfreds Lapkovskis",
      "Enzo Mingozzi",
      "Schahram Dustdar"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.MA",
      "cs.NI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07202v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07202v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.07163v1",
    "title": "Combining digital data streams and epidemic networks for real time   outbreak detection",
    "summary": "Responding to disease outbreaks requires close surveillance of their trajectories, but outbreak detection is hindered by the high noise in epidemic time series. Aggregating information across data sources has shown great denoising ability in other fields, but remains underexplored in epidemiology. Here, we present LRTrend, an interpretable machine learning framework to identify outbreaks in real time. LRTrend effectively aggregates diverse health and behavioral data streams within one region and learns disease-specific epidemic networks to aggregate information across regions. We reveal diverse epidemic clusters and connections across the United States that are not well explained by commonly used human mobility networks and may be informative for future public health coordination. We apply LRTrend to 2 years of COVID-19 data in 305 hospital referral regions and frequently detect regional Delta and Omicron waves within 2 weeks of the outbreak's start, when case counts are a small fraction of the wave's resulting peak.",
    "authors": [
      "Ruiqi Lyu",
      "Alistair Turcan",
      "Bryan Wilder"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07163v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07163v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.07414v1",
    "title": "Wasserstein-Cramér-Rao Theory of Unbiased Estimation",
    "summary": "The quantity of interest in the classical Cram\\'er-Rao theory of unbiased estimation (e.g., the Cram\\'er-Rao lower bound, its exact attainment for exponential families, and asymptotic efficiency of maximum likelihood estimation) is the variance, which represents the instability of an estimator when its value is compared to the value for an independently-sampled data set from the same distribution. In this paper we are interested in a quantity which represents the instability of an estimator when its value is compared to the value for an infinitesimal additive perturbation of the original data set; we refer to this as the \"sensitivity\" of an estimator. The resulting theory of sensitivity is based on the Wasserstein geometry in the same way that the classical theory of variance is based on the Fisher-Rao (equivalently, Hellinger) geometry, and this insight allows us to determine a collection of results which are analogous to the classical case: a Wasserstein-Cram\\'er-Rao lower bound for the sensitivity of any unbiased estimator, a characterization of models in which there exist unbiased estimators achieving the lower bound exactly, and some concrete results that show that the Wasserstein projection estimator achieves the lower bound asymptotically. We use these results to treat many statistical examples, sometimes revealing new optimality properties for existing estimators and other times revealing entirely new estimators.",
    "authors": [
      "Nicolás García Trillos",
      "Adam Quinn Jaffe",
      "Bodhisattva Sen"
    ],
    "categories": [
      "math.ST",
      "math.OC",
      "stat.ME",
      "stat.ML",
      "stat.TH",
      "62B11, 62F10, 62F12, 35Q49, 49Q22"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07414v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07414v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.07278v1",
    "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache   Retrieval and Compression",
    "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant potential in the areas of video captioning, search, and summarization. However, current Video-LLMs still face challenges with long real-world videos. Recent methods have introduced a retrieval mechanism that retrieves query-relevant KV caches for question answering, enhancing the efficiency and accuracy of long real-world videos. However, the compression and retrieval of KV caches are still not fully explored. In this paper, we propose \\textbf{StreamKV}, a training-free framework that seamlessly equips Video-LLMs with advanced KV cache retrieval and compression. Compared to previous methods that used uniform partitioning, StreamKV dynamically partitions video streams into semantic segments, which better preserves semantic information. For KV cache retrieval, StreamKV calculates a summary vector for each segment to retain segment-level information essential for retrieval. For KV cache compression, StreamKV introduces a guidance prompt designed to capture the key semantic elements within each segment, ensuring only the most informative KV caches are retained for answering questions. Moreover, StreamKV unifies KV cache retrieval and compression within a single module, performing both in a layer-adaptive manner, thereby further improving the effectiveness of streaming video question answering. Extensive experiments on public StreamingVQA benchmarks demonstrate that StreamKV significantly outperforms existing Online Video-LLMs, achieving superior accuracy while substantially improving both memory efficiency and computational latency. The code has been released at https://github.com/sou1p0wer/StreamKV.",
    "authors": [
      "Yilong Chen",
      "Xiang Bai",
      "Zhibin Wang",
      "Chengyu Bai",
      "Yuhan Dai",
      "Ming Lu",
      "Shanghang Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07278v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07278v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.07161v1",
    "title": "LLMscape",
    "summary": "LLMscape is an interactive installation that investigates how humans and AI construct meaning under shared conditions of uncertainty. Within a mutable, projection-mapped landscape, human participants reshape the world and engage with multiple AI agents, each developing incomplete and provisional accounts of their environment. Exhibited in Shanghai and continually evolving, the work positions AI not as deterministic tools but as embodied co-witnesses to an unstable world, examining the parallels between human and artificial meaning-making and inviting reflection on our shared epistemic limits.",
    "authors": [
      "Gottfried Haider",
      "Jie Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07161v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07161v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.07094v1",
    "title": "Task-Adaptive Low-Dose CT Reconstruction",
    "summary": "Deep learning-based low-dose computed tomography reconstruction methods already achieve high performance on standard image quality metrics like peak signal-to-noise ratio and structural similarity index measure. Yet, they frequently fail to preserve the critical anatomical details needed for diagnostic tasks. This fundamental limitation hinders their clinical applicability despite their high metric scores. We propose a novel task-adaptive reconstruction framework that addresses this gap by incorporating a frozen pre-trained task network as a regularization term in the reconstruction loss function. Unlike existing joint-training approaches that simultaneously optimize both reconstruction and task networks, and risk diverging from satisfactory reconstructions, our method leverages a pre-trained task model to guide reconstruction training while still maintaining diagnostic quality. We validate our framework on a liver and liver tumor segmentation task. Our task-adaptive models achieve Dice scores up to 0.707, approaching the performance of full-dose scans (0.874), and substantially outperforming joint-training approaches (0.331) and traditional reconstruction methods (0.626). Critically, our framework can be integrated into any existing deep learning-based reconstruction model through simple loss function modification, enabling widespread adoption for task-adaptive optimization in clinical practice. Our codes are available at: https://github.com/itu-biai/task_adaptive_ct",
    "authors": [
      "Necati Sefercioglu",
      "Mehmet Ozan Unal",
      "Metin Ertas",
      "Isa Yildirim"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07094v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07094v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.07067v1",
    "title": "RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent   Diffusion",
    "summary": "Millimeter-wave radar offers a promising sensing modality for autonomous systems thanks to its robustness in adverse conditions and low cost. However, its utility is significantly limited by the sparsity and low resolution of radar point clouds, which poses challenges for tasks requiring dense and accurate 3D perception. Despite that recent efforts have shown great potential by exploring generative approaches to address this issue, they often rely on dense voxel representations that are inefficient and struggle to preserve structural detail. To fill this gap, we make the key observation that latent diffusion models (LDMs), though successful in other modalities, have not been effectively leveraged for radar-based 3D generation due to a lack of compatible representations and conditioning strategies. We introduce RaLD, a framework that bridges this gap by integrating scene-level frustum-based LiDAR autoencoding, order-invariant latent representations, and direct radar spectrum conditioning. These insights lead to a more compact and expressive generation process. Experiments show that RaLD produces dense and accurate 3D point clouds from raw radar spectrums, offering a promising solution for robust perception in challenging environments.",
    "authors": [
      "Ruijie Zhang",
      "Bixin Zeng",
      "Shengpeng Wang",
      "Fuhui Zhou",
      "Wei Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07067v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07067v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.07046v1",
    "title": "Learning Quantized Continuous Controllers for Integer Hardware",
    "summary": "Deploying continuous-control reinforcement learning policies on embedded hardware requires meeting tight latency and power budgets. Small FPGAs can deliver these, but only if costly floating point pipelines are avoided. We study quantization-aware training (QAT) of policies for integer inference and we present a learning-to-hardware pipeline that automatically selects low-bit policies and synthesizes them to an Artix-7 FPGA. Across five MuJoCo tasks, we obtain policy networks that are competitive with full precision (FP32) policies but require as few as 3 or even only 2 bits per weight, and per internal activation value, as long as input precision is chosen carefully. On the target hardware, the selected policies achieve inference latencies on the order of microseconds and consume microjoules per action, favorably comparing to a quantized reference. Last, we observe that the quantized policies exhibit increased input noise robustness compared to the floating-point baseline.",
    "authors": [
      "Fabian Kresse",
      "Christoph H. Lampert"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07046v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07046v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.07002v1",
    "title": "Automated Circuit Interpretation via Probe Prompting",
    "summary": "Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.   Across five prompts including classic \"capitals\" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.   We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.",
    "authors": [
      "Giuseppe Birardi"
    ],
    "categories": [
      "cs.CL",
      "I.2.0; I.2.6; I.2.7; I.2.4"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07002v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07002v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.06918v1",
    "title": "Proceedings of the 2025 XCSP3 Competition",
    "summary": "This document represents the proceedings of the 2025 XCSP3 Competition. The results of this competition of constraint solvers were presented at CP'25 (31st International Conference on Principles and Practice of Constraint Programming).",
    "authors": [
      "Gilles Audemard",
      "Christophe Lecoutre",
      "Emmanuel Lonca"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06918v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06918v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.06901v1",
    "title": "Classification of Microplastic Particles in Water using Polarized Light   Scattering and Machine Learning Methods",
    "summary": "Facing the critical need for continuous, large-scale microplastic monitoring, which is hindered by the limitations of gold-standard methods in aquatic environments, this paper introduces and validates a novel, reflection-based approach for the in-situ classification and identification of microplastics directly in water bodies, which is based on polarized light scattering. In this experiment, we classify colorless microplastic particles (50-300 $\\mu$m) by illuminating them with linearly polarized laser light and capturing their reflected signals using a polarization-sensitive camera. This reflection-based technique successfully circumvents the transmission-based interference issues that plague many conventional methods when applied in water. Using a deep convolutional neural network (CNN) for image-based classification, we successfully identified three common polymer types, high-density polyethylene, low-density polyethylene, and polypropylene, achieving a peak mean classification accuracy of 80% on the test dataset. A subsequent feature hierarchy analysis demonstrated that the CNN's decision-making process relies mainly on the microstructural integrity and internal texture (polarization patterns) of the particle rather than its macroshape. Critically, we found that the Angle of Linear Polarization (AOLP) signal is significantly more robust against contextual noise than the Degree of Linear Polarization (DOLP) signal. While the AOLP-based classification achieved superior overall performance, its strength lies in distinguishing between the two polyethylene plastics, showing a lower confusion rate between high-density and low-density polyethylene. Conversely, the DOLP signal demonstrated slightly worse overall classification results but excels at accurately identifying the polypropylene class, which it isolated with greater success than AOLP.",
    "authors": [
      "Leonard Saur",
      "Marc von Pawlowski",
      "Ulrich Gengenbach",
      "Ingo Sieber",
      "Hossein Shirali",
      "Lorenz Wührl",
      "Rainer Kiko",
      "Christian Pylatiuk"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06901v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06901v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.07409v1",
    "title": "DIMO: Diverse 3D Motion Generation for Arbitrary Objects",
    "summary": "We present DIMO, a generative approach capable of generating diverse 3D motions for arbitrary objects from a single image. The core idea of our work is to leverage the rich priors in well-trained video models to extract the common motion patterns and then embed them into a shared low-dimensional latent space. Specifically, we first generate multiple videos of the same object with diverse motions. We then embed each motion into a latent vector and train a shared motion decoder to learn the distribution of motions represented by a structured and compact motion representation, i.e., neural key point trajectories. The canonical 3D Gaussians are then driven by these key points and fused to model the geometry and appearance. During inference time with learned latent space, we can instantly sample diverse 3D motions in a single-forward pass and support several interesting applications including 3D motion interpolation and language-guided motion generation. Our project page is available at https://linzhanm.github.io/dimo.",
    "authors": [
      "Linzhan Mou",
      "Jiahui Lei",
      "Chen Wang",
      "Lingjie Liu",
      "Kostas Daniilidis"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07409v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07409v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.07004v1",
    "title": "Exploring the \"Great Unseen\" in Medieval Manuscripts: Instance-Level   Labeling of Legacy Image Collections with Zero-Shot Models",
    "summary": "We aim to theorize the medieval manuscript page and its contents more holistically, using state-of-the-art techniques to segment and describe the entire manuscript folio, for the purpose of creating richer training data for computer vision techniques, namely instance segmentation, and multimodal models for medieval-specific visual content.",
    "authors": [
      "Christofer Meinecke",
      "Estelle Guéville",
      "David Joseph Wrisley"
    ],
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07004v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07004v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.06953v1",
    "title": "GFix: Perceptually Enhanced Gaussian Splatting Video Compression",
    "summary": "3D Gaussian Splatting (3DGS) enhances 3D scene reconstruction through explicit representation and fast rendering, demonstrating potential benefits for various low-level vision tasks, including video compression. However, existing 3DGS-based video codecs generally exhibit more noticeable visual artifacts and relatively low compression ratios. In this paper, we specifically target the perceptual enhancement of 3DGS-based video compression, based on the assumption that artifacts from 3DGS rendering and quantization resemble noisy latents sampled during diffusion training. Building on this premise, we propose a content-adaptive framework, GFix, comprising a streamlined, single-step diffusion model that serves as an off-the-shelf neural enhancer. Moreover, to increase compression efficiency, We propose a modulated LoRA scheme that freezes the low-rank decompositions and modulates the intermediate hidden states, thereby achieving efficient adaptation of the diffusion backbone with highly compressible updates. Experimental results show that GFix delivers strong perceptual quality enhancement, outperforming GSVC with up to 72.1% BD-rate savings in LPIPS and 21.4% in FID.",
    "authors": [
      "Siyue Teng",
      "Ge Gao",
      "Duolikun Danier",
      "Yuxuan Jiang",
      "Fan Zhang",
      "Thomas Davis",
      "Zoe Liu",
      "David Bull"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.06953v1",
    "pdf_url": "https://arxiv.org/pdf/2511.06953v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.07231v1",
    "title": "Mapping Reduced Accessibility to WASH Facilities in Rohingya Refugee   Camps with Sub-Meter Imagery",
    "summary": "Access to Water, Sanitation, and Hygiene (WASH) services remains a major public health concern in refugee camps. This study introduces a remote sensing-driven framework to quantify WASH accessibility-specifically to water pumps, latrines, and bathing cubicles-in the Rohingya camps of Cox's Bazar, one of the world's most densely populated displacement settings. Detecting refugee shelters in such emergent camps presents substantial challenges, primarily due to their dense spatial configuration and irregular geometric patterns. Using sub-meter satellite images, we develop a semi-supervised segmentation framework that achieves an F1-score of 76.4% in detecting individual refugee shelters. Applying the framework across multi-year data reveals declining WASH accessibility, driven by rapid refugee population growth and reduced facility availability, rising from 25 people per facility in 2022 to 29.4 in 2025. Gender-disaggregated analysis further shows that women and girls experience reduced accessibility, in scenarios with inadequate safety-related segregation in WASH facilities. These findings suggest the importance of demand-responsive allocation strategies that can identify areas with under-served populations-such as women and girls-and ensure that limited infrastructure serves the greatest number of people in settings with fixed or shrinking budgets. We also discuss the value of high-resolution remote sensing and machine learning to detect inequality and inform equitable resource planning in complex humanitarian environments.",
    "authors": [
      "Kyeongjin Ahn",
      "YongHun Suh",
      "Sungwon Han",
      "Jeasurk Yang",
      "Hannes Taubenböck",
      "Meeyoung Cha"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07231v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07231v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2511.07206v1",
    "title": "Geometric implicit neural representations for signed distance functions",
    "summary": "\\textit{Implicit neural representations} (INRs) have emerged as a promising framework for representing signals in low-dimensional spaces. This survey reviews the existing literature on the specialized INR problem of approximating \\textit{signed distance functions} (SDFs) for surface scenes, using either oriented point clouds or a set of posed images. We refer to neural SDFs that incorporate differential geometry tools, such as normals and curvatures, in their loss functions as \\textit{geometric} INRs. The key idea behind this 3D reconstruction approach is to include additional \\textit{regularization} terms in the loss function, ensuring that the INR satisfies certain global properties that the function should hold -- such as having unit gradient in the case of SDFs. We explore key methodological components, including the definition of INR, the construction of geometric loss functions, and sampling schemes from a differential geometry perspective. Our review highlights the significant advancements enabled by geometric INRs in surface reconstruction from oriented point clouds and posed images.",
    "authors": [
      "Luiz Schirmer",
      "Tiago Novello",
      "Vinícius da Silva",
      "Guilherme Schardong",
      "Daniel Perazzo",
      "Hélio Lopes",
      "Nuno Gonçalves",
      "Luiz Velho"
    ],
    "categories": [
      "cs.CV",
      "cs.CG",
      "cs.GR"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07206v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07206v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.07199v1",
    "title": "Automated Estimation of Anatomical Risk Metrics for Endoscopic Sinus   Surgery Using Deep Learning",
    "summary": "Endoscopic sinus surgery requires careful preoperative assessment of the skull base anatomy to minimize risks such as cerebrospinal fluid leakage. Anatomical risk scores like the Keros, Gera and Thailand-Malaysia-Singapore score offer a standardized approach but require time-consuming manual measurements on coronal CT or CBCT scans. We propose an automated deep learning pipeline that estimates these risk scores by localizing key anatomical landmarks via heatmap regression. We compare a direct approach to a specialized global-to-local learning strategy and find mean absolute errors on the relevant anatomical measurements of 0.506mm for the Keros, 4.516{\\deg} for the Gera and 0.802mm / 0.777mm for the TMS classification.",
    "authors": [
      "Konrad Reuter",
      "Lennart Thaysen",
      "Bilkay Doruk",
      "Sarah Latus",
      "Brigitte Holst",
      "Benjamin Becker",
      "Dennis Eggert",
      "Christian Betz",
      "Anna-Sophie Hoffmann",
      "Alexander Schlaefer"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07199v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07199v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.07261v1",
    "title": "High-dimensional Bayesian filtering through deep density approximation",
    "summary": "In this work, we benchmark two recently developed deep density methods for nonlinear filtering. Starting from the Fokker--Planck equation with Bayes updates, we model the filtering density of a discretely observed SDE. The two filters: the deep splitting filter and the deep BSDE filter, are both based on Feynman--Kac formulas, Euler--Maruyama discretizations and neural networks. The two methods are extended to logarithmic formulations providing sound and robust implementations in increasing state dimension. Comparing to the classical particle filters and ensemble Kalman filters, we benchmark the methods on numerous examples. In the low-dimensional examples the particle filters work well, but when we scale up to a partially observed 100-dimensional Lorenz-96 model the particle-based methods fail and the logarithmic deep density method prevails. In terms of computational efficiency, the deep density methods reduce inference time by roughly two to five orders of magnitude relative to the particle-based filters.",
    "authors": [
      "Kasper Bågmark",
      "Filip Rydin"
    ],
    "categories": [
      "math.NA",
      "cs.NA",
      "stat.CO",
      "stat.ML",
      "60G25, 60G35, 62F15, 62G07, 62M20, 65C30, 65M75, 68T07"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07261v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07261v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2511.07150v1",
    "title": "Improved Runtime Guarantees for the SPEA2 Multi-Objective Optimizer",
    "summary": "Together with the NSGA-II, the SPEA2 is one of the most widely used domination-based multi-objective evolutionary algorithms. For both algorithms, the known runtime guarantees are linear in the population size; for the NSGA-II, matching lower bounds exist. With a careful study of the more complex selection mechanism of the SPEA2, we show that it has very different population dynamics. From these, we prove runtime guarantees for the OneMinMax, LeadingOnesTrailingZeros, and OneJumpZeroJump benchmarks that depend less on the population size. For example, we show that the SPEA2 with parent population size $\\mu \\ge n - 2k + 3$ and offspring population size $\\lambda$ computes the Pareto front of the OneJumpZeroJump benchmark with gap size $k$ in an expected number of $O( (\\lambda+\\mu)n + n^{k+1})$ function evaluations. This shows that the best runtime guarantee of $O(n^{k+1})$ is not only achieved for $\\mu = \\Theta(n)$ and $\\lambda = O(n)$ but for arbitrary $\\mu, \\lambda = O(n^k)$. Thus, choosing suitable parameters -- a key challenge in using heuristic algorithms -- is much easier for the SPEA2 than the NSGA-II.",
    "authors": [
      "Benjamin Doerr",
      "Martin S. Krejca",
      "Milan Stanković"
    ],
    "categories": [
      "cs.NE"
    ],
    "published": "2025-11-10",
    "url": "https://arxiv.org/abs/2511.07150v1",
    "pdf_url": "https://arxiv.org/pdf/2511.07150v1.pdf",
    "date": "2025-11-11",
    "source": "arxiv",
    "research_score": 0.43
  }
]