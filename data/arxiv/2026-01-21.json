[
  {
    "arxiv_id": "2601.14235v1",
    "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration",
    "summary": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC's primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.",
    "authors": [
      " LSST Dark Energy Science Collaboration",
      "Eric Aubourg",
      "Camille Avestruz",
      "Matthew R. Becker",
      "Biswajit Biswas",
      "Rahul Biswas",
      "Boris Bolliet",
      "Adam S. Bolton",
      "Clecio R. Bom",
      "Raphaël Bonnet-Guerrini",
      "Alexandre Boucaud",
      "Jean-Eric Campagne",
      "Chihway Chang",
      "Aleksandra Ćiprijanović",
      "Johann Cohen-Tanugi",
      "Michael W. Coughlin",
      "John Franklin Crenshaw",
      "Juan C. Cuevas-Tello",
      "Juan de Vicente",
      "Seth W. Digel",
      "Steven Dillmann",
      "Mariano Javier de León Dominguez Romero",
      "Alex Drlica-Wagner",
      "Sydney Erickson",
      "Alexander T. Gagliano",
      "Christos Georgiou",
      "Aritra Ghosh",
      "Matthew Grayling",
      "Kirill A. Grishin",
      "Alan Heavens",
      "Lindsay R. House",
      "Mustapha Ishak",
      "Wassim Kabalan",
      "Arun Kannawadi",
      "François Lanusse",
      "C. Danielle Leonard",
      "Pierre-François Léget",
      "Michelle Lochner",
      "Yao-Yuan Mao",
      "Peter Melchior",
      "Grant Merz",
      "Martin Millon",
      "Anais Möller",
      "Gautham Narayan",
      "Yuuki Omori",
      "Hiranya Peiris",
      "Laurence Perreault-Levasseur",
      "Andrés A. Plazas Malagón",
      "Nesar Ramachandra",
      "Benjamin Remy",
      "Cécile Roucelle",
      "Jaime Ruiz-Zapatero",
      "Stefan Schuldt",
      "Ignacio Sevilla-Noarbe",
      "Ved G. Shah",
      "Tjitske Starkenburg",
      "Stephen Thorp",
      "Laura Toribio San Cipriano",
      "Tilman Tröster",
      "Roberto Trotta",
      "Padma Venkatraman",
      "Amanda Wasserman",
      "Tim White",
      "Justine Zeghal",
      "Tianqing Zhang",
      "Yuanyuan Zhang"
    ],
    "categories": [
      "astro-ph.IM",
      "astro-ph.CO",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14235v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14235v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2601.14092v1",
    "title": "Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning",
    "summary": "Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.",
    "authors": [
      "Babacar Toure",
      "Dimitrios Tsilimantos",
      "Omid Esrafilian",
      "Marios Kountouris"
    ],
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14092v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14092v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.86
  },
  {
    "arxiv_id": "2601.14154v1",
    "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery",
    "summary": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.",
    "authors": [
      "Shubham Pandey",
      "Bhavin Jawade",
      "Srirangaraj Setlur",
      "Venu Govindaraju",
      "Kenneth Seastedt"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14154v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14154v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.85
  },
  {
    "arxiv_id": "2601.13671v1",
    "title": "The Orchestration of Multi-Agent Systems: Architectures, Protocols, and Enterprise Adoption",
    "summary": "Orchestrated multi-agent systems represent the next stage in the evolution of artificial intelligence, where autonomous agents collaborate through structured coordination and communication to achieve complex, shared objectives. This paper consolidates and formalizes the technical composition of such systems, presenting a unified architectural framework that integrates planning, policy enforcement, state management, and quality operations into a coherent orchestration layer. Another primary contribution of this work is the in-depth technical delineation of two complementary communication protocols - the Model Context Protocol, which standardizes how agents access external tools and contextual data, and the Agent2Agent protocol, which governs peer coordination, negotiation, and delegation. Together, these protocols establish an interoperable communication substrate that enables scalable, auditable, and policy-compliant reasoning across distributed agent collectives. Beyond protocol design, the paper details how orchestration logic, governance frameworks, and observability mechanisms collectively sustain system coherence, transparency, and accountability. By synthesizing these elements into a cohesive technical blueprint, this paper provides comprehensive treatments of orchestrated multi-agent systems - bridging conceptual architectures with implementation-ready design principles for enterprise-scale AI ecosystems.",
    "authors": [
      "Apoorva Adimulam",
      "Rajesh Gupta",
      "Sumit Kumar"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13671v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13671v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.85
  },
  {
    "arxiv_id": "2601.13809v1",
    "title": "DroneVLA: VLA based Aerial Manipulation",
    "summary": "As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.",
    "authors": [
      "Fawad Mehboob",
      "Monijesu James",
      "Amir Habel",
      "Jeffrin Sam",
      "Miguel Altamirano Cabrera",
      "Dzmitry Tsetserukou"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13809v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13809v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.84
  },
  {
    "arxiv_id": "2601.13824v1",
    "title": "ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware Hierarchical Federated Learning over Resource-Constrained Edge Networks",
    "summary": "Training large language models (LLMs) at the network edge faces fundamental challenges arising from device resource constraints, severe data heterogeneity, and heightened privacy risks. To address these, we propose ELSA (Efficient LLM-centric Split Aggregation), a novel framework that systematically integrates split learning (SL) and hierarchical federated learning (HFL) for distributed LLM fine-tuning over resource-constrained edge networks. ELSA introduces three key innovations. First, it employs a task-agnostic, behavior-aware client clustering mechanism that constructs semantic fingerprints using public probe inputs and symmetric KL divergence, further enhanced by prediction-consistency-based trust scoring and latency-aware edge assignment to jointly address data heterogeneity, client unreliability, and communication constraints. Second, it splits the LLM into three parts across clients and edge servers, with the cloud used only for adapter aggregation, enabling an effective balance between on-device computation cost and global convergence stability. Third, it incorporates a lightweight communication scheme based on computational sketches combined with semantic subspace orthogonal perturbation (SS-OP) to reduce communication overhead while mitigating privacy leakage during model exchanges. Experiments across diverse NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art methods in terms of adaptability, convergence behavior, and robustness, establishing a scalable and privacy-aware solution for edge-side LLM fine-tuning under resource constraints.",
    "authors": [
      "Xiaohong Yang",
      "Tong Xie",
      "Minghui Liwang",
      "Chikai Shang",
      "Yang Lu",
      "Zhenzhen Jiao",
      "Liqun Fu",
      "Seyyedali Hosseinalipour"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13824v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13824v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2601.13742v1",
    "title": "Dimension-First Evaluation of Speech-to-Speech Models with Structured Acoustic Cues",
    "summary": "Large Language Model (LLM) judges exhibit strong reasoning capabilities but are limited to textual content. This leaves current automatic Speech-to-Speech (S2S) evaluation methods reliant on opaque and expensive Audio Language Models (ALMs). In this work, we propose TRACE (Textual Reasoning over Audio Cues for Evaluation), a novel framework that enables LLM judges to reason over audio cues to achieve cost-efficient and human-aligned S2S evaluation. To demonstrate the strength of the framework, we first introduce a Human Chain-of-Thought (HCoT) annotation protocol to improve the diagnostic capability of existing judge benchmarks by separating evaluation into explicit dimensions: content (C), voice quality (VQ), and paralinguistics (P). Using this data, TRACE constructs a textual blueprint of inexpensive audio signals and prompts an LLM to render dimension-wise judgments, fusing them into an overall rating via a deterministic policy. TRACE achieves higher agreement with human raters than ALMs and transcript-only LLM judges while being significantly more cost-effective. We will release the HCoT annotations and the TRACE framework to enable scalable and human-aligned S2S evaluation.",
    "authors": [
      "Arjun Chandra",
      "Kevin Miller",
      "Venkatesh Ravichandran",
      "Constantinos Papayiannis",
      "Venkatesh Saligrama"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13742v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13742v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2601.13719v1",
    "title": "Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search",
    "summary": "Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.",
    "authors": [
      "Xinlei Yin",
      "Xiulian Peng",
      "Xiao Li",
      "Zhiwei Xiong",
      "Yan Lu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13719v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13719v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2601.14175v1",
    "title": "A model of errors in transformers",
    "summary": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory'' perspective: the LLM's many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning'', or an inability to express ``compositional'' functions. Finally, we show how to construct prompts to reduce the error rate.",
    "authors": [
      "Suvrat Raju",
      "Praneeth Netrapalli"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "hep-th"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14175v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14175v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2601.14079v1",
    "title": "VENI: Variational Encoder for Natural Illumination",
    "summary": "Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.",
    "authors": [
      "Paul Walker",
      "James A. D. Gardner",
      "Andreea Ardelean",
      "William A. P. Smith",
      "Bernhard Egger"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14079v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14079v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2601.14234v1",
    "title": "Q-learning with Adjoint Matching",
    "summary": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic's action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.",
    "authors": [
      "Qiyang Li",
      "Sergey Levine"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14234v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14234v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2601.13919v1",
    "title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs",
    "summary": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker",
    "authors": [
      "Yuezhe Yang",
      "Hao Wang",
      "Yige Peng",
      "Jinman Kim",
      "Lei Bi"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13919v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13919v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2601.14243v1",
    "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.",
    "authors": [
      "Haocheng Xi",
      "Charlie Ruan",
      "Peiyuan Liao",
      "Yujun Lin",
      "Han Cai",
      "Yilong Zhao",
      "Shuo Yang",
      "Kurt Keutzer",
      "Song Han",
      "Ligeng Zhu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14243v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14243v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2601.13711v1",
    "title": "GerAV: Towards New Heights in German Authorship Verification using Fine-Tuned LLMs on a New Benchmark",
    "summary": "Authorship verification (AV) is the task of determining whether two texts were written by the same author and has been studied extensively, predominantly for English data. In contrast, large-scale benchmarks and systematic evaluations for other languages remain scarce. We address this gap by introducing GerAV, a comprehensive benchmark for German AV comprising over 600k labeled text pairs. GerAV is built from Twitter and Reddit data, with the Reddit part further divided into in-domain and cross-domain message-based subsets, as well as a profile-based subset. This design enables controlled analysis of the effects of data source, topical domain, and text length. Using the provided training splits, we conduct a systematic evaluation of strong baselines and state-of-the-art models and find that our best approach, a fine-tuned large language model, outperforms recent baselines by up to 0.09 absolute F1 score and surpasses GPT-5 in a zero-shot setting by 0.08. We further observe a trade-off between specialization and generalization: models trained on specific data types perform best under matching conditions but generalize less well across data regimes, a limitation that can be mitigated by combining training sources. Overall, GerAV provides a challenging and versatile benchmark for advancing research on German and cross-domain AV.",
    "authors": [
      "Lotta Kiefer",
      "Christoph Leiter",
      "Sotaro Takeshita",
      "Elena Schmidt",
      "Steffen Eger"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13711v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13711v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2601.13707v1",
    "title": "Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs",
    "summary": "Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model's internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model's representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.",
    "authors": [
      "Yujin Jo",
      "Sangyoon Bae",
      "Taesup Kim"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13707v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13707v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2601.14099v1",
    "title": "Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping",
    "summary": "Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.",
    "authors": [
      "Shi-Shun Chen",
      "Xiao-Yang Li",
      "Enrico Zio"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14099v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14099v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2601.13793v1",
    "title": "PAtt: A Pattern Attention Network for ETA Prediction Using Historical Speed Profiles",
    "summary": "In this paper, we propose an ETA model (Estimated Time of Arrival) that leverages an attention mechanism over historical road speed patterns. As autonomous driving and intelligent transportation systems become increasingly prevalent, the need for accurate and reliable ETA estimation has grown, playing a vital role in navigation, mobility planning, and traffic management. However, predicting ETA remains a challenging task due to the dynamic and complex nature of traffic flow. Traditional methods often combine real-time and historical traffic data in simplistic ways, or rely on complex rule-based computations. While recent deep learning models have shown potential, they often require high computational costs and do not effectively capture the spatio-temporal patterns crucial for ETA prediction. ETA prediction inherently involves spatio-temporal causality, and our proposed model addresses this by leveraging attention mechanisms to extract and utilize temporal features accumulated at each spatio-temporal point along a route. This architecture enables efficient and accurate ETA estimation while keeping the model lightweight and scalable. We validate our approach using real-world driving datasets and demonstrate that our approach outperforms existing baselines by effectively integrating road characteristics, real-time traffic conditions, and historical speed patterns in a task-aware manner.",
    "authors": [
      "ByeoungDo Kim",
      "JunYeop Na",
      "Kyungwook Tak",
      "JunTae Kim",
      "DongHyeon Kim",
      "Duckky Kim"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13793v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13793v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2601.13676v1",
    "title": "Autoregressive deep learning for real-time simulation of soft tissue dynamics during virtual neurosurgery",
    "summary": "Accurate simulation of brain deformation is a key component for developing realistic, interactive neurosurgical simulators, as complex nonlinear deformations must be captured to ensure realistic tool-tissue interactions. However, traditional numerical solvers often fall short in meeting real-time performance requirements. To overcome this, we introduce a deep learning-based surrogate model that efficiently simulates transient brain deformation caused by continuous interactions between surgical instruments and the virtual brain geometry. Building on Universal Physics Transformers, our approach operates directly on large-scale mesh data and is trained on an extensive dataset generated from nonlinear finite element simulations, covering a broad spectrum of temporal instrument-tissue interaction scenarios. To reduce the accumulation of errors in autoregressive inference, we propose a stochastic teacher forcing strategy applied during model training. Specifically, training consists of short stochastic rollouts in which the proportion of ground truth inputs is gradually decreased in favor of model-generated predictions. Our results show that the proposed surrogate model achieves accurate and efficient predictions across a range of transient brain deformation scenarios, scaling to meshes with up to 150,000 nodes. The introduced stochastic teacher forcing technique substantially improves long-term rollout stability, reducing the maximum prediction error from 6.7 mm to 3.5 mm. We further integrate the trained surrogate model into an interactive neurosurgical simulation environment, achieving runtimes below 10 ms per simulation step on consumer-grade inference hardware. Our proposed deep learning framework enables rapid, smooth and accurate biomechanical simulations of dynamic brain tissue deformation, laying the foundation for realistic surgical training environments.",
    "authors": [
      "Fabian Greifeneder",
      "Wolfgang Fenz",
      "Benedikt Alkin",
      "Johannes Brandstetter",
      "Michael Giretzlehner",
      "Philipp Moser"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13676v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13676v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2601.13658v1",
    "title": "Beyond Known Facts: Generating Unseen Temporal Knowledge to Address Data Contamination in LLM Evaluation",
    "summary": "The automatic extraction of information is important for populating large web knowledge bases such as Wikidata. The temporal version of that task, temporal knowledge graph extraction (TKGE), involves extracting temporally grounded facts from text, represented as semantic quadruples (subject, relation, object, timestamp). Many recent systems take advantage of large language models (LLMs), which are becoming a new cornerstone of the web due to their performance on many tasks across the natural language processing (NLP) field. Despite the importance of TKGE, existing datasets for training and evaluation remain scarce, and contamination of evaluation data is an unaddressed issue, potentially inflating LLMs' perceived performance due to overlaps between training and evaluation sets. To mitigate these challenges, we propose a novel synthetic evaluation dataset constructed from predicted future, previously unseen temporal facts, thereby eliminating contamination and enabling robust and unbiased benchmarking. Our dataset creation involves a two-step approach: (1) Temporal Knowledge Graph Forecasting (TKGF) generates plausible future quadruples, which are subsequently filtered to adhere to the original knowledge base schema; (2) LLMs perform quadruple-to-text generation, creating semantically aligned textual descriptions. We benchmark Extract, Define and Canonicalize (EDC), a state-of-the-art LLM-based extraction framework, demonstrating that LLM performance decreases when evaluated on our dataset compared to a dataset of known facts. We publicly release our dataset consisting of 4.2K future quadruples and corresponding textual descriptions, along with the generation methodology, enabling continuous creation of unlimited future temporal datasets to serve as long-term, contamination-free benchmarks for TKGE.",
    "authors": [
      "Arthur Amalvy",
      "Hen-Hsen Huang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13658v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13658v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2601.13653v1",
    "title": "TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation",
    "summary": "Time series data widely exist in real-world cyber-physical systems. Though analyzing and interpreting them contributes to significant values, e.g, disaster prediction and financial risk control, current workflows mainly rely on human data scientists, which requires significant labor costs and lacks automation. To tackle this, we introduce TimeART, a framework fusing the analytical capability of strong out-of-the-box tools and the reasoning capability of Large Language Models (LLMs), which serves as a fully agentic data scientist for Time Series Question Answering (TSQA). To teach the LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs' generalization capability, we then devise a four-stage training strategy, which boosts TSRMs through learning from their own early experiences and self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and equip it with the TimeART framework, and it achieves consistent state-of-the-art performance on multiple TSQA tasks, which pioneers a novel approach towards agentic time series reasoning.",
    "authors": [
      "Xingjian Wu",
      "Junkai Lu",
      "Zhengyu Li",
      "Xiangfei Qiu",
      "Jilin Hu",
      "Chenjuan Guo",
      "Christian S. Jensen",
      "Bin Yang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13653v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13653v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2601.13995v1",
    "title": "From Tags to Trees: Structuring Fine-Grained Knowledge for Controllable Data Selection in LLM Instruction Tuning",
    "summary": "Effective and controllable data selection is critical for LLM instruction tuning, especially with massive open-source datasets. Existing approaches primarily rely on instance-level quality scores, or diversity metrics based on embedding clusters or semantic tags. However, constrained by the flatness of embedding spaces or the coarseness of tags, these approaches overlook fine-grained knowledge and its intrinsic hierarchical dependencies, consequently hindering precise data valuation and knowledge-aligned sampling. To address this challenge, we propose Tree-aware Aligned Global Sampling (TAGS), a unified framework that leverages a knowledge tree built from fine-grained tags, thereby enabling joint control of global quality, diversity, and target alignment. Using an LLM-based tagger, we extract atomic knowledge concepts, which are organized into a global tree through bottom-up hierarchical clustering. By grounding data instances onto this tree, a tree-aware metric then quantifies data quality and diversity, facilitating effective sampling. Our controllable sampling strategy maximizes tree-level information gain and enforces leaf-level alignment via KL-divergence for specific domains. Extensive experiments demonstrate that TAGS significantly outperforms state-of-the-art baselines. Notably, it surpasses the full-dataset model by \\textbf{+5.84\\%} using only \\textbf{5\\%} of the data, while our aligned sampling strategy further boosts average performance by \\textbf{+4.24\\%}.",
    "authors": [
      "Zihan Niu",
      "Wenping Hu",
      "Junmin Chen",
      "Xiyue Wang",
      "Tong Xu",
      "Ruiming Tang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13995v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13995v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.13987v1",
    "title": "SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration",
    "summary": "Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE's core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at https://github.com/xuwayyy/SHARE.",
    "authors": [
      "Jiangwei Xie",
      "Zhang Wen",
      "Mike Davies",
      "Dongdong Chen"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13987v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13987v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.13964v1",
    "title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning",
    "summary": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.",
    "authors": [
      "Cheol-Hui Lee",
      "Hwa-Yeon Lee",
      "Dong-Joo Kim"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13964v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13964v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.13926v1",
    "title": "SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field Conditions",
    "summary": "Aortic valve opening (AO) events are crucial for detecting frequency and rhythm disorders, especially in real-world settings where seismocardiography (SCG) signals collected via consumer smartphones are subject to noise, motion artifacts, and variability caused by device heterogeneity. In this work, we present a robust deep-learning framework for SCG segmentation and rhythm analysis using accelerometer recordings obtained with consumer smartphones. We develop an enhanced U-Net v3 architecture that integrates multi-scale convolutions, residual connections, and attention gates, enabling reliable segmentation of noisy SCG signals. A dedicated post-processing pipeline converts probability masks into precise AO timestamps, whereas a novel adaptive 3D-to-1D projection method ensures robustness to arbitrary smartphone orientation. Experimental results demonstrate that the proposed method achieves consistently high accuracy and robustness across various device types and unsupervised data-collection conditions. Our approach enables practical, low-cost, and automated cardiac-rhythm monitoring using everyday mobile devices, paving the way for scalable, field-deployable cardiovascular assessment and future multimodal diagnostic systems.",
    "authors": [
      "Peter Golenderov",
      "Yaroslav Matushenko",
      "Anastasia Tushina",
      "Michal Barodkin"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "eess.SP"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13926v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13926v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.13897v1",
    "title": "TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for Fiber Tractography",
    "summary": "Tractography plays a pivotal role in the non-invasive reconstruction of white matter fiber pathways, providing vital information on brain connectivity and supporting precise neurosurgical planning. Although traditional methods relied mainly on classical deterministic and probabilistic approaches, recent progress has benefited from supervised deep learning (DL) and deep reinforcement learning (DRL) to improve tract reconstruction. A persistent challenge in tractography is accurately reconstructing white matter tracts while minimizing spurious connections. To address this, we propose TractRLFusion, a novel GPT-based policy fusion framework that integrates multiple RL policies through a data-driven fusion strategy. Our method employs a two-stage training data selection process for effective policy fusion, followed by a multi-critic fine-tuning phase to enhance robustness and generalization. Experiments on HCP, ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms individual RL policies as well as state-of-the-art classical and DRL methods in accuracy and anatomical reliability.",
    "authors": [
      "Ankita Joshi",
      "Ashutosh Sharma",
      "Anoushkrit Goel",
      "Ranjeet Ranjan Jha",
      "Chirag Ahuja",
      "Arnav Bhavsar",
      "Aditya Nigam"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13897v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13897v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.13895v1",
    "title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3",
    "summary": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.",
    "authors": [
      "Xu Zhang",
      "Danyang Li",
      "Yingjie Xia",
      "Xiaohang Dong",
      "Hualong Yu",
      "Jianye Wang",
      "Qicheng Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13895v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13895v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.14053v1",
    "title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems",
    "summary": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.",
    "authors": [
      "Badri N. Patro",
      "Vijay S. Agneeswaran"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.MA",
      "eess.IV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14053v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14053v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.14039v1",
    "title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation",
    "summary": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.",
    "authors": [
      "Wesam Moustafa",
      "Hossam Elsafty",
      "Helen Schneider",
      "Lorenz Sparrenberg",
      "Rafet Sifa"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14039v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14039v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.14001v1",
    "title": "Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural Information Retrieval",
    "summary": "Query formulation from internal information needs remains fundamentally challenging across all Information Retrieval paradigms due to cognitive complexity and physical impairments. Brain Passage Retrieval (BPR) addresses this by directly mapping EEG signals to passage representations without intermediate text translation. However, existing BPR research exclusively uses visual stimuli, leaving critical questions unanswered: Can auditory EEG enable effective retrieval for voice-based interfaces and visually impaired users? Can training on combined EEG datasets from different sensory modalities improve performance despite severe data scarcity? We present the first systematic investigation of auditory EEG for BPR and evaluate cross-sensory training benefits. Using dual encoder architectures with four pooling strategies (CLS, mean, max, multi-vector), we conduct controlled experiments comparing auditory-only, visual-only, and combined training on the Alice (auditory) and Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently outperforms visual EEG, and cross-sensory training with CLS pooling achieves substantial improvements over individual training: 31% in MRR (0.474), 43% in Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural queries as competitive with traditional retrieval whilst enabling accessible interfaces. These findings validate auditory neural interfaces for IR tasks and demonstrate that cross-sensory training addresses data scarcity whilst outperforming single-modality approaches Code: https://github.com/NiallMcguire/Audio_BPR",
    "authors": [
      "Niall McGuire",
      "Yashar Moshfeghi"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14001v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14001v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.13780v1",
    "title": "Principled Latent Diffusion for Graphs via Laplacian Autoencoders",
    "summary": "Graph diffusion models achieve state-of-the-art performance in graph generation but suffer from quadratic complexity in the number of nodes -- and much of their capacity is wasted modeling the absence of edges in sparse graphs. Inspired by latent diffusion in other modalities, a natural idea is to compress graphs into a low-dimensional latent space and perform diffusion there. However, unlike images or text, graph generation requires nearly lossless reconstruction, as even a single error in decoding an adjacency matrix can render the entire sample invalid. This challenge has remained largely unaddressed. We propose LG-Flow, a latent graph diffusion framework that directly overcomes these obstacles. A permutation-equivariant autoencoder maps each node into a fixed-dimensional embedding from which the full adjacency is provably recoverable, enabling near-lossless reconstruction for both undirected graphs and DAGs. The dimensionality of this latent representation scales linearly with the number of nodes, eliminating the quadratic bottleneck and making it feasible to train larger and more expressive models. In this latent space, we train a Diffusion Transformer with flow matching, enabling efficient and expressive graph generation. Our approach achieves competitive results against state-of-the-art graph diffusion models, while achieving up to $1000\\times$ speed-up.",
    "authors": [
      "Antoine Siraudin",
      "Christopher Morris"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13780v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13780v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.13693v1",
    "title": "End-to-End Reverse Screening Identifies Protein Targets of Small Molecules Using HelixFold3",
    "summary": "Identifying protein targets for small molecules, or reverse screening, is essential for understanding drug action, guiding compound repurposing, predicting off-target effects, and elucidating the molecular mechanisms of bioactive compounds. Despite its critical role, reverse screening remains challenging because accurately capturing interactions between a small molecule and structurally diverse proteins is inherently complex, and conventional step-wise workflows often propagate errors across decoupled steps such as target structure modeling, pocket identification, docking, and scoring. Here, we present an end-to-end reverse screening strategy leveraging HelixFold3, a high-accuracy biomolecular structure prediction model akin to AlphaFold3, which simultaneously models the folding of proteins from a protein library and the docking of small-molecule ligands within a unified framework. We validate this approach on a diverse and representative set of approximately one hundred small molecules. Compared with conventional reverse docking, our method improves screening accuracy and demonstrates enhanced structural fidelity, binding-site precision, and target prioritization. By systematically linking small molecules to their protein targets, this framework establishes a scalable and straightforward platform for dissecting molecular mechanisms, exploring off-target interactions, and supporting rational drug discovery.",
    "authors": [
      "Shengjie Xu",
      "Xianbin Ye",
      "Mengran Zhu",
      "Xiaonan Zhang",
      "Shanzhuo Zhang",
      "Xiaomin Fang"
    ],
    "categories": [
      "q-bio.BM",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13693v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13693v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.14160v1",
    "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law",
    "summary": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.",
    "authors": [
      "Ali Hamza Bashir",
      "Muhammad Rehan Khalid",
      "Kostadin Cvejoski",
      "Jana Birr",
      "Jule Berghaus",
      "Armin Berger",
      "Sandra Halscheidt",
      "Christian Temath",
      "Rafet Sifa",
      "David Berghaus"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14160v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14160v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.14121v1",
    "title": "NewsRECON: News article REtrieval for image CONtextualization",
    "summary": "Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.",
    "authors": [
      "Jonathan Tonglet",
      "Iryna Gurevych",
      "Tinne Tuytelaars",
      "Marie-Francine Moens"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14121v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14121v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.13918v1",
    "title": "AgentEHR: Advancing Autonomous Clinical Decision-Making via Retrospective Summarization",
    "summary": "Large Language Models have demonstrated profound utility in the medical domain. However, their application to autonomous Electronic Health Records~(EHRs) navigation remains constrained by a reliance on curated inputs and simplified retrieval tasks. To bridge the gap between idealized experimental settings and realistic clinical environments, we present AgentEHR. This benchmark challenges agents to execute complex decision-making tasks, such as diagnosis and treatment planning, requiring long-range interactive reasoning directly within raw and high-noise databases. In tackling these tasks, we identify that existing summarization methods inevitably suffer from critical information loss and fractured reasoning continuity. To address this, we propose RetroSum, a novel framework that unifies a retrospective summarization mechanism with an evolving experience strategy. By dynamically re-evaluating interaction history, the retrospective mechanism prevents long-context information loss and ensures unbroken logical coherence. Additionally, the evolving strategy bridges the domain gap by retrieving accumulated experience from a memory bank. Extensive empirical evaluations demonstrate that RetroSum achieves performance gains of up to 29.16% over competitive baselines, while significantly decreasing total interaction errors by up to 92.3%.",
    "authors": [
      "Yusheng Liao",
      "Chuan Xuan",
      "Yutong Cai",
      "Lina Yang",
      "Zhe Chen",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13918v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13918v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.13684v1",
    "title": "HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache Compression for Long-Context LLM Inference",
    "summary": "The linear memory growth of the KV cache poses a significant bottleneck for LLM inference in long-context tasks. Existing static compression methods often fail to preserve globally important information, principally because they overlook the attention drift phenomenon where token significance evolves dynamically. Although recent dynamic retrieval approaches attempt to address this issue, they typically suffer from coarse-grained caching strategies and incur high I/O overhead due to frequent data transfers. To overcome these limitations, we propose HeteroCache, a training-free dynamic compression framework. Our method is built on two key insights: attention heads exhibit diverse temporal heterogeneity, and there is significant spatial redundancy among heads within the same layer. Guided by these insights, HeteroCache categorizes heads based on stability and redundancy. Consequently, we apply a fine-grained weighting strategy that allocates larger cache budgets to heads with rapidly shifting attention to capture context changes, thereby addressing the inefficiency of coarse-grained strategies. Furthermore, we employ a hierarchical storage mechanism in which a subset of representative heads monitors attention shift, and trigger an asynchronous, on-demand retrieval of contexts from the CPU, effectively hiding I/O latency. Finally, experiments demonstrate that HeteroCache achieves state-of-the-art performance on multiple long-context benchmarks and accelerates decoding by up to $3\\times$ compared to the original model in the 224K context. Our code will be open-source.",
    "authors": [
      "Zhiyuan Shi",
      "Qibo Qiu",
      "Feng Xue",
      "Zhonglin Jiang",
      "Li Yu",
      "Jian Jiang",
      "Xiaofei He",
      "Wenxiao Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13684v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13684v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.14230v1",
    "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems",
    "summary": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.",
    "authors": [
      "Yiyang Wang",
      "Yiqiao Jin",
      "Alex Cabral",
      "Josiah Hester"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14230v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14230v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.14032v1",
    "title": "RM-Distiller: Exploiting Generative LLM for Reward Model Distillation",
    "summary": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. Due to the difficulty of obtaining high-quality human preference annotations, distilling preferences from generative LLMs has emerged as a standard practice. However, existing approaches predominantly treat teacher models as simple binary annotators, failing to fully exploit the rich knowledge and capabilities for RM distillation. To address this, we propose RM-Distiller, a framework designed to systematically exploit the multifaceted capabilities of teacher LLMs: (1) Refinement capability, which synthesizes highly correlated response pairs to create fine-grained and contrastive signals. (2) Scoring capability, which guides the RM in capturing precise preference strength via a margin-aware optimization objective. (3) Generation capability, which incorporates the teacher's generative distribution to regularize the RM to preserve its fundamental linguistic knowledge. Extensive experiments demonstrate that RM-Distiller significantly outperforms traditional distillation methods both on RM benchmarks and reinforcement learning-based alignment, proving that exploiting multifaceted teacher capabilities is critical for effective reward modeling. To the best of our knowledge, this is the first systematic research on RM distillation from generative LLMs.",
    "authors": [
      "Hongli Zhou",
      "Hui Huang",
      "Wei Liu",
      "Chenglong Wang",
      "Xingyuan Bu",
      "Lvyuan Han",
      "Fuhai Song",
      "Muyun Yang",
      "Wenhao Jiang",
      "Hailong Cao",
      "Tiejun Zhao"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14032v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14032v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.14031v1",
    "title": "Intermittent time series forecasting: local vs global models",
    "summary": "Intermittent time series, characterised by the presence of a significant amount of zeros, constitute a large percentage of inventory items in supply chain. Probabilistic forecasts are needed to plan the inventory levels; the predictive distribution should cover non-negative values, have a mass in zero and a long upper tail. Intermittent time series are commonly forecast using local models, which are trained individually on each time series. In the last years global models, which are trained on a large collection of time series, have become popular for time series forecasting. Global models are often based on neural networks. However, they have not yet been exhaustively tested on intermittent time series. We carry out the first study comparing state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR, Transformers) on intermittent time series. For neural networks models we consider three different distribution heads suitable for intermittent time series: negative binomial, hurdle-shifted negative binomial and Tweedie. We use, for the first time, the last two distribution heads with neural networks. We perform experiments on five large datasets comprising more than 40'000 real-world time series. Among neural networks D-Linear provides best accuracy; it also consistently outperforms the local models. Moreover, it has also low computational requirements. Transformers-based architectures are instead much more computationally demanding and less accurate. Among the distribution heads, the Tweedie provides the best estimates of the highest quantiles, while the negative binomial offers overall the best performance.",
    "authors": [
      "Stefano Damato",
      "Nicolò Rubattu",
      "Dario Azzimonti",
      "Giorgio Corani"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14031v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14031v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.13948v1",
    "title": "Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization via Neural Audio Codec and Language Models",
    "summary": "Protecting speaker identity is crucial for online voice applications, yet streaming speaker anonymization (SA) remains underexplored. Recent research has demonstrated that neural audio codec (NAC) provides superior speaker feature disentanglement and linguistic fidelity. NAC can also be used with causal language models (LM) to enhance linguistic fidelity and prompt control for streaming tasks. However, existing NAC-based online LM systems are designed for voice conversion (VC) rather than anonymization, lacking the techniques required for privacy protection. Building on these advances, we present Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures specifically for streaming SA by integrating anonymization techniques. Our anonymization approach incorporates pseudo-speaker representation sampling, a speaker embedding mixing and diverse prompt selection strategies for LM conditioning that leverage the disentanglement properties of quantized content codes to prevent speaker information leakage. Additionally, we compare dynamic and fixed delay configurations to explore latency-privacy trade-offs in real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol, Stream-Voice-Anon achieves substantial improvements in intelligibility (up to 46% relative WER reduction) and emotion preservation (up to 28% UAR relative) compared to the previous state-of-the-art streaming method DarkStream while maintaining comparable latency (180ms vs 200ms) and privacy protection against lazy-informed attackers, though showing 15% relative degradation against semi-informed attackers.",
    "authors": [
      "Nikita Kuzmin",
      "Songting Liu",
      "Kong Aik Lee",
      "Eng Siong Chng"
    ],
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13948v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13948v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.13806v1",
    "title": "Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning",
    "summary": "LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \\textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.",
    "authors": [
      "Dezhao Song",
      "Guglielmo Bonifazi",
      "Frank Schilder",
      "Jonathan Richard Schwarz"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13806v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13806v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.13761v1",
    "title": "DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution",
    "summary": "Self-play with large language models has emerged as a promising paradigm for achieving self-improving artificial intelligence. However, existing self-play frameworks often suffer from optimization instability, due to (i) non-stationary objectives induced by solver-dependent reward feedback for the Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels used to supervise the Solver. To mitigate these challenges, we introduce DARC (Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that stabilizes the self-evolution process. First, we train the Questioner to synthesize difficulty-calibrated questions, conditioned on explicit difficulty levels and external corpora. Second, we train the Solver with an asymmetric self-distillation mechanism, where a document-augmented teacher generates high-quality pseudo-labels to supervise the student Solver that lacks document access. Empirical results demonstrate that DARC is model-agnostic, yielding an average improvement of 10.9 points across nine reasoning benchmarks and three backbone models. Moreover, DARC consistently outperforms all baselines and approaches the performance of fully supervised models without relying on human annotations.The code is available at https://github.com/RUCBM/DARC.",
    "authors": [
      "Shengda Fan",
      "Xuyan Ye",
      "Yankai Lin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13761v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13761v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.13734v1",
    "title": "Towards robust long-context understanding of large language model via active recap learning",
    "summary": "In this paper, we propose active recap learning (ARL), a framework for enhancing large language model (LLM) in understanding long contexts. ARL enables models to revisit and summarize earlier content through targeted sequence construction during contined pretraining and retrospective summarization at inference. First, we identify key tokens in prepared long context based on loss gaps between long and short forward contexts and find most revant preceding paragraphs, then summarize them using an LLM. Second, ARL equips models with the ability to autonomously generate and utilize these retrospective summaries during inference, thereby establishing a recursive memory mechanism across paragraphs. Experimental results show substantial gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement on LongBench. Overall, ARL offers a simple yet effective continued pretraining-based approach to strengthen long-context understanding, advancing scalable memory augmentation in LLM",
    "authors": [
      "Chenyu Hui"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13734v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13734v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.13708v1",
    "title": "Generative Adversarial Networks for Resource State Generation",
    "summary": "We introduce a physics-informed Generative Adversarial Network framework that recasts quantum resource-state generation as an inverse-design task. By embedding task-specific utility functions into training, the model learns to generate valid two-qubit states optimized for teleportation and entanglement broadcasting. Comparing decomposition-based and direct-generation architectures reveals that structural enforcement of Hermiticity, trace-one, and positivity yields higher fidelity and training stability than loss-only approaches. The framework reproduces theoretical resource boundaries for Werner-like and Bell-diagonal states with fidelities exceeding ~98%, establishing adversarial learning as a lightweight yet effective method for constraint-driven quantum-state discovery. This approach provides a scalable foundation for automated design of tailored quantum resources for information-processing applications, exemplified with teleportation and broadcasting of entanglement, and it opens up the possibility of using such states in efficient quantum network design.",
    "authors": [
      "Shahbaz Shaik",
      "Sourav Chatterjee",
      "Sayantan Pramanik",
      "Indranil Chakrabarty"
    ],
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13708v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13708v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.14172v1",
    "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
    "summary": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (\"does any value appear?\") and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.",
    "authors": [
      "Víctor Yeste",
      "Paolo Rosso"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14172v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14172v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.14051v1",
    "title": "Kakugo: Distillation of Low-Resource Languages into Small Language Models",
    "summary": "We present Kakugo, a novel and cost-effective pipeline designed to train general-purpose Small Language Models (SLMs) for low-resource languages using only the language name as input. By using a large teacher model to generate synthetic prompts and translate instruction datasets, we produced training data and SLMs for 54 low-resource languages. Evaluations across a diverse set of general natural language processing tasks, including translation, classification, and question answering, demonstrate that our pipeline consistently improves performance over base models. With a total generation and training cost of under $50 per language, Kakugo offers an accessible method for communities to develop language-specific AI.",
    "authors": [
      "Peter Devine",
      "Mardhiyah Sanni",
      "Farid Adilazuarda",
      "Julieta Gil Loizaga",
      "Barry Haddow"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14051v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14051v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.14042v1",
    "title": "Federated Balanced Learning",
    "summary": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.",
    "authors": [
      "Jiaze Li",
      "Haoran Xu",
      "Wanyi Wu",
      "Changwei Wang",
      "Shuaiguang Li",
      "Jianzhong Ju",
      "Zhenbo Luo",
      "Jian Luan",
      "Youyang Qu",
      "Longxiang Gao",
      "Xudong Yang",
      "Lumin Xing"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14042v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14042v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.14033v1",
    "title": "PAC-Private Responses with Adversarial Composition",
    "summary": "Modern machine learning models are increasingly deployed behind APIs. This renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy at the cost of utility. While model weights may vary significantly across training datasets, model responses to specific inputs are much lower dimensional and more stable. This motivates enforcing privacy guarantees directly on model outputs.   We approach this under PAC privacy, which provides instance-based privacy guarantees for arbitrary black-box functions by controlling mutual information (MI). Importantly, PAC privacy explicitly rewards output stability with reduced noise levels. However, a central challenge remains: response privacy requires composing a large number of adaptively chosen, potentially adversarial queries issued by untrusted users, where existing composition results on PAC privacy are inadequate. We introduce a new algorithm that achieves adversarial composition via adaptive noise calibration and prove that mutual information guarantees accumulate linearly under adaptive and adversarial querying.   Experiments across tabular, vision, and NLP tasks show that our method achieves high utility at extremely small per-query privacy budgets. On CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$. This enables serving one million queries while provably bounding membership inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04, 10^{-5})$-DP. Furthermore, we show that private responses can be used to label public data to distill a publishable privacy-preserving model; using an ImageNet subset as a public dataset, our model distilled from 210,000 responses achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%, which is comparable to $(0.02,10^{-5})$-DP.",
    "authors": [
      "Xiaochen Zhu",
      "Mayuri Sridhar",
      "Srinivas Devadas"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14033v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14033v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.13942v1",
    "title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning",
    "summary": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.",
    "authors": [
      "Hongbo Bai",
      "Yujin Zhou",
      "Yile Wu",
      "Chi-Min Chan",
      "Pengcheng Wen",
      "Kunhao Pan",
      "Sirui Han",
      "Yike Guo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13942v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13942v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.13836v1",
    "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs",
    "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).",
    "authors": [
      "Qian Chen",
      "Jinlan Fu",
      "Changsong Li",
      "See-Kiong Ng",
      "Xipeng Qiu"
    ],
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13836v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13836v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.14127v1",
    "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
    "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.",
    "authors": [
      "Renmiao Chen",
      "Yida Lu",
      "Shiyao Cui",
      "Xuan Ouyang",
      "Victor Shea-Jay Huang",
      "Shumin Zhang",
      "Chengwei Pan",
      "Han Qiu",
      "Minlie Huang"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14127v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14127v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.14060v1",
    "title": "Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration",
    "summary": "Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.",
    "authors": [
      "Yongcong Ye",
      "Kai Zhang",
      "Yanghai Zhang",
      "Enhong Chen",
      "Longfei Li",
      "Jun Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14060v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14060v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.13953v1",
    "title": "Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition",
    "summary": "Learning precise Boolean logic via gradient descent remains challenging: neural networks typically converge to \"fuzzy\" approximations that degrade under quantization. We introduce Hierarchical Spectral Composition, a differentiable architecture that selects spectral coefficients from a frozen Boolean Fourier basis and composes them via Sinkhorn-constrained routing with column-sign modulation. Our approach draws on recent insights from Manifold-Constrained Hyper-Connections (mHC), which demonstrated that projecting routing matrices onto the Birkhoff polytope preserves identity mappings and stabilizes large-scale training. We adapt this framework to logic synthesis, adding column-sign modulation to enable Boolean negation -- a capability absent in standard doubly stochastic routing.   We validate our approach across four phases of increasing complexity: (1) For n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100% accuracy with zero routing drift and zero-loss quantization to ternary masks. (2) For n=3 (10 three-variable operations), gradient descent achieves 76% accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that optimal ternary masks exist for all operations (100% accuracy, 39% sparsity). (3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis -- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC refinement with parallel tempering -- achieves 100% accuracy on all operations. This progression establishes (a) that ternary polynomial threshold representations exist for all tested functions, and (b) that finding them requires methods beyond pure gradient descent as dimensionality grows. All operations enable single-cycle combinational logic inference at 10,959 MOps/s on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic synthesis.",
    "authors": [
      "Gorgi Pavlov"
    ],
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.LO"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13953v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13953v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.13879v1",
    "title": "Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring",
    "summary": "While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.",
    "authors": [
      "Dongxu Zhang",
      "Yiding Sun",
      "Cheng Tan",
      "Wenbiao Yan",
      "Ning Yang",
      "Jihua Zhu",
      "Hiajun Zhang"
    ],
    "categories": [
      "cs.MM",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13879v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13879v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.13768v1",
    "title": "vLinear: A Powerful Linear Model for Multivariate Time Series Forecasting",
    "summary": "In this paper, we present \\textbf{vLinear}, an effective yet efficient \\textbf{linear}-based multivariate time series forecaster featuring two components: the \\textbf{v}ecTrans module and the WFMLoss objective. Many state-of-the-art forecasters rely on self-attention or its variants to capture multivariate correlations, typically incurring $\\mathcal{O}(N^2)$ computational complexity with respect to the number of variates $N$. To address this, we propose vecTrans, a lightweight module that utilizes a learnable vector to model multivariate correlations, reducing the complexity to $\\mathcal{O}(N)$. Notably, vecTrans can be seamlessly integrated into Transformer-based forecasters, delivering up to 5$\\times$ inference speedups and consistent performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching Loss) as the objective. In contrast to typical \\textbf{velocity-oriented} flow matching objectives, we demonstrate that a \\textbf{final-series-oriented} formulation yields significantly superior forecasting accuracy. WFMLoss also incorporates path- and horizon-weighted strategies to focus learning on more reliable paths and horizons. Empirically, vLinear achieves state-of-the-art performance across 22 benchmarks and 124 forecasting settings. Moreover, WFMLoss serves as an effective plug-and-play objective, consistently improving existing forecasters. The code is available at https://anonymous.4open.science/r/vLinear.",
    "authors": [
      "Wenzhen Yue",
      "Ruohao Guo",
      "Ji Shi",
      "Zihan Hao",
      "Shiyu Hu",
      "Xianghua Ying"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13768v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13768v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.13748v1",
    "title": "EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention and Neural Memory",
    "summary": "Accurate epileptic seizure prediction from electroencephalography (EEG) remains challenging because pre-ictal dynamics may span long time horizons while clinically relevant signatures can be subtle and transient. Many deep learning models face a persistent trade-off between capturing local spatiotemporal patterns and maintaining informative long-range context when operating on ultralong sequences. We propose EEG-Titans, a dualbranch architecture that incorporates a modern neural memory mechanism for long-context modeling. The model combines sliding-window attention to capture short-term anomalies with a recurrent memory pathway that summarizes slower, progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under a chronological holdout protocol, EEG-Titans achieves 99.46% average segment-level sensitivity across 18 subjects. We further analyze safety-first operating points on artifact-prone recordings and show that a hierarchical context strategy extending the receptive field for high-noise subjects can markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without sacrificing sensitivity. These results indicate that memory-augmented long-context modeling can provide robust seizure forecasting under clinically constrained evaluation",
    "authors": [
      "Tien-Dat Pham",
      "Xuan-The Tran"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13748v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13748v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.13729v1",
    "title": "On Temperature-Constrained Non-Deterministic Machine Translation: Potential and Evaluation",
    "summary": "In recent years, the non-deterministic properties of language models have garnered considerable attention and have shown a significant influence on real-world applications. However, such properties remain under-explored in machine translation (MT), a complex, non-deterministic NLP task. In this study, we systematically evaluate modern MT systems and identify temperature-constrained Non-Deterministic MT (ND-MT) as a distinct phenomenon. Additionally, we demonstrate that ND-MT exhibits significant potential in addressing the multi-modality issue that has long challenged MT research and provides higher-quality candidates than Deterministic MT (D-MT) under temperature constraints. However, ND-MT introduces new challenges in evaluating system performance. Specifically, the evaluation framework designed for D-MT fails to yield consistent evaluation results when applied to ND-MT. We further investigate this emerging challenge by evaluating five state-of-the-art ND-MT systems across three open datasets using both lexical-based and semantic-based metrics at varying sampling sizes. The results reveal a Buckets effect across these systems: the lowest-quality candidate generated by ND-MT consistently determines the overall system ranking across different sampling sizes for all reasonable metrics. Furthermore, we propose the ExpectoSample strategy to automatically assess the reliability of evaluation metrics for selecting robust ND-MT.",
    "authors": [
      "Weichuan Wang",
      "Mingyang Liu",
      "Linqi Song",
      "Chen Ma"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13729v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13729v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.13713v1",
    "title": "SWE-Tester: Training Open-Source LLMs for Issue Reproduction in Real-World Repositories",
    "summary": "Software testing is crucial for ensuring the correctness and reliability of software systems. Automated generation of issue reproduction tests from natural language issue descriptions enhances developer productivity by simplifying root cause analysis, promotes test-driven development -- \"test first, write code later\", and can be used for improving the effectiveness of automated issue resolution systems like coding agents. Existing methods proposed for this task predominantly rely on closed-source LLMs, with limited exploration of open models. To address this, we propose SWE-Tester -- a novel pipeline for training open-source LLMs to generate issue reproduction tests. First, we curate a high-quality training dataset of 41K instances from 2.6K open-source GitHub repositories and use it to train LLMs of varying sizes and families. The fine-tuned models achieve absolute improvements of up to 10\\% in success rate and 21\\% in change coverage on SWT-Bench Verified. Further analysis shows consistent improvements with increased inference-time compute, more data, and larger models. These results highlight the effectiveness of our framework for advancing open-source LLMs in this domain.",
    "authors": [
      "Aditya Bharat Soni",
      "Rajat Ghosh",
      "Vaishnavi Bhargava",
      "Valerie Chen",
      "Debojyoti Dutta"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13713v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13713v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.14101v1",
    "title": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition",
    "summary": "Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.   We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.   Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.",
    "authors": [
      "Emily Kim",
      "Allen Wu",
      "Jessica Hodgins"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14101v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14101v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.14063v1",
    "title": "XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in LLMs",
    "summary": "Cross-cultural competence in large language models (LLMs) requires the ability to identify Culture-Specific Items (CSIs) and to adapt them appropriately across cultural contexts. Progress in evaluating this capability has been constrained by the scarcity of high-quality CSI-annotated corpora with parallel cross-cultural sentence pairs. To address this limitation, we introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning tasks with corresponding evaluation metrics. Our corpus integrates Newmark's CSI framework with Hall's Triad of Culture, enabling systematic analysis of cultural reasoning beyond surface-level artifacts and into semi-visible and invisible cultural elements such as social norms, beliefs, and values. Our findings show that state-of-the-art LLMs exhibit consistent weaknesses in identifying and adapting CSIs related to social etiquette and cultural reference. Additionally, we find evidence that LLMs encode regional and ethno-religious biases even within a single linguistic setting during cultural adaptation. We release our corpus and code to facilitate future research on cross-cultural NLP.",
    "authors": [
      "Mohsinul Kabir",
      "Tasnim Ahmed",
      "Md Mezbaur Rahman",
      "Shaoxiong Ji",
      "Hassan Alhuzali",
      "Sophia Ananiadou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14063v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14063v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.14056v1",
    "title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion",
    "summary": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.",
    "authors": [
      "Andrea Rigo",
      "Luca Stornaiuolo",
      "Weijie Wang",
      "Mauro Martino",
      "Bruno Lepri",
      "Nicu Sebe"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14056v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14056v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.14052v1",
    "title": "Vision Also You Need: Navigating Out-of-Distribution Detection with Multimodal Large Language Model",
    "summary": "Out-of-Distribution (OOD) detection is a critical task that has garnered significant attention. The emergence of CLIP has spurred extensive research into zero-shot OOD detection, often employing a training-free approach. Current methods leverage expert knowledge from large language models (LLMs) to identify potential outliers. However, these approaches tend to over-rely on knowledge in the text space, neglecting the inherent challenges involved in detecting out-of-distribution samples in the image space. In this paper, we propose a novel pipeline, MM-OOD, which leverages the multimodal reasoning capabilities of MLLMs and their ability to conduct multi-round conversations for enhanced outlier detection. Our method is designed to improve performance in both near OOD and far OOD tasks. Specifically, (1) for near OOD tasks, we directly feed ID images and corresponding text prompts into MLLMs to identify potential outliers; and (2) for far OOD tasks, we introduce the sketch-generate-elaborate framework: first, we sketch outlier exposure using text prompts, then generate corresponding visual OOD samples, and finally elaborate by using multimodal prompts. Experiments demonstrate that our method achieves significant improvements on widely used multimodal datasets such as Food-101, while also validating its scalability on ImageNet-1K.",
    "authors": [
      "Haoran Xu",
      "Yanlin Liu",
      "Zizhao Tong",
      "Jiaze Li",
      "Kexue Fu",
      "Yuyang Zhang",
      "Longxiang Gao",
      "Shuaiguang Li",
      "Xingyu Li",
      "Yanran Xu",
      "Changwei Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14052v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14052v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.13935v1",
    "title": "TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation",
    "summary": "White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.",
    "authors": [
      "Anoushkrit Goel",
      "Simroop Singh",
      "Ankita Joshi",
      "Ranjeet Ranjan Jha",
      "Chirag Ahuja",
      "Aditya Nigam",
      "Arnav Bhavsar"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13935v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13935v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.13904v1",
    "title": "PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation",
    "summary": "Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.",
    "authors": [
      "Jaeyoung Moon",
      "Youjin Choi",
      "Yucheon Park",
      "David Melhart",
      "Georgios N. Yannakakis",
      "Kyung-Joong Kim"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13904v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13904v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.13731v1",
    "title": "Breaking the Data Barrier in Learning Symbolic Computation: A Case Study on Variable Ordering Suggestion for Cylindrical Algebraic Decomposition",
    "summary": "Symbolic computation, powered by modern computer algebra systems, has important applications in mathematical reasoning through exact deep computations. The efficiency of symbolic computation is largely constrained by such deep computations in high dimension. This creates a fundamental barrier on labelled data acquisition if leveraging supervised deep learning to accelerate symbolic computation. Cylindrical algebraic decomposition (CAD) is a pillar symbolic computation method for reasoning with first-order logic formulas over reals with many applications in formal verification and automatic theorem proving. Variable orderings have a huge impact on its efficiency. Impeded by the difficulty to acquire abundant labelled data, existing learning-based approaches are only competitive with the best expert-based heuristics. In this work, we address this problem by designing a series of intimately connected tasks for which a large amount of annotated data can be easily obtained. We pre-train a Transformer model with these data and then fine-tune it on the datasets for CAD ordering. Experiments on publicly available CAD ordering datasets show that on average the orderings predicted by the new model are significantly better than those suggested by the best heuristic methods.",
    "authors": [
      "Rui-Juan Jing",
      "Yuegang Zhao",
      "Changbo Chen"
    ],
    "categories": [
      "cs.SC",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13731v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13731v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.13651v1",
    "title": "Face-Voice Association with Inductive Bias for Maximum Class Separation",
    "summary": "Face-voice association is widely studied in multimodal learning and is approached representing faces and voices with embeddings that are close for a same person and well separated from those of others. Previous work achieved this with loss functions. Recent advancements in classification have shown that the discriminative ability of embeddings can be strengthened by imposing maximum class separation as inductive bias. This technique has never been used in the domain of face-voice association, and this work aims at filling this gap. More specifically, we develop a method for face-voice association that imposes maximum class separation among multimodal representations of different speakers as an inductive bias. Through quantitative experiments we demonstrate the effectiveness of our approach, showing that it achieves SOTA performance on two task formulation of face-voice association. Furthermore, we carry out an ablation study to show that imposing inductive bias is most effective when combined with losses for inter-class orthogonality. To the best of our knowledge, this work is the first that applies and demonstrates the effectiveness of maximum class separation as an inductive bias in multimodal learning; it hence paves the way to establish a new paradigm.",
    "authors": [
      "Marta Moscati",
      "Oleksandr Kats",
      "Mubashir Noman",
      "Muhammad Zaigham Zaheer",
      "Yufang Hou",
      "Markus Schedl",
      "Shah Nawaz"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13651v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13651v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.13647v1",
    "title": "Fusion Segment Transformer: Bi-Directional Attention Guided Fusion Network for AI-Generated Music Detection",
    "summary": "With the rise of generative AI technology, anyone can now easily create and deploy AI-generated music, which has heightened the need for technical solutions to address copyright and ownership issues. While existing works mainly focused on short-audio, the challenge of full-audio detection, which requires modeling long-term structure and context, remains insufficiently explored. To address this, we propose an improved version of the Segment Transformer, termed the Fusion Segment Transformer. As in our previous work, we extract content embeddings from short music segments using diverse feature extractors. Furthermore, we enhance the architecture for full-audio AI-generated music detection by introducing a Gated Fusion Layer that effectively integrates content and structural information, enabling the capture of long-term context. Experiments on the SONICS and AIME datasets show that our approach outperforms the previous model and recent baselines, achieving state-of-the-art results in AI-generated music detection.",
    "authors": [
      "Yumin Kim",
      "Seonghyeon Go"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13647v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13647v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.14171v1",
    "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
    "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.",
    "authors": [
      "Qianli Ma",
      "Chang Guo",
      "Zhiheng Tian",
      "Siyu Wang",
      "Jipeng Xiao",
      "Yuanhao Yue",
      "Zhipeng Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14171v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14171v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.14152v1",
    "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
    "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.",
    "authors": [
      "Hyunjong Ok",
      "Jaeho Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14152v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14152v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.14133v1",
    "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers",
    "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.",
    "authors": [
      "Bin Yu",
      "Shijie Lian",
      "Xiaopeng Lin",
      "Yuliang Wei",
      "Zhaolong Shen",
      "Changti Wu",
      "Yuzhuo Miao",
      "Xinming Wang",
      "Bailing Wang",
      "Cong Huang",
      "Kai Chen"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14133v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14133v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.14091v1",
    "title": "Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems",
    "summary": "Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.",
    "authors": [
      "Hossein Naderi",
      "Alireza Shojaei",
      "Lifu Huang",
      "Philip Agee",
      "Kereshmeh Afsari",
      "Abiola Akanmu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14091v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14091v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.13992v1",
    "title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework",
    "summary": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.",
    "authors": [
      "Jin Cui",
      "Jiaqi Guo",
      "Jiepeng Zhou",
      "Ruixuan Yang",
      "Jiayi Lu",
      "Jiajun Xu",
      "Jiangcheng Song",
      "Boran Zhao",
      "Pengju Ren"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13992v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13992v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.13913v1",
    "title": "On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation",
    "summary": "Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.",
    "authors": [
      "Pavlo Melnyk",
      "Cuong Le",
      "Urs Waldmann",
      "Per-Erik Forssén",
      "Bastian Wandt"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13913v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13913v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.13885v1",
    "title": "Confident Rankings with Fewer Items: Adaptive LLM Evaluation with Continuous Scores",
    "summary": "Computerized Adaptive Testing (CAT) has proven effective for efficient LLM evaluation on multiple-choice benchmarks, but modern LLM evaluation increasingly relies on generation tasks where outputs are scored continuously rather than marked correct/incorrect. We present a principled extension of IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU, LLM-as-a-Judge) by replacing the Bernoulli response distribution with a heteroskedastic normal distribution. Building on this, we introduce an uncertainty aware ranker with adaptive stopping criteria that achieves reliable model ranking while testing as few items and as cheaply as possible. We validate our method on five benchmarks spanning n-gram-based, embedding-based, and LLM-as-judge metrics. Our method uses 2% of the items while improving ranking correlation by 0.12 τ over random sampling, with 95% accuracy on confident predictions.",
    "authors": [
      "Esma Balkır",
      "Alice Pernthaller",
      "Marco Basaldella",
      "José Hernández-Orallo",
      "Nigel Collier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13885v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13885v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.13802v1",
    "title": "Habibi: Laying the Open-Source Foundation of Unified-Dialectal Arabic Speech Synthesis",
    "summary": "A notable gap persists in speech synthesis research and development for Arabic dialects, particularly from a unified modeling perspective. Despite its high practical value, the inherent linguistic complexity of Arabic dialects, further compounded by a lack of standardized data, benchmarks, and evaluation guidelines, steers researchers toward safer ground. To bridge this divide, we present Habibi, a suite of specialized and unified text-to-speech models that harnesses existing open-source ASR corpora to support a wide range of high- to low-resource Arabic dialects through linguistically-informed curriculum learning. Our approach outperforms the leading commercial service in generation quality, while maintaining extensibility through effective in-context learning, without requiring text diacritization. We are committed to open-sourcing the model, along with creating the first systematic benchmark for multi-dialect Arabic speech synthesis. Furthermore, by identifying the key challenges in and establishing evaluation standards for the process, we aim to provide a solid groundwork for subsequent research. Resources at https://SWivid.github.io/Habibi/ .",
    "authors": [
      "Yushen Chen",
      "Junzhe Liu",
      "Yujie Tu",
      "Zhikang Niu",
      "Yuzhe Liang",
      "Kai Yu",
      "Chunyu Qiang",
      "Chen Zhang",
      "Xie Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13802v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13802v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.13717v1",
    "title": "Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff",
    "summary": "Evaluating LLM forecasting capabilities is constrained by a fundamental tension: prospective evaluation offers methodological rigor but prohibitive latency, while retrospective forecasting (RF) -- evaluating on already-resolved events -- faces rapidly shrinking clean evaluation data as SOTA models possess increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting models to suppress pre-cutoff knowledge, has emerged as a potential solution. We provide the first systematic test of whether SI can approximate True Ignorance (TI). Across 477 competition-level questions and 9 models, we find that SI fails systematically: (1) cutoff instructions leave a 52% performance gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior knowledge, even when reasoning traces contain no explicit post-cutoff references; (3) reasoning-optimized models exhibit worse SI fidelity despite superior reasoning trace quality. These findings demonstrate that prompts cannot reliably \"rewind\" model knowledge. We conclude that RF on pre-cutoff events is methodologically flawed; we recommend against using SI-based retrospective setups to benchmark forecasting capabilities.",
    "authors": [
      "Zehan Li",
      "Yuxuan Wang",
      "Ali El Lahib",
      "Ying-Jieh Xia",
      "Xinyu Pi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13717v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13717v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.13683v1",
    "title": "Dynamic Differential Linear Attention: Enhancing Linear Diffusion Transformer for High-Quality Image Generation",
    "summary": "Diffusion transformers (DiTs) have emerged as a powerful architecture for high-fidelity image generation, yet the quadratic cost of self-attention poses a major scalability bottleneck. To address this, linear attention mechanisms have been adopted to reduce computational cost; unfortunately, the resulting linear diffusion transformers (LiTs) models often come at the expense of generative performance, frequently producing over-smoothed attention weights that limit expressiveness. In this work, we introduce Dynamic Differential Linear Attention (DyDiLA), a novel linear attention formulation that enhances the effectiveness of LiTs by mitigating the oversmoothing issue and improving generation quality. Specifically, the novelty of DyDiLA lies in three key designs: (i) dynamic projection module, which facilitates the decoupling of token representations by learning with dynamically assigned knowledge; (ii) dynamic measure kernel, which provides a better similarity measurement to capture fine-grained semantic distinctions between tokens by dynamically assigning kernel functions for token processing; and (iii) token differential operator, which enables more robust query-to-key retrieval by calculating the differences between the tokens and their corresponding information redundancy produced by dynamic measure kernel. To capitalize on DyDiLA, we introduce a refined LiT, termed DyDi-LiT, that systematically incorporates our advancements. Extensive experiments show that DyDi-LiT consistently outperforms current state-of-the-art (SOTA) models across multiple metrics, underscoring its strong practical potential.",
    "authors": [
      "Boyuan Cao",
      "Xingbo Yao",
      "Chenhui Wang",
      "Jiaxin Ye",
      "Yujie Wei",
      "Hongming Shan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13683v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13683v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.14226v1",
    "title": "Deep Learning Approaches to Quantum Error Mitigation",
    "summary": "We present a systematic investigation of deep learning methods applied to quantum error mitigation of noisy output probability distributions from measured quantum circuits. We compare different architectures, from fully connected neural networks to transformers, and we test different design/training modalities, identifying sequence-to-sequence, attention-based models as the most effective on our datasets. These models consistently produce mitigated distributions that are closer to the ideal outputs when tested on both simulated and real device data obtained from IBM superconducting quantum processing units (QPU) up to five qubits. Across several different circuit depths, our approach outperforms other baseline error mitigation techniques. We perform a series of ablation studies to examine: how different input features (circuit, device properties, noisy output statistics) affect performance; cross-dataset generalization across circuit families; and transfer learning to a different IBM QPU. We observe that generalization performance across similar devices with the same architecture works effectively, without needing to fully retrain models.",
    "authors": [
      "Leonardo Placidi",
      "Ifan Williams",
      "Enrico Rinaldi",
      "Daniel Mills",
      "Cristina Cîrstoiu",
      "Vanya Eccles",
      "Ross Duncan"
    ],
    "categories": [
      "quant-ph",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14226v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14226v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.14069v1",
    "title": "Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management",
    "summary": "Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.",
    "authors": [
      "Nattapong Kurpukdee",
      "Adrian G. Bors"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14069v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14069v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.14012v1",
    "title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting",
    "summary": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.",
    "authors": [
      "Youngmoon Jung",
      "Myunghun Jung",
      "Joon-Young Yang",
      "Yong-Hyeok Lee",
      "Jaeyoung Roh",
      "Hoon-Young Cho"
    ],
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14012v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14012v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.13920v1",
    "title": "Asymmetric regularization mechanism for GAN training with Variational Inequalities",
    "summary": "We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.",
    "authors": [
      "Spyridon C. Giagtzoglou",
      "Mark H. M. Winands",
      "Barbara Franci"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13920v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13920v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.13876v1",
    "title": "Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education",
    "summary": "Science demonstrations are important for effective STEM education, yet teachers face challenges in conducting them safely and consistently across multiple occasions, where robotics can be helpful. However, current Vision-Language-Action (VLA) models require substantial computational resources and sacrifice language generation capabilities to maximize efficiency, making them unsuitable for resource-constrained educational settings that require interpretable, explanation-generating systems. We present \\textit{Pedagogical VLA Framework}, a framework that applies pedagogical alignment to lightweight VLA models through four components: text healing to restore language generation capabilities, large language model (LLM) distillation to transfer pedagogical knowledge, safety training for educational environments, and pedagogical evaluation adjusted to science education contexts. We evaluate Pedagogical VLA Framework across five science demonstrations spanning physics, chemistry, biology, and earth science, using an evaluation framework developed in collaboration with science education experts. Our evaluation assesses both task performance (success rate, protocol compliance, efficiency, safety) and pedagogical quality through teacher surveys and LLM-as-Judge assessment. We additionally provide qualitative analysis of generated texts. Experimental results demonstrate that Pedagogical VLA Framework achieves comparable task performance to baseline models while producing contextually appropriate educational explanations.",
    "authors": [
      "Unggi Lee",
      "Jahyun Jeong",
      "Sunyoung Shin",
      "Haeun Park",
      "Jeongsu Moon",
      "Youngchang Song",
      "Jaechang Shim",
      "JaeHwan Lee",
      "Yunju Noh",
      "Seungwon Choi",
      "Ahhyun Kim",
      "TaeHyeon Kim",
      "Kyungtae Joo",
      "Taeyeong Kim",
      "Gyeonggeon Lee"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13876v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13876v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.13851v1",
    "title": "Inverting Self-Organizing Maps: A Unified Activation-Based Framework",
    "summary": "Self-Organizing Maps provide topology-preserving projections of high-dimensional data and have been widely used for visualization, clustering, and vector quantization. In this work, we show that the activation pattern of a SOM - the squared distances to its prototypes - can be inverted to recover the exact input under mild geometric conditions. This follows from a classical fact in Euclidean distance geometry: a point in $D$ dimensions is uniquely determined by its distances to $D{+}1$ affinely independent references. We derive the corresponding linear system and characterize the conditions under which the inversion is well-posed. Building upon this mechanism, we introduce the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which enables controlled, semantically meaningful trajectories in latent space. MUSIC modifies squared distances to selected prototypes while preserving others, resulting in a deterministic geometric flow aligned with the SOM's piecewise-linear structure. Tikhonov regularization stabilizes the update rule and ensures smooth motion on high-dimensional datasets. Unlike variational or probabilistic generative models, MUSIC does not rely on sampling, latent priors, or encoder-decoder architectures. If no perturbation is applied, inversion recovers the exact input; when a target cluster or prototype is specified, MUSIC produces coherent semantic variations while remaining on the data manifold. This leads to a new perspective on data augmentation and controllable latent exploration based solely on prototype geometry. We validate the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the Wild dataset. Across all settings, MUSIC produces smooth, interpretable trajectories that reveal the underlying geometry of the learned manifold, illustrating the advantages of SOM-based inversion over unsupervised clustering.",
    "authors": [
      "Alessandro Londei",
      "Matteo Benati",
      "Denise Lanzieri",
      "Vittorio Loreto"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13851v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13851v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.13776v1",
    "title": "Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz Building Blocks",
    "summary": "Orthogonal and 1-Lipschitz neural network layers are essential building blocks in robust deep learning architectures, crucial for certified adversarial robustness, stable generative models, and reliable recurrent networks. Despite significant advancements, existing implementations remain fragmented, limited, and computationally demanding. To address these issues, we introduce Orthogonium , a unified, efficient, and comprehensive PyTorch library providing orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard convolution features-including support for strides, dilation, grouping, and transposed-while maintaining strict mathematical guarantees. Its optimized implementations reduce overhead on large scale benchmarks such as ImageNet. Moreover, rigorous testing within the library has uncovered critical errors in existing implementations, emphasizing the importance of standardized and reliable tools. Orthogonium thus significantly lowers adoption barriers, enabling scalable experimentation and integration across diverse applications requiring orthogonality and robust Lipschitz constraints. Orthogonium is available at https://github.com/deel-ai/orthogonium.",
    "authors": [
      "Thibaut Boissin",
      "Franck Mamalet",
      "Valentin Lafargue",
      "Mathieu Serrurier"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13776v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13776v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.13690v1",
    "title": "Dr. Assistant: Enhancing Clinical Diagnostic Inquiry via Structured Diagnostic Reasoning Data and Reinforcement Learning",
    "summary": "Clinical Decision Support Systems (CDSSs) provide reasoning and inquiry guidance for physicians, yet they face notable challenges, including high maintenance costs and low generalization capability. Recently, Large Language Models (LLMs) have been widely adopted in healthcare due to their extensive knowledge reserves, retrieval, and communication capabilities. While LLMs show promise and excel at medical benchmarks, their diagnostic reasoning and inquiry skills are constrained. To mitigate this issue, we propose (1) Clinical Diagnostic Reasoning Data (CDRD) structure to capture abstract clinical reasoning logic, and a pipeline for its construction, and (2) the Dr. Assistant, a clinical diagnostic model equipped with clinical reasoning and inquiry skills. Its training involves a two-stage process: SFT, followed by RL with a tailored reward function. We also introduce a benchmark to evaluate both diagnostic reasoning and inquiry. Our experiments demonstrate that the Dr. Assistant outperforms open-source models and achieves competitive performance to closed-source models, providing an effective solution for clinical diagnostic inquiry guidance.",
    "authors": [
      "Yue Guo",
      "Fanfu Wang",
      "Jianwei Lv",
      "Xincheng Shi",
      "Yuchen Li",
      "Youya Wang",
      "Yunsheng Zeng",
      "Yujing Liu",
      "Yunhao Qiao",
      "Gen Li",
      "Junfeng Wang",
      "Bo Yuan"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13690v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13690v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.14228v1",
    "title": "Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment",
    "summary": "Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.",
    "authors": [
      "Punit Kumar",
      "Vaibhav Saran",
      "Divyesh Patel",
      "Nitin Kulkarni",
      "Alina Vereshchaka"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14228v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14228v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.14192v1",
    "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
    "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.",
    "authors": [
      "Xiaofang Yang",
      "Lijun Li",
      "Heng Zhou",
      "Tong Zhu",
      "Xiaoye Qu",
      "Yuchen Fan",
      "Qianshan Wei",
      "Rui Ye",
      "Li Kang",
      "Yiran Qin",
      "Zhiqiang Kou",
      "Daizong Liu",
      "Qi Li",
      "Ning Ding",
      "Siheng Chen",
      "Jing Shao"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14192v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14192v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.14086v1",
    "title": "Two-Stream temporal transformer for video action classification",
    "summary": "Motion representation plays an important role in video understanding and has many applications including action recognition, robot and autonomous guidance or others. Lately, transformer networks, through their self-attention mechanism capabilities, have proved their efficiency in many applications. In this study, we introduce a new two-stream transformer video classifier, which extracts spatio-temporal information from content and optical flow representing movement information. The proposed model identifies self-attention features across the joint optical flow and temporal frame domain and represents their relationships within the transformer encoder mechanism. The experimental results show that our proposed methodology provides excellent classification results on three well-known video datasets of human activities.",
    "authors": [
      "Nattapong Kurpukdee",
      "Adrian G. Bors"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14086v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14086v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.13986v1",
    "title": "Equivariant Learning for Unsupervised Image Dehazing",
    "summary": "Image Dehazing (ID) aims to produce a clear image from an observation contaminated by haze. Current ID methods typically rely on carefully crafted priors or extensive haze-free ground truth, both of which are expensive or impractical to acquire, particularly in the context of scientific imaging. We propose a new unsupervised learning framework called Equivariant Image Dehazing (EID) that exploits the symmetry of image signals to restore clarity to hazy observations. By enforcing haze consistency and systematic equivariance, EID can recover clear patterns directly from raw, hazy images. Additionally, we propose an adversarial learning strategy to model unknown haze physics and facilitate EID learning. Experiments on two scientific image dehazing benchmarks (including cell microscopy and medical endoscopy) and on natural image dehazing have demonstrated that EID significantly outperforms state-of-the-art approaches. By unifying equivariant learning with modelling haze physics, we hope that EID will enable more versatile and effective haze removal in scientific imaging. Code and datasets will be published.",
    "authors": [
      "Zhang Wen",
      "Jiangwei Xie",
      "Dongdong Chen"
    ],
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13986v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13986v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.13892v1",
    "title": "Multi-Objective Hierarchical Optimization with Large Language Models",
    "summary": "Despite their widespread adoption in various domains, especially due to their powerful reasoning capabilities, Large Language Models (LLMs) are not the off-the-shelf choice to drive multi-objective optimization yet. Conventional strategies rank high in benchmarks due to their intrinsic capabilities to handle numerical inputs and careful modelling choices that balance exploration and Pareto-front exploitation, as well as handle multiple (conflicting) objectives. In this paper, we close this gap by leveraging LLMs as surrogate models and candidate samplers inside a structured hierarchical search strategy. By adaptively partitioning the input space into disjoint hyperrectangular regions and ranking them with a composite score function, we restrict the generative process of the LLM to specific, high-potential sub-spaces, hence making the problem easier to solve as the LLM doesn't have to reason about the global structure of the problem, but only locally instead. We show that under standard regularity assumptions, our algorithm generates candidate solutions that converge to the true Pareto set in Hausdorff distance. Empirically, it consistently outperforms the global LLM-based multi-objective optimizer and is on par with standard evolutionary and Bayesian optimization algorithm on synthetic and real-world benchmarks.",
    "authors": [
      "Andrej Schwanke",
      "Lyubomir Ivanov",
      "David Salinas",
      "Frank Hutter",
      "Arber Zela"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13892v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13892v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.13752v1",
    "title": "Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering",
    "summary": "Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \\textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.",
    "authors": [
      "Chak Tou Leong",
      "Dingwei Chen",
      "Heming Xia",
      "Qingyu Yin",
      "Sunbowen Lee",
      "Jian Wang",
      "Wenjie Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13752v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13752v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.13669v1",
    "title": "CommunityBench: Benchmarking Community-Level Alignment across Diverse Groups and Tasks",
    "summary": "Large language models (LLMs) alignment ensures model behaviors reflect human value. Existing alignment strategies primarily follow two paths: one assumes a universal value set for a unified goal (i.e., one-size-fits-all), while the other treats every individual as unique to customize models (i.e., individual-level). However, assuming a monolithic value space marginalizes minority norms, while tailoring individual models is prohibitively expensive. Recognizing that human society is organized into social clusters with high intra-group value alignment, we propose community-level alignment as a \"middle ground\". Practically, we introduce CommunityBench, the first large-scale benchmark for community-level alignment evaluation, featuring four tasks grounded in Common Identity and Common Bond theory. With CommunityBench, we conduct a comprehensive evaluation of various foundation models on CommunityBench, revealing that current LLMs exhibit limited capacity to model community-specific preferences. Furthermore, we investigate the potential of community-level alignment in facilitating individual modeling, providing a promising direction for scalable and pluralistic alignment.",
    "authors": [
      "Jiayu Lin",
      "Zhongyu Wei"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13669v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13669v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.14255v1",
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.",
    "authors": [
      "Sangbeom Lim",
      "Seoung Wug Oh",
      "Jiahui Huang",
      "Heeji Yoon",
      "Seungryong Kim",
      "Joon-Young Lee"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14255v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14255v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.14157v1",
    "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
    "summary": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.",
    "authors": [
      "Bruno Sienkiewicz",
      "Łukasz Neumann",
      "Mateusz Modrzejewski"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14157v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14157v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.14041v1",
    "title": "Top 10 Open Challenges Steering the Future of Diffusion Language Model and Its Variants",
    "summary": "The paradigm of Large Language Models (LLMs) is currently defined by auto-regressive (AR) architectures, which generate text through a sequential ``brick-by-brick'' process. Despite their success, AR models are inherently constrained by a causal bottleneck that limits global structural foresight and iterative refinement. Diffusion Language Models (DLMs) offer a transformative alternative, conceptualizing text generation as a holistic, bidirectional denoising process akin to a sculptor refining a masterpiece. However, the potential of DLMs remains largely untapped as they are frequently confined within AR-legacy infrastructures and optimization frameworks. In this Perspective, we identify ten fundamental challenges ranging from architectural inertia and gradient sparsity to the limitations of linear reasoning that prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic roadmap organized into four pillars: foundational infrastructure, algorithmic optimization, cognitive reasoning, and unified multimodal intelligence. By shifting toward a diffusion-native ecosystem characterized by multi-scale tokenization, active remasking, and latent thinking, we can move beyond the constraints of the causal horizon. We argue that this transition is essential for developing next-generation AI capable of complex structural reasoning, dynamic self-correction, and seamless multimodal integration.",
    "authors": [
      "Yunhe Wang",
      "Kai Han",
      "Huiling Zhen",
      "Yuchuan Tian",
      "Hanting Chen",
      "Yongbing Huang",
      "Yufei Cui",
      "Yingte Shu",
      "Shan Gao",
      "Ismail Elezi",
      "Roy Vaughan Miles",
      "Songcen Xu",
      "Feng Wen",
      "Chao Xu",
      "Sinan Zeng",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14041v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14041v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.14022v1",
    "title": "Credible CO2 Comparisons: A Machine Learning Approach to Vehicle Powertrain Assessment",
    "summary": "Decarbonizing road transport requires consistent and transparent methods for comparing CO2 emissions across vehicle technologies. This paper proposes a machine learning-based framework for like-for-like operational assessment of internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under identical, real-world driving conditions. The approach isolates technology-specific effects by holding the observed speed profile and environmental context fixed, enabling direct comparison of powertrain performance. Recurrent neural network models are trained independently for each domain to learn the mapping from contextual driving variables (speed, acceleration, temperature) to internal actuation variables (torque, throttle) and instantaneous CO2-equivalent emission rates. This structure allows the construction of counterfactual scenarios that answer: What emissions would an EV have generated if it had followed the same driving profile as an ICEV? By aligning both vehicle types on a unified instantaneous emissions metric, the framework enables fair and reproducible evaluation of powertrain technologies. It offers a scalable foundation for credible, data-driven assessments of vehicle carbon performance under real-world operating conditions.",
    "authors": [
      "Rodrigo Pereira David",
      "Luciano Araujo Dourado Filho",
      "Daniel Marques da Silva",
      "João Alfredo Cal-Braz"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14022v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14022v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.13945v1",
    "title": "Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework",
    "summary": "As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.",
    "authors": [
      "Yixuan Deng",
      "Tongrun Wu",
      "Donghao Wu",
      "Zeyu Wei",
      "Jiayuan Wang",
      "Zhenglong Sun",
      "Yuqing Tang",
      "Xiaoqiang Ji"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13945v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13945v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.13922v1",
    "title": "Automatic Prompt Optimization for Dataset-Level Feature Discovery",
    "summary": "Feature extraction from unstructured text is a critical step in many downstream classification pipelines, yet current approaches largely rely on hand-crafted prompts or fixed feature schemas. We formulate feature discovery as a dataset-level prompt optimization problem: given a labelled text corpus, the goal is to induce a global set of interpretable and discriminative feature definitions whose realizations optimize a downstream supervised learning objective. To this end, we propose a multi-agent prompt optimization framework in which language-model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on this structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. This formulation departs from prior prompt optimization methods that rely on per-sample supervision and provides a principled mechanism for automatic feature discovery from unstructured text.",
    "authors": [
      "Adrian Cosma",
      "Oleg Szehr",
      "David Kletz",
      "Alessandro Antonucci",
      "Olivier Pelletier"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13922v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13922v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.13710v1",
    "title": "Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML for CRS Surgical Outcome Prediction",
    "summary": "Artificial intelligence has reshaped medical imaging, yet the use of AI on clinical data for prospective decision support remains limited. We study pre-operative prediction of clinically meaningful improvement in chronic rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all patients underwent surgery, we ask whether models using only pre-operative clinical data could have identified those who would have poor outcomes, i.e. those who should have avoided surgery. We benchmark supervised ML (logistic regression, tree ensembles, and an in-house MLP) against generative AI (ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs and constraining outputs to binary recommendations with confidence. Our best ML model (MLP) achieves 85 % accuracy with superior calibration and decision-curve net benefit. GenAI models underperform on discrimination and calibration across zero-shot setting. Notably, GenAI justifications align with clinician heuristics and the MLP's feature importance, repeatedly highlighting baseline SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow: deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an explainer to enhance transparency and shared decision-making.",
    "authors": [
      "Sayeed Shafayet Chowdhury",
      "Snehasis Mukhopadhyay",
      "Shiaofen Fang",
      "Vijay R. Ramakrishnan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13710v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13710v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.13697v1",
    "title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
    "summary": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring.",
    "authors": [
      "Zhihang Yuan",
      "Chengyu Yue",
      "Long Huang",
      "Litu Ou",
      "Lei Shi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13697v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13697v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.13662v1",
    "title": "Reinforcement Learning for Opportunistic Routing in Software-Defined LEO-Terrestrial Systems",
    "summary": "The proliferation of large-scale low Earth orbit (LEO) satellite constellations is driving the need for intelligent routing strategies that can effectively deliver data to terrestrial networks under rapidly time-varying topologies and intermittent gateway visibility. Leveraging the global control capabilities of a geostationary (GEO)-resident software-defined networking (SDN) controller, we introduce opportunistic routing, which aims to minimize delivery delay by forwarding packets to any currently available ground gateways rather than fixed destinations. This makes it a promising approach for achieving low-latency and robust data delivery in highly dynamic LEO networks. Specifically, we formulate a constrained stochastic optimization problem and employ a residual reinforcement learning framework to optimize opportunistic routing for reducing transmission delay. Simulation results over multiple days of orbital data demonstrate that our method achieves significant improvements in queue length reduction compared to classical backpressure and other well-known queueing algorithms.",
    "authors": [
      "Sivaram Krishnan",
      "Zhouyou Gu",
      "Jihong Park",
      "Sung-Min Oh",
      "Jinho Choi"
    ],
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13662v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13662v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.13655v1",
    "title": "Why Does the LLM Stop Computing: An Empirical Study of User-Reported Failures in Open-Source LLMs",
    "summary": "The democratization of open-source Large Language Models (LLMs) allows users to fine-tune and deploy models on local infrastructure but exposes them to a First Mile deployment landscape. Unlike black-box API consumption, the reliability of user-managed orchestration remains a critical blind spot. To bridge this gap, we conduct the first large-scale empirical study of 705 real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.   Our analysis reveals a paradigm shift: white-box orchestration relocates the reliability bottleneck from model algorithmic defects to the systemic fragility of the deployment stack. We identify three key phenomena: (1) Diagnostic Divergence: runtime crashes distinctively signal infrastructure friction, whereas incorrect functionality serves as a signature for internal tokenizer defects. (2) Systemic Homogeneity: Root causes converge across divergent series, confirming reliability barriers are inherent to the shared ecosystem rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate from intrinsic configuration struggles during fine-tuning to compounded environmental incompatibilities during inference. Supported by our publicly available dataset, these insights provide actionable guidance for enhancing the reliability of the LLM landscape.",
    "authors": [
      "Guangba Yu",
      "Zirui Wang",
      "Yujie Huang",
      "Renyi Zhong",
      "Yuedong Zhong",
      "Yilun Wang",
      "Michael R. Lyu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13655v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13655v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.14256v1",
    "title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding",
    "summary": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.",
    "authors": [
      "Matthew Gwilliam",
      "Xiao Wang",
      "Xuefeng Hu",
      "Zhenheng Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14256v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14256v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.14210v1",
    "title": "HALT: Hallucination Assessment via Latent Testing",
    "summary": "Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.",
    "authors": [
      "Rohan Bhatnagar",
      "Youran Sun",
      "Chi Andrew Zhang",
      "Yixin Wen",
      "Haizhao Yang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14210v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14210v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.14208v1",
    "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting",
    "summary": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.",
    "authors": [
      "Nitin Kulkarni",
      "Akhil Devarashetti",
      "Charlie Cluss",
      "Livio Forte",
      "Dan Buckmaster",
      "Philip Schneider",
      "Chunming Qiao",
      "Alina Vereshchaka"
    ],
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14208v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14208v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.14180v1",
    "title": "Progressive self-supervised blind-spot denoising method for LDCT denoising",
    "summary": "Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.",
    "authors": [
      "Yichao Liu",
      "Yueyang Teng",
      "Junwen Guo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14180v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14180v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.14165v1",
    "title": "ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction",
    "summary": "Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.",
    "authors": [
      "Zhenghong Li",
      "Wensheng Cheng",
      "Congwu Du",
      "Yingtian Pan",
      "Zhaozheng Yin",
      "Haibin Ling"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14165v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14165v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.14044v1",
    "title": "Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology",
    "summary": "While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model's reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.",
    "authors": [
      "Kaiyu Wu",
      "Pucheng Han",
      "Hualong Zhang",
      "Naigeng Wu",
      "Keze Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14044v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14044v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.13994v1",
    "title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch",
    "summary": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.",
    "authors": [
      "Mingyuan Chi"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13994v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13994v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.13852v1",
    "title": "Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation",
    "summary": "Linear discriminant analysis improves class separability but struggles with non-linearly separable data. To overcome this, we introduce Deep Discriminant Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep networks. To ensure stable training and avoid computational instabilities, we incorporate signed between-class variance, bound outputs with a sigmoid function, and convert multiplicative relationships into additive ones. We present two stable DDA loss functions and augment them with a probability loss, resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap in output distributions, producing highly confident predictions with reduced within-class variance. When applied to wind blade segmentation, PDDA showcases notable advances in performance and consistency, critical for wind energy maintenance. To our knowledge, this is the first application of DDA to image segmentation.",
    "authors": [
      "Raül Pérez-Gonzalo",
      "Andreas Espersen",
      "Antonio Agudo"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13852v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13852v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.13849v1",
    "title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control",
    "summary": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.",
    "authors": [
      "Ziyi Yang",
      "Li Rao",
      "Zhengding Luo",
      "Dongyuan Shi",
      "Qirui Huang",
      "Woon-Seng Gan"
    ],
    "categories": [
      "eess.AS",
      "cs.LG",
      "eess.SP"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13849v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13849v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.13695v1",
    "title": "OptiSQL: Executable SQL Generation from Optical TokensOptiSQL: Executable SQL Generation from Optical Tokens",
    "summary": "Executable SQL generation is typically studied in text-to-SQL settings, where tables are provided as fully linearized textual schemas and contents. While effective, this formulation assumes access to structured text and incurs substantial token overhead, which is misaligned with many real-world scenarios where tables appear as visual artifacts in documents or webpages. We investigate whether compact optical representations can serve as an efficient interface for executable semantic parsing. We present OptiSQL, a vision-driven framework that generates executable SQL directly from table images and natural language questions using compact optical tokens. OptiSQL leverages an OCR-oriented visual encoder to compress table structure and content into a small set of optical tokens and fine-tunes a pretrained decoder for SQL generation while freezing the encoder to isolate representation sufficiency. Experiments on a visualized version of Spider 2.0-Snow show that OptiSQL retains strong execution accuracy while reducing table input tokens by an order of magnitude. Robustness analyses further demonstrate that optical tokens preserve essential structural information under visual perturbations.",
    "authors": [
      "Sifan Li",
      "Hongkai Chen",
      "Yujun Cai",
      "Liyang Chen",
      "Qingwen Ye",
      "Yiwei Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13695v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13695v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.13687v1",
    "title": "Understanding Mental States to Guide Social Influence in Multi-Person Group Dialogue",
    "summary": "Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models in a passive role: the model reads a sequence of connected scenarios and reports what people believe, feel, intend, and do as these states change. In real social interaction, ToM is also used for action: a speaker plans what to say in order to shift another person's mental-state trajectory toward a goal. We introduce SocialMindChange, a benchmark that moves from tracking minds to changing minds in social interaction. Each instance defines a social context with 4 characters and five connected scenes. The model plays one character and generates dialogue across the five scenes to reach the target while remaining consistent with the evolving states of all participants. SocialMindChange also includes selected higher-order states. Using a structured four-step framework, we construct 1,200 social contexts, covering 6000 scenarios and over 90,000 questions, each validated for realism and quality. Evaluations on ten state-of-the-art LLMs show that their average performance is 54.2% below human performance. This gap suggests that current LLMs still struggle to maintain and change mental-state representations across long, linked interactions.",
    "authors": [
      "Zhichao Liang",
      "Satoshi Nakamura"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13687v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13687v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.13659v1",
    "title": "Temporal-Spatial Decouple before Act: Disentangled Representation Learning for Multimodal Sentiment Analysis",
    "summary": "Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic. Mainstream approaches based on modality-invariant and modality-specific factorization or on complex fusion still rely on spatiotemporal mixed modeling. This ignores spatiotemporal heterogeneity, leading to spatiotemporal information asymmetry and thus limited performance. Hence, we propose TSDA, Temporal-Spatial Decouple before Act, which explicitly decouples each modality into temporal dynamics and spatial structural context before any interaction. For every modality, a temporal encoder and a spatial encoder project signals into separate temporal and spatial body. Factor-Consistent Cross-Modal Alignment then aligns temporal features only with their temporal counterparts across modalities, and spatial features only with their spatial counterparts. Factor specific supervision and decorrelation regularization reduce cross factor leakage while preserving complementarity. A Gated Recouple module subsequently recouples the aligned streams for task. Extensive experiments show that TSDA outperforms baselines. Ablation analysis studies confirm the necessity and interpretability of the design.",
    "authors": [
      "Chunlei Meng",
      "Ziyang Zhou",
      "Lucas He",
      "Xiaojing Du",
      "Chun Ouyang",
      "Zhongxue Gan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13659v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13659v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.14209v1",
    "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.",
    "authors": [
      "Matthew Y. R. Yang",
      "Hao Bai",
      "Ian Wu",
      "Gene Yang",
      "Amrith Setlur",
      "Aviral Kumar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14209v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14209v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.14207v1",
    "title": "Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints",
    "summary": "We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.",
    "authors": [
      "Rotem Gatenyo",
      "Ohad Fried"
    ],
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14207v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14207v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.14050v1",
    "title": "Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering",
    "summary": "Mixture-of-Experts (MoE) architectures have shown strong multilingual capabilities, yet the internal mechanisms underlying performance gains and cross-language differences remain insufficiently understood. In this work, we conduct a systematic analysis of MoE models, examining routing behavior and expert specialization across languages and network depth. Our analysis reveals that multilingual processing in MoE models is highly structured: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts despite weaker performance. Layerwise interventions further show that early and late MoE layers support language-specific processing, whereas middle layers serve as language-agnostic capacity hubs. Building on these insights, we propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, leading to consistent multilingual performance improvements, particularly for linguistically related language pairs. Our code is available at https://github.com/conctsai/Multilingualism-in-Mixture-of-Experts-LLMs.",
    "authors": [
      "Yuxin Chen",
      "Zhengzhou Cai",
      "Xiangtian Ji",
      "Weixiang Zhao",
      "An Zhang",
      "Xiang Wang",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14050v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14050v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.14047v1",
    "title": "Collective intelligence in science: direct elicitation of diverse information from experts with unknown information structure",
    "summary": "Suppose we need a deep collective analysis of an open scientific problem: there is a complex scientific hypothesis and a large online group of mutually unrelated experts with relevant private information of a diverse and unpredictable nature. This information may be results of experts' individual experiments, original reasoning of some of them, results of AI systems they use, etc. We propose a simple mechanism based on a self-resolving play-money prediction market entangled with a chat. We show that such a system can easily be brought to an equilibrium where participants directly share their private information on the hypothesis through the chat and trade as if the market were resolved in accordance with the truth of the hypothesis. This approach will lead to efficient aggregation of relevant information in a completely interpretable form even if the ground truth cannot be established and experts initially know nothing about each other and cannot perform complex Bayesian calculations. Finally, by rewarding the experts with some real assets proportionally to the play money they end up with, we can get an innovative way to fund large-scale collaborative studies of any type.",
    "authors": [
      "Alexey V. Osipov",
      "Nikolay N. Osipov"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.MA",
      "cs.SI",
      "econ.TH"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14047v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14047v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13938v1",
    "title": "IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative Engine Optimization",
    "summary": "As Generative Engines revolutionize information retrieval by synthesizing direct answers from retrieved sources, ensuring source visibility becomes a significant challenge. Improving it through targeted content revisions is a practical strategy termed Generative Engine Optimization (GEO). However, optimizing a document for diverse queries presents a constrained optimization challenge where heterogeneous queries often impose conflicting and competing revision requirements under a limited content budget. To address this challenge, we propose IF-GEO, a \"diverge-then-converge\" framework comprising two phases: (i) mining distinct optimization preferences from representative latent queries; (ii) synthesizing a Global Revision Blueprint for guided editing by coordinating preferences via conflict-aware instruction fusion. To explicitly quantify IF-GEO's objective of cross-query stability, we introduce risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate that IF-GEO achieves substantial performance gains while maintaining robustness across diverse retrieval scenarios.",
    "authors": [
      "Heyang Zhou",
      "JiaJia Chen",
      "Xiaolu Chen",
      "Jie Bao",
      "Zhen Chen",
      "Yong Liao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13938v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13938v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13887v1",
    "title": "Human Simulation Computation: A Human-Inspired Framework for Adaptive AI Systems",
    "summary": "Large language models (LLMs) have demonstrated strong capabilities in knowledge representation and reasoning based on textual data. However, their reliance on language material alone limits their ability to adapt, verify reasoning outcomes, and operate effectively in open and dynamic real-world environments. In this paper, we propose Human Simulation Computation (HSC), a human-inspired computational framework that models intelligence as a continuous, closed-loop process involving thinking, action, learning, reflection, and activity scheduling, collectively referred to as the internal reasoning process. HSC emphasizes active participation both within the internal reasoning process and in interactions with the environment, where actions are used not only to achieve goals but also to automatically refine and improve internal reasoning mechanisms without external intervention. Furthermore, HSC incorporates commonly used human thinking strategies across all stages of the internal reasoning process, such as main-feature-oriented reasoning, scope expansion through action, and on-time learning driven by environmental feedback. Through theoretical analysis, we argue that human simulation strategies cannot be fully learned from language material alone, and that human-like reasoning processes and action-grounded reasoning methods are essential for robust adaptation and effective interaction with real-world environments.",
    "authors": [
      "Hong Su"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13887v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13887v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13880v1",
    "title": "LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal Health Assistants in Digital Health",
    "summary": "Personalized digital health support requires long-horizon, cross-dimensional reasoning over heterogeneous lifestyle signals, and recent advances in mobile sensing and large language models (LLMs) make such support increasingly feasible. However, the capabilities of current LLMs in this setting remain unclear due to the lack of systematic benchmarks. In this paper, we introduce LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional, and multi-user lifestyle health reasoning, containing 22,573 questions spanning from basic retrieval to complex reasoning. We release an extensible benchmark construction pipeline and a standardized evaluation protocol to enable reliable and scalable assessment of LLM-based health assistants. We then systematically evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in long-horizon aggregation and cross-dimensional reasoning. Motivated by these findings, we propose LifeAgent as a strong baseline agent for health assistant that integrates multi-step evidence retrieval with deterministic aggregation, achieving significant improvements compared with two widely used baselines. Case studies further demonstrate its potential in realistic daily-life scenarios. The benchmark is publicly available at https://anonymous.4open.science/r/LifeAgentBench-CE7B.",
    "authors": [
      "Ye Tian",
      "Zihao Wang",
      "Onat Gungor",
      "Xiaoran Fan",
      "Tajana Rosing"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13880v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13880v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13745v1",
    "title": "Variational Dual-path Attention Network for CSI-Based Gesture Recognition",
    "summary": "Wi-Fi gesture recognition based on Channel State Information (CSI) is challenged by high-dimensional noise and resource constraints on edge devices. Prevailing end-to-end models tightly couple feature extraction with classification, overlooking the inherent time-frequency sparsity of CSI and leading to redundancy and poor generalization. To address this, this paper proposes a lightweight feature preprocessing module--the Variational Dual-path Attention Network (VDAN). It performs structured feature refinement through frequency-domain filtering and temporal detection. Variational inference is introduced to model the uncertainty in attention weights, thereby enhancing robustness to noise. The design principles of the module are explained from the perspectives of the information bottleneck and regularization. Experiments on a public dataset demonstrate that the learned attention weights align with the physical sparse characteristics of CSI, verifying its interpretability. This work provides an efficient and explainable front-end processing solution for resource-constrained wireless sensing systems.",
    "authors": [
      "N. Zhang"
    ],
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13745v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13745v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13735v1",
    "title": "Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N Selection",
    "summary": "Probabilistic confidence metrics are increasingly adopted as proxies for reasoning quality in Best-of-N selection, under the assumption that higher confidence reflects higher reasoning fidelity. In this work, we challenge this assumption by investigating whether these metrics truly capture inter-step causal dependencies necessary for valid reasoning. We introduce three classes of inter-step causality perturbations that systematically disrupt dependencies between reasoning steps while preserving local fluency. Surprisingly, across diverse model families and reasoning benchmarks, we find that selection accuracy degrades only marginally under these disruptions. Even severe interventions, such as applying hard attention masks that directly prevent the model from attending to prior reasoning steps, do not substantially reduce selection performance. These findings provide strong evidence that current probabilistic metrics are largely insensitive to logical structure, and primarily capture surface-level fluency or in-distribution priors instead. Motivated by this gap, we propose a contrastive causality metric that explicitly isolates inter-step causal dependencies, and demonstrate that it yields more faithful output selection than existing probability-based approaches.",
    "authors": [
      "Hojin Kim",
      "Jaehyung Kim"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13735v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13735v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13715v1",
    "title": "MVGD-Net: A Novel Motion-aware Video Glass Surface Detection Network",
    "summary": "Glass surface ubiquitous in both daily life and professional environments presents a potential threat to vision-based systems, such as robot and drone navigation. To solve this challenge, most recent studies have shown significant interest in Video Glass Surface Detection (VGSD). We observe that objects in the reflection (or transmission) layer appear farther from the glass surfaces. Consequently, in video motion scenarios, the notable reflected (or transmitted) objects on the glass surface move slower than objects in non-glass regions within the same spatial plane, and this motion inconsistency can effectively reveal the presence of glass surfaces. Based on this observation, we propose a novel network, named MVGD-Net, for detecting glass surfaces in videos by leveraging motion inconsistency cues. Our MVGD-Net features three novel modules: the Cross-scale Multimodal Fusion Module (CMFM) that integrates extracted spatial features and estimated optical flow maps, the History Guided Attention Module (HGAM) and Temporal Cross Attention Module (TCAM), both of which further enhances temporal features. A Temporal-Spatial Decoder (TSD) is also introduced to fuse the spatial and temporal features for generating the glass region mask. Furthermore, for learning our network, we also propose a large-scale dataset, which comprises 312 diverse glass scenarios with a total of 19,268 frames. Extensive experiments demonstrate that our MVGD-Net outperforms relevant state-of-the-art methods.",
    "authors": [
      "Yiwei Lu",
      "Hao Huang",
      "Tao Yan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13715v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13715v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13704v1",
    "title": "Performance and Complexity Trade-off Optimization of Speech Models During Training",
    "summary": "In speech machine learning, neural network models are typically designed by choosing an architecture with fixed layer sizes and structure. These models are then trained to maximize performance on metrics aligned with the task's objective. While the overall architecture is usually guided by prior knowledge of the task, the sizes of individual layers are often chosen heuristically. However, this approach does not guarantee an optimal trade-off between performance and computational complexity; consequently, post hoc methods such as weight quantization or model pruning are typically employed to reduce computational cost. This occurs because stochastic gradient descent (SGD) methods can only optimize differentiable functions, while factors influencing computational complexity, such as layer sizes and floating-point operations per second (FLOP/s), are non-differentiable and require modifying the model structure during training. We propose a reparameterization technique based on feature noise injection that enables joint optimization of performance and computational complexity during training using SGD-based methods. Unlike traditional pruning methods, our approach allows the model size to be dynamically optimized for a target performance-complexity trade-off, without relying on heuristic criteria to select which weights or structures to remove. We demonstrate the effectiveness of our method through three case studies, including a synthetic example and two practical real-world applications: voice activity detection and audio anti-spoofing. The code related to our work is publicly available to encourage further research.",
    "authors": [
      "Esteban Gómez",
      "Tom Bäckström"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13704v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13704v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13698v1",
    "title": "Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via Chernoff Information Neural Estimation",
    "summary": "Fairness and privacy are two vital pillars of trustworthy machine learning. Despite extensive research on these individual topics, the relationship between fairness and privacy has received significantly less attention. In this paper, we utilize the information-theoretic measure Chernoff Information to highlight the data-dependent nature of the relationship among the triad of fairness, privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that allows us to analyze the relationship among the triad simultaneously. We then show that for synthetic data, this value behaves in 3 distinct ways (depending on the distribution of the data). We highlight the data distributions involved in these cases and explore their fairness and privacy implications. Additionally, we show that Noisy Chernoff Difference acts as a proxy for the steepness of the fairness-accuracy curves. Finally, we propose a method for estimating Chernoff Information on data from unknown distributions and utilize this framework to examine the triad dynamic on real datasets. This work builds towards a unified understanding of the fairness-privacy-accuracy relationship and highlights its data-dependent nature.",
    "authors": [
      "Arjun Nichani",
      "Hsiang Hsu",
      " Chun-Fu",
      " Chen",
      "Haewon Jeong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13698v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13698v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.13677v1",
    "title": "Finally Outshining the Random Baseline: A Simple and Effective Solution for Active Learning in 3D Biomedical Imaging",
    "summary": "Active learning (AL) has the potential to drastically reduce annotation costs in 3D biomedical image segmentation, where expert labeling of volumetric data is both time-consuming and expensive. Yet, existing AL methods are unable to consistently outperform improved random sampling baselines adapted to 3D data, leaving the field without a reliable solution. We introduce Class-stratified Scheduled Power Predictive Entropy (ClaSP PE), a simple and effective query strategy that addresses two key limitations of standard uncertainty-based AL methods: class imbalance and redundancy in early selections. ClaSP PE combines class-stratified querying to ensure coverage of underrepresented structures and log-scale power noising with a decaying schedule to enforce query diversity in early-stage AL and encourage exploitation later. In our evaluation on 24 experimental settings using four 3D biomedical datasets within the comprehensive nnActive benchmark, ClaSP PE is the only method that generally outperforms improved random baselines in terms of both segmentation quality with statistically significant gains, whilst remaining annotation efficient. Furthermore, we explicitly simulate the real-world application by testing our method on four previously unseen datasets without manual adaptation, where all experiment parameters are set according to predefined guidelines. The results confirm that ClaSP PE robustly generalizes to novel tasks without requiring dataset-specific tuning. Within the nnActive framework, we present compelling evidence that an AL method can consistently outperform random baselines adapted to 3D segmentation, in terms of both performance and annotation efficiency in a realistic, close-to-production scenario. Our open-source implementation and clear deployment guidelines make it readily applicable in practice. Code is at https://github.com/MIC-DKFZ/nnActive.",
    "authors": [
      "Carsten T. Lüth",
      "Jeremias Traub",
      "Kim-Celine Kahl",
      "Till J. Bungert",
      "Lukas Klein",
      "Lars Krämer",
      "Paul F. Jäger",
      "Klaus Maier-Hein",
      "Fabian Isensee"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13677v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13677v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.14238v1",
    "title": "Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression",
    "summary": "Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \\textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.",
    "authors": [
      "Shaurya Mathur",
      "Shreyas Bellary Manjunath",
      "Nitin Kulkarni",
      "Alina Vereshchaka"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14238v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14238v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14173v1",
    "title": "Penalizing Localized Dirichlet Energies in Low Rank Tensor Products",
    "summary": "We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To address this limitation, we propose a novel regularization strategy based on local Dirichlet energies defined on small hypercubes centered at the training points. Leveraging pretrained TPBS models, we also introduce two estimators for inference from incomplete samples. Comparative experiments with neural networks demonstrate that TPBS models outperform neural networks in the overfitting regime for most datasets, and maintain competitive performance otherwise. Overall, TPBS models exhibit greater robustness to overfitting and consistently benefit from regularization, while neural networks are more sensitive to overfitting and less effective in leveraging regularization.",
    "authors": [
      "Paris A. Karakasis",
      "Nicholas D. Sidiropoulos"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14173v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14173v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14123v1",
    "title": "A Systematic Analysis of Chunking Strategies for Reliable Question Answering",
    "summary": "We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ~5k tokens; (iii) a \"context cliff\" reduces quality beyond ~2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).",
    "authors": [
      "Sofia Bennani",
      "Charles Moslonka"
    ],
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14123v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14123v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14115v1",
    "title": "Riemannian Liquid Spatio-Temporal Graph Network",
    "summary": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io",
    "authors": [
      "Liangsi Lu",
      "Jingchao Wang",
      "Zhaorong Dai",
      "Hanqian Liu",
      "Yang Shi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14115v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14115v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14112v1",
    "title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns",
    "summary": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.",
    "authors": [
      "George Mihaila"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14112v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14112v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14105v1",
    "title": "Truth with a Twist: The Rhetoric of Persuasion in Professional vs. Community-Authored Fact-Checks",
    "summary": "This study presents the first large-scale comparison of persuasion techniques present in crowd- versus professionally-written debunks. Using extensive datasets from Community Notes (CNs), EUvsDisinfo, and the Database of Known Fakes (DBKF), we quantify the prevalence and types of persuasion techniques across these fact-checking ecosystems. Contrary to prior hypothesis that community-produced debunks rely more heavily on subjective or persuasive wording, we find no evidence that CNs contain a higher average number of persuasion techniques than professional fact-checks. We additionally identify systematic rhetorical differences between CNs and professional debunking efforts, reflecting differences in institutional norms and topical coverage. Finally, we examine how the crowd evaluates persuasive language in CNs and show that, although notes with more persuasive elements receive slightly higher overall helpfulness ratings, crowd raters are effective at penalising the use of particular problematic rhetorical means",
    "authors": [
      "Olesya Razuvayevskaya",
      "Kalina Bontcheva"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14105v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14105v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14084v1",
    "title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning",
    "summary": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.",
    "authors": [
      "Abdurrahim Yilmaz",
      "Ozan Erdem",
      "Ece Gokyayla",
      "Ayda Acar",
      "Burc Bugra Dagtas",
      "Dilara Ilhan Erdil",
      "Gulsum Gencoglan",
      "Burak Temelkuran"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14084v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14084v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14054v1",
    "title": "SecureSplit: Mitigating Backdoor Attacks in Split Learning",
    "summary": "Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.",
    "authors": [
      "Zhihao Dou",
      "Dongfei Cui",
      "Weida Wang",
      "Anjun Gao",
      "Yueyang Quan",
      "Mengyao Ma",
      "Viet Vo",
      "Guangdong Bai",
      "Zhuqing Liu",
      "Minghong Fang"
    ],
    "categories": [
      "cs.CR",
      "cs.DC",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14054v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14054v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14046v1",
    "title": "PRiSM: Benchmarking Phone Realization in Speech Models",
    "summary": "Phone recognition (PR) serves as the atomic interface for language-agnostic modeling for cross-lingual speech processing and phonetic analysis. Despite prolonged efforts in developing PR systems, current evaluations only measure surface-level transcription accuracy. We introduce PRiSM, the first open-source benchmark designed to expose blind spots in phonetic perception through intrinsic and extrinsic evaluation of PR systems. PRiSM standardizes transcription-based evaluation and assesses downstream utility in clinical, educational, and multilingual settings with transcription and representation probes. We find that diverse language exposure during training is key to PR performance, encoder-CTC models are the most stable, and specialized PR models still outperform Large Audio Language Models. PRiSM releases code, recipes, and datasets to move the field toward multilingual speech models with robust phonetic ability: https://github.com/changelinglab/prism.",
    "authors": [
      "Shikhar Bharadwaj",
      "Chin-Jou Li",
      "Yoonjae Kim",
      "Kwanghee Choi",
      "Eunjung Yeo",
      "Ryan Soh-Eun Shim",
      "Hanyu Zhou",
      "Brendon Boldt",
      "Karen Rosero Jacome",
      "Kalvin Chang",
      "Darsh Agrawal",
      "Keer Xu",
      "Chao-Han Huck Yang",
      "Jian Zhu",
      "Shinji Watanabe",
      "David R. Mortensen"
    ],
    "categories": [
      "cs.CL",
      "cs.SD"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14046v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14046v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14004v1",
    "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models",
    "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.",
    "authors": [
      "Hengyuan Zhang",
      "Zhihao Zhang",
      "Mingyang Wang",
      "Zunhai Su",
      "Yiwei Wang",
      "Qianli Wang",
      "Shuzhou Yuan",
      "Ercong Nie",
      "Xufeng Duan",
      "Qibo Xue",
      "Zeping Yu",
      "Chenming Shang",
      "Xiao Liang",
      "Jing Xiong",
      "Hui Shen",
      "Chaofan Tao",
      "Zhengwu Liu",
      "Senjie Jin",
      "Zhiheng Xi",
      "Dongdong Zhang",
      "Sophia Ananiadou",
      "Tao Gui",
      "Ruobing Xie",
      "Hayden Kwok-Hay So",
      "Hinrich Schütze",
      "Xuanjing Huang",
      "Qi Zhang",
      "Ngai Wong"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14004v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14004v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14000v1",
    "title": "Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior",
    "summary": "Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.",
    "authors": [
      "Junwoo Chang",
      "Joseph Park",
      "Roberto Horowitz",
      "Jongmin Lee",
      "Jongeun Choi"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14000v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14000v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.13999v1",
    "title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification",
    "summary": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.",
    "authors": [
      "Youngmoon Jung",
      "Joon-Young Yang",
      "Ju-ho Kim",
      "Jaeyoung Roh",
      "Chang Woo Han",
      "Hoon-Young Cho"
    ],
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13999v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13999v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.13886v1",
    "title": "Revisiting Multi-Task Visual Representation Learning",
    "summary": "Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity \"expert\" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves \"best-of-both-worlds\" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.",
    "authors": [
      "Shangzhe Di",
      "Zhonghua Zhai",
      "Weidi Xie"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13886v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13886v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.13844v1",
    "title": "Optimal L2 Regularization in High-dimensional Continual Linear Regression",
    "summary": "We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.",
    "authors": [
      "Gilad Karpel",
      "Edward Moroshko",
      "Ran Levinstein",
      "Ron Meir",
      "Daniel Soudry",
      "Itay Evron"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13844v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13844v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.13751v1",
    "title": "HiT: History-Injection Transformers for Onboard Continuous Flood Change Detection",
    "summary": "Natural disaster monitoring through continuous satellite observation requires processing multi-temporal data under strict operational constraints. This paper addresses flood detection, a critical application for hazard management, by developing an onboard change detection system that operates within the memory and computational limits of small satellites. We propose History Injection mechanism for Transformer models (HiT), that maintains historical context from previous observations while reducing data storage by over 99\\% of original image size. Moreover, testing on the STTORM-CD flood dataset confirms that the HiT mechanism within the Prithvi-tiny foundation model maintains detection accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in nanosats. This work establishes a practical framework for satellite-based continuous monitoring of natural disasters, supporting real-time hazard assessment without dependency on ground-based processing infrastructure. Architecture as well as model checkpoints is available at https://github.com/zaitra/HiT-change-detection",
    "authors": [
      "Daniel Kyselica",
      "Jonáš Herec",
      "Oliver Kutis",
      "Rado Pitoňák"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13751v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13751v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.13749v1",
    "title": "Pro-AI Bias in Large Language Models",
    "summary": "Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.",
    "authors": [
      "Benaya Trabelsi",
      "Jonathan Shaki",
      "Sarit Kraus"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13749v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13749v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.14212v1",
    "title": "Generalization and Completeness of Stochastic Local Search Algorithms",
    "summary": "We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO) are presented. Then, we use our model to prove the Turing-completeness of SLS algorithms in general. The proof uses our framework to construct a GA able to simulate any Turing machine. This Turing-completeness implies that determining any non-trivial property concerning the relationship between the inputs and the computed outputs is undecidable for GA and, by extension, for the general set of SLS methods (although not necessarily for each particular method). Similar proofs are more informally presented for PSO and ACO.",
    "authors": [
      "Daniel Loscos",
      "Narciso Marti-Oliet",
      "Ismael Rodriguez"
    ],
    "categories": [
      "cs.NE",
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14212v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14212v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14196v1",
    "title": "Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery",
    "summary": "Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.",
    "authors": [
      "Albina Galiullina",
      "Wouter van Heeswijk",
      "Tom van Woensel"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14196v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14196v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14124v1",
    "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic",
    "summary": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.",
    "authors": [
      "Saad Mankarious",
      "Aya Zirikly"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14124v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14124v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14066v1",
    "title": "VERIDAH: Solving Enumeration Anomaly Aware Vertebra Labeling across Imaging Sequences",
    "summary": "The human spine commonly consists of seven cervical, twelve thoracic, and five lumbar vertebrae. However, enumeration anomalies may result in individuals having eleven or thirteen thoracic vertebrae and four or six lumbar vertebrae. Although the identification of enumeration anomalies has potential clinical implications for chronic back pain and operation planning, the thoracolumbar junction is often poorly assessed and rarely described in clinical reports. Additionally, even though multiple deep-learning-based vertebra labeling algorithms exist, there is a lack of methods to automatically label enumeration anomalies. Our work closes that gap by introducing \"Vertebra Identification with Anomaly Handling\" (VERIDAH), a novel vertebra labeling algorithm based on multiple classification heads combined with a weighted vertebra sequence prediction algorithm. We show that our approach surpasses existing models on T2w TSE sagittal (98.30% vs. 94.24% of subjects with all vertebrae correctly labeled, p < 0.001) and CT imaging (99.18% vs. 77.26% of subjects with all vertebrae correctly labeled, p < 0.001) and works in arbitrary field-of-view images. VERIDAH correctly labeled the presence 2 Möller et al. of thoracic enumeration anomalies in 87.80% and 96.30% of T2w and CT images, respectively, and lumbar enumeration anomalies in 94.48% and 97.22% for T2w and CT, respectively. Our code and models are available at: https://github.com/Hendrik-code/spineps.",
    "authors": [
      "Hendrik Möller",
      "Hanna Schoen",
      "Robert Graf",
      "Matan Atad",
      "Nathan Molinier",
      "Anjany Sekuboyina",
      "Bettina K. Budai",
      "Fabian Bamberg",
      "Steffen Ringhof",
      "Christopher Schlett",
      "Tobias Pischon",
      "Thoralf Niendorf",
      "Josua A. Decker",
      "Marc-André Weber",
      "Bjoern Menze",
      "Daniel Rueckert",
      "Jan S. Kirschke"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14066v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14066v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14055v1",
    "title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI",
    "summary": "Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.",
    "authors": [
      "Andrea Protani",
      "Marc Molina Van Den Bosch",
      "Lorenzo Giusti",
      "Heloisa Barbosa Da Silva",
      "Paolo Cacace",
      "Albert Sund Aillet",
      "Miguel Angel Gonzalez Ballester",
      "Friedhelm Hummel",
      "Luigi Serio"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14055v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14055v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14038v1",
    "title": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving",
    "summary": "Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.",
    "authors": [
      "Alexandre Justo Miro",
      "Ludvig af Klinteberg",
      "Bogdan Timus",
      "Aron Asefaw",
      "Ajinkya Khoche",
      "Thomas Gustafsson",
      "Sina Sharif Mansouri",
      "Masoud Daneshtalab"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14038v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14038v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14026v1",
    "title": "Universal Approximation Theorem for Input-Connected Multilayer Perceptrons",
    "summary": "We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a feedforward neural network architecture in which each hidden neuron receives, in addition to the outputs of the preceding layer, a direct affine connection from the raw input. We first study this architecture in the univariate setting and give an explicit and systematic description of IC-MLPs with an arbitrary finite number of hidden layers, including iterated formulas for the network functions. In this setting, we prove a universal approximation theorem showing that deep IC-MLPs can approximate any continuous function on a closed interval of the real line if and only if the activation function is nonlinear. We then extend the analysis to vector-valued inputs and establish a corresponding universal approximation theorem for continuous functions on compact subsets of $\\mathbb{R}^n$.",
    "authors": [
      "Vugar Ismailov"
    ],
    "categories": [
      "cs.LG",
      "cs.NE",
      "math.FA"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14026v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14026v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14007v1",
    "title": "BACH-V: Bridging Abstract and Concrete Human-Values in Large Language Models",
    "summary": "Do large language models (LLMs) genuinely understand abstract concepts, or merely manipulate them as statistical patterns? We introduce an abstraction-grounding framework that decomposes conceptual understanding into three capacities: interpretation of abstract concepts (Abstract-Abstract, A-A), grounding of abstractions in concrete events (Abstract-Concrete, A-C), and application of abstract principles to regulate concrete decisions (Concrete-Concrete, C-C). Using human values as a testbed - given their semantic richness and centrality to alignment - we employ probing (detecting value traces in internal activations) and steering (modifying representations to shift behavior). Across six open-source LLMs and ten value dimensions, probing shows that diagnostic probes trained solely on abstract value descriptions reliably detect the same values in concrete event narratives and decision reasoning, demonstrating cross-level transfer. Steering reveals an asymmetry: intervening on value representations causally shifts concrete judgments and decisions (A-C, C-C), yet leaves abstract interpretations unchanged (A-A), suggesting that encoded abstract values function as stable anchors rather than malleable activations. These findings indicate LLMs maintain structured value representations that bridge abstraction and action, providing a mechanistic and operational foundation for building value-driven autonomous AI systems with more transparent, generalizable alignment and control.",
    "authors": [
      "Junyu Zhang",
      "Yipeng Kang",
      "Jiong Guo",
      "Jiayu Zhan",
      "Junqi Wang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14007v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14007v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.13989v1",
    "title": "A universal linearized subspace refinement framework for neural networks",
    "summary": "Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.",
    "authors": [
      "Wenbo Cao",
      "Weiwei Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13989v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13989v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.13976v1",
    "title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation",
    "summary": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.",
    "authors": [
      "Jing Zuo",
      "Lingzhou Mu",
      "Fan Jiang",
      "Chengcheng Ma",
      "Mu Xu",
      "Yonggang Qi"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13976v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13976v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.13882v1",
    "title": "OpenLearnLM Benchmark: A Unified Framework for Evaluating Knowledge, Skill, and Attitude in Educational Large Language Models",
    "summary": "Large Language Models are increasingly deployed as educational tools, yet existing benchmarks focus on narrow skills and lack grounding in learning sciences. We introduce OpenLearnLM Benchmark, a theory-grounded framework evaluating LLMs across three dimensions derived from educational assessment theory: Knowledge (curriculum-aligned content and pedagogical understanding), Skills (scenario-based competencies organized through a four-level center-role-scenario-subscenario hierarchy), and Attitude (alignment consistency and deception resistance). Our benchmark comprises 124K+ items spanning multiple subjects, educational roles, and difficulty levels based on Bloom's taxonomy. The Knowledge domain prioritizes authentic assessment items from established benchmarks, while the Attitude domain adapts Anthropic's Alignment Faking methodology to detect behavioral inconsistency under varying monitoring conditions. Evaluation of seven frontier models reveals distinct capability profiles: Claude-Opus-4.5 excels in practical skills despite lower content knowledge, while Grok-4.1-fast leads in knowledge but shows alignment concerns. Notably, no single model dominates all dimensions, validating the necessity of multi-axis evaluation. OpenLearnLM provides an open, comprehensive framework for advancing LLM readiness in authentic educational contexts.",
    "authors": [
      "Unggi Lee",
      "Sookbun Lee",
      "Heungsoo Choi",
      "Jinseo Lee",
      "Haeun Park",
      "Younghoon Jeon",
      "Sungmin Cho",
      "Minju Kang",
      "Junbo Koh",
      "Jiyeong Bae",
      "Minwoo Nam",
      "Juyeon Eun",
      "Yeonji Jung",
      "Yeil Jeong"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13882v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13882v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.13816v1",
    "title": "Discriminant Learning-based Colorspace for Blade Segmentation",
    "summary": "Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.",
    "authors": [
      "Raül Pérez-Gonzalo",
      "Andreas Espersen",
      "Antonio Agudo"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13816v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13816v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.13770v1",
    "title": "Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in Point-in-Time LLMs for Finance",
    "summary": "We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and practical financial workflows. Unlike most existing approaches that primarily test inner lookahead knowledge via Q\\\\&A, our benchmark evaluates model behavior in practical scenarios. To distinguish genuine predictive capability from memorization-based performance, we analyze performance decay across temporally distinct market regimes, incorporating several quantitative baselines to establish performance thresholds. We evaluate prominent open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in standard LLMs, as measured with alpha decay, unlike Pitinf models, which demonstrate improved generalization and reasoning abilities as they scale in size. This work establishes a foundation for the standardized evaluation of temporal bias in financial LLMs and provides a practical framework for identifying models suitable for real-world deployment. Code is available on GitHub: https://github.com/benstaf/lookaheadbench",
    "authors": [
      "Mostapha Benhenda"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-fin.CP",
      "q-fin.GN"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13770v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13770v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.13724v1",
    "title": "Facial Spatiotemporal Graphs: Leveraging the 3D Facial Surface for Remote Physiological Measurement",
    "summary": "Facial remote photoplethysmography (rPPG) methods estimate physiological signals by modeling subtle color changes on the 3D facial surface over time. However, existing methods fail to explicitly align their receptive fields with the 3D facial surface-the spatial support of the rPPG signal. To address this, we propose the Facial Spatiotemporal Graph (STGraph), a novel representation that encodes facial color and structure using 3D facial mesh sequences-enabling surface-aligned spatiotemporal processing. We introduce MeshPhys, a lightweight spatiotemporal graph convolutional network that operates on the STGraph to estimate physiological signals. Across four benchmark datasets, MeshPhys achieves state-of-the-art or competitive performance in both intra- and cross-dataset settings. Ablation studies show that constraining the model's receptive field to the facial surface acts as a strong structural prior, and that surface-aligned, 3D-aware node features are critical for robustly encoding facial surface color. Together, the STGraph and MeshPhys constitute a novel, principled modeling paradigm for facial rPPG, enabling robust, interpretable, and generalizable estimation. Code is available at https://samcantrill.github.io/facial-stgraph-rppg/ .",
    "authors": [
      "Sam Cantrill",
      "David Ahmedt-Aristizabal",
      "Lars Petersson",
      "Hanna Suominen",
      "Mohammad Ali Armin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13724v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13724v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.14249v1",
    "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "summary": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model's current behavior but overlooking more informative ones. Addressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory. RSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment. Concretely, RSR is defined as the ratio of a trajectory's average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret. Across five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics. We further demonstrate its practical utility in both trajectory selection and teacher selection.",
    "authors": [
      "Yuming Yang",
      "Mingyoung Lai",
      "Wanxu Zhao",
      "Xiaoran Fan",
      "Zhiheng Xi",
      "Mingqi Wu",
      "Chiyue Huang",
      "Jun Zhao",
      "Haijun Lv",
      "Jian Tong",
      "Yunhua Zhou",
      "Yicheng Zou",
      "Qipeng Guo",
      "Tao Gui",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14249v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14249v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.13975v1",
    "title": "Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains",
    "summary": "Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic \"Context Collapse\" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.",
    "authors": [
      "Marco Piccolo",
      "Qiwei Han",
      "Astrid van Toor",
      "Joachim Vanneste"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13975v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13975v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.13797v1",
    "title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval",
    "summary": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.",
    "authors": [
      "Gabriele Serussi",
      "David Vainshtein",
      "Jonathan Kouchly",
      "Dotan Di Castro",
      "Chaim Baskin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13797v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13797v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.13722v1",
    "title": "OP-Bench: Benchmarking Over-Personalization for Memory-Augmented Personalized Conversational Agents",
    "summary": "Memory-augmented conversational agents enable personalized interactions using long-term user memory and have gained substantial traction. However, existing benchmarks primarily focus on whether agents can recall and apply user information, while overlooking whether such personalization is used appropriately. In fact, agents may overuse personal information, producing responses that feel forced, intrusive, or socially inappropriate to users. We refer to this issue as \\emph{over-personalization}. In this work, we formalize over-personalization into three types: Irrelevance, Repetition, and Sycophancy, and introduce \\textbf{OP-Bench} a benchmark of 1,700 verified instances constructed from long-horizon dialogue histories. Using \\textbf{OP-Bench}, we evaluate multiple large language models and memory-augmentation methods, and find that over-personalization is widespread when memory is introduced. Further analysis reveals that agents tend to retrieve and over-attend to user memories even when unnecessary. To address this issue, we propose \\textbf{Self-ReCheck}, a lightweight, model-agnostic memory filtering mechanism that mitigates over-personalization while preserving personalization performance. Our work takes an initial step toward more controllable and appropriate personalization in memory-augmented dialogue systems.",
    "authors": [
      "Yulin Hu",
      "Zimo Long",
      "Jiahe Guo",
      "Xingyu Sui",
      "Xing Fu",
      "Weixiang Zhao",
      "Yanyan Zhao",
      "Bing Qin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13722v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13722v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.14251v1",
    "title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR",
    "summary": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.",
    "authors": [
      "Said Taghadouini",
      "Adrien Cavaillès",
      "Baptiste Aubertin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14251v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14251v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.14250v1",
    "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer",
    "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.",
    "authors": [
      "Pengze Zhang",
      "Yanze Wu",
      "Mengtian Li",
      "Xu Bai",
      "Songtao Zhao",
      "Fulong Ye",
      "Chong Mou",
      "Xinghui Li",
      "Zhuowei Chen",
      "Qian He",
      "Mingyuan Gao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14250v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14250v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.14242v1",
    "title": "APEX-Agents",
    "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.",
    "authors": [
      "Bertie Vidgen",
      "Austin Mann",
      "Abby Fennelly",
      "John Wright Stanly",
      "Lucas Rothman",
      "Marco Burstein",
      "Julien Benchek",
      "David Ostrofsky",
      "Anirudh Ravichandran",
      "Debnil Sur",
      "Neel Venugopal",
      "Alannah Hsia",
      "Isaac Robinson",
      "Calix Huang",
      "Olivia Varones",
      "Daniyal Khan",
      "Michael Haines",
      "Zach Richards",
      "Chirag Mahapatra",
      "Brendan Foody",
      "Osvald Nitski"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14242v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14242v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.14096v1",
    "title": "Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems",
    "summary": "The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.",
    "authors": [
      "Benedikt Hartl",
      "Léo Pio-Lopez",
      "Chris Fields",
      "Michael Levin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14096v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14096v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.14087v1",
    "title": "'1'-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators",
    "summary": "Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on '1'-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\\% BT reduction compared to 20.42% of precise implementation.",
    "authors": [
      "Ruichi Han",
      "Yizhi Chen",
      "Tong Lei",
      "Jordi Altayo Gonzalez",
      "Ahmed Hemani"
    ],
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14087v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14087v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.14027v1",
    "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics",
    "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.",
    "authors": [
      "Junqi Liu",
      "Zihao Zhou",
      "Zekai Zhu",
      "Marco Dos Santos",
      "Weikun He",
      "Jiawei Liu",
      "Ran Wang",
      "Yunzhou Xie",
      "Junqiao Zhao",
      "Qiufeng Wang",
      "Lihong Zhi",
      "Jia Li",
      "Wenda Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14027v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14027v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13969v1",
    "title": "Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval",
    "summary": "Retrieving evidence for language model queries from knowledge graphs requires balancing broad search across the graph with multi-hop traversal to follow relational links. Similarity-based retrievers provide coverage but remain shallow, whereas traversal-based methods rely on selecting seed nodes to start exploration, which can fail when queries span multiple entities and relations. We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that gives a language model control over this breadth-depth tradeoff using a two-operation toolset: global lexical search over node descriptors and one-hop neighborhood exploration that composes into multi-hop traversal. ARK alternates between breadth-oriented discovery and depth-oriented expansion without depending on a fragile seed selection, a pre-set hop depth, or requiring retrieval training. ARK adapts tool use to queries, using global search for language-heavy queries and neighborhood exploration for relation-heavy queries. On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving average Hit@1 by up to 31.4% and average MRR by up to 28.0% over retrieval-based and agentic training-free methods. Finally, we distill ARK's tool-use trajectories from a large teacher into an 8B model via label-free imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining up to 98.5% of the teacher's Hit@1 rate.",
    "authors": [
      "Joaquín Polonuer",
      "Lucas Vittor",
      "Iñaki Arango",
      "Ayush Noori",
      "David A. Clifton",
      "Luciano Del Corro",
      "Marinka Zitnik"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13969v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13969v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13931v1",
    "title": "Towards Effective Negation Modeling in Joint Audio-Text Models for Music",
    "summary": "Joint audio-text models are widely used for music retrieval, yet they struggle with semantic phenomena such as negation. Negation is fundamental for distinguishing the absence (or presence) of musical elements (e.g., \"with vocals\" vs. \"without vocals\"), but current systems fail to represent this reliably. In this work, we investigate and mitigate this limitation by training CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD captions. We introduce negation through text augmentation and a dissimilarity-based contrastive loss, designed to explicitly separate original and negated captions in the joint embedding space. To evaluate progress, we propose two protocols that frame negation modeling as retrieval and binary classification tasks. Experiments demonstrate that both methods, individually and combined, improve negation handling while largely preserving retrieval performance.",
    "authors": [
      "Yannis Vasilakis",
      "Rachel Bittner",
      "Johan Pauwels"
    ],
    "categories": [
      "cs.SD",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13931v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13931v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13846v1",
    "title": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments",
    "summary": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.",
    "authors": [
      "Glinskaya Maria"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13846v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13846v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13837v1",
    "title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation",
    "summary": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \\OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.",
    "authors": [
      "Xinya Ji",
      "Sebastian Weiss",
      "Manuel Kansy",
      "Jacek Naruniec",
      "Xun Cao",
      "Barbara Solenthaler",
      "Derek Bradley"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13837v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13837v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13817v1",
    "title": "Device Association and Resource Allocation for Hierarchical Split Federated Learning in Space-Air-Ground Integrated Network",
    "summary": "6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground Integrated Network (SAGIN), yet FL confronts challenges such as resource constrained and unbalanced data distribution. To address these issues, this paper proposes a Hierarchical Split Federated Learning (HSFL) framework and derives its upper bound of loss function. To minimize the weighted sum of training loss and latency, we formulate a joint optimization problem that integrates device association, model split layer selection, and resource allocation. We decompose the original problem into several subproblems, where an iterative optimization algorithm for device association and resource allocation based on brute-force split point search is proposed. Simulation results demonstrate that the proposed algorithm can effectively balance training efficiency and model accuracy for FL in SAGIN.",
    "authors": [
      "Haitao Zhao",
      "Xiaoyu Tang",
      "Bo Xu",
      "Jinlong Sun",
      "Linghao Zhang"
    ],
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13817v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13817v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13798v1",
    "title": "Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders",
    "summary": "Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.",
    "authors": [
      "Kai Wittenmayer",
      "Sukrut Rao",
      "Amin Parchami-Araghi",
      "Bernt Schiele",
      "Jonas Fischer"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13798v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13798v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.13709v1",
    "title": "Hidden in Plain Text: Measuring LLM Deception Quality Against Human Baselines Using Social Deduction Games",
    "summary": "Large Language Model (LLM) agents are increasingly used in many applications, raising concerns about their safety. While previous work has shown that LLMs can deceive in controlled tasks, less is known about their ability to deceive using natural language in social contexts. In this paper, we study deception in the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving others through conversation. Unlike previous SDG studies, we use an asynchronous multi-agent framework which better simulates realistic social contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a Mafia Detector using GPT-4-Turbo to analyze game transcripts without player role information to predict the mafia players. We use prediction accuracy as a surrogate marker for deception quality. We compare this prediction accuracy to that of 28 human games and a random baseline. Results show that the Mafia Detector's mafia prediction accuracy is lower on LLM games than on human games. The result is consistent regardless of the game days and the number of mafias detected. This indicates that LLMs blend in better and thus deceive more effectively. We also release a dataset of LLM Mafia transcripts to support future research. Our findings underscore both the sophistication and risks of LLM deception in social contexts.",
    "authors": [
      "Christopher Kao",
      "Vanshika Vats",
      "James Davis"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13709v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13709v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.14253v1",
    "title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
    "summary": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.",
    "authors": [
      "Hongyuan Chen",
      "Xingyu Chen",
      "Youjia Zhang",
      "Zexiang Xu",
      "Anpei Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14253v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14253v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.14077v1",
    "title": "MooneyMaker: A Python package to create ambiguous two-tone images",
    "summary": "Mooney images are high-contrast, two-tone visual stimuli, created by thresholding photographic images. They allow researchers to separate image content from image understanding, making them valuable for studying visual perception. An ideal Mooney image for this purpose achieves a specific balance: it initially appears unrecognizable but becomes fully interpretable to the observer after seeing the original template. Researchers traditionally created these stimuli manually using subjective criteria, which is labor-intensive and can introduce inconsistencies across studies. Automated generation techniques now offer an alternative to this manual approach. Here, we present MooneyMaker, an open-source Python package that automates the generation of ambiguous Mooney images using several complementary approaches. Users can choose between various generation techniques that range from approaches based on image statistics to deep learning models. These models strategically alter edge information to increase initial ambiguity. The package lets users create two-tone images with multiple methods and directly compare the results visually. In an experiment, we validate MooneyMaker by generating Mooney images using different techniques and assess their recognizability for human observers before and after disambiguating them by presenting the template images. Our results reveal that techniques with lower initial recognizability are associated with higher post-template recognition (i.e. a larger disambiguation effect). To help vision scientists build effective databases of Mooney stimuli, we provide practical guidelines for technique selection. By standardizing the generation process, MooneyMaker supports more consistent and reproducible visual perception research.",
    "authors": [
      "Lars C. Reining",
      "Thabo Matthies",
      "Luisa Haussner",
      "Rabea Turon",
      "Thomas S. A. Wallis"
    ],
    "categories": [
      "q-bio.NC",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14077v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14077v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.14062v1",
    "title": "Demystifying the trend of the healthcare index: Is historical price a key driver?",
    "summary": "Healthcare sector indices consolidate the economic health of pharmaceutical, biotechnology, and healthcare service firms. The short-term movements in these indices are closely intertwined with capital allocation decisions affecting research and development investment, drug availability, and long-term health outcomes. This research investigates whether historical open-high-low-close (OHLC) index data contain sufficient information for predicting the directional movement of the opening index on the subsequent trading day. The problem is formulated as a supervised classification task involving a one-step-ahead rolling window. A diverse feature set is constructed, comprising original prices, volatility-based technical indicators, and a novel class of nowcasting features derived from mutual OHLC ratios. The framework is evaluated on data from healthcare indices in the U.S. and Indian markets over a five-year period spanning multiple economic phases, including the COVID-19 pandemic. The results demonstrate robust predictive performance, with accuracy exceeding 0.8 and Matthews correlation coefficients above 0.6. Notably, the proposed nowcasting features have emerged as a key determinant of the market movement. We have employed the Shapley-based explainability paradigm to further elucidate the contribution of the features: outcomes reveal the dominant role of the nowcasting features, followed by a more moderate contribution of original prices. This research offers a societal utility: the proposed features and model for short-term forecasting of healthcare indices can reduce information asymmetry and support a more stable and equitable health economy.",
    "authors": [
      "Payel Sadhukhan",
      "Samrat Gupta",
      "Subhasis Ghosh",
      "Tanujit Chakraborty"
    ],
    "categories": [
      "q-fin.ST",
      "stat.AP",
      "stat.ML"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14062v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14062v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.13954v1",
    "title": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging",
    "summary": "Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.",
    "authors": [
      "Adrien Meyer",
      "Didier Mutter",
      "Nicolas Padoy"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13954v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13954v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.13864v1",
    "title": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation",
    "summary": "Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.",
    "authors": [
      "Qirui Chen",
      "Jingxian Shuai",
      "Shuangwu Chen",
      "Shenghao Ye",
      "Zijian Wen",
      "Xufei Su",
      "Jie Jin",
      "Jiangming Li",
      "Jun Chen",
      "Xiaobin Tan",
      "Jian Yang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13864v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13864v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.13835v1",
    "title": "The Role of Prosodic and Lexical Cues in Turn-Taking with Self-Supervised Speech Representations",
    "summary": "Fluid turn-taking remains a key challenge in human-robot interaction. Self-supervised speech representations (S3Rs) have driven many advances, but it remains unclear whether S3R-based turn-taking models rely on prosodic cues, lexical cues or both. We introduce a vocoder-based approach to control prosody and lexical cues in speech more cleanly than prior work. This allows us to probe the voice-activity projection model, an S3R-based turn-taking model. We find that prediction on prosody-matched, unintelligible noise is similar to accuracy on clean speech. This reveals both prosodic and lexical cues support turn-taking, but either can be used in isolation. Hence, future models may only require prosody, providing privacy and potential performance benefits. When either prosodic or lexical information is disrupted, the model exploits the other without further training, indicating they are encoded in S3Rs with limited interdependence. Results are consistent in CPC-based and wav2vec2.0 S3Rs. We discuss our findings and highlight a number of directions for future work. All code is available to support future research.",
    "authors": [
      "Sam OConnor Russell",
      "Delphine Charuau",
      "Naomi Harte"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13835v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13835v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.13706v1",
    "title": "ParkingTwin: Training-Free Streaming 3D Reconstruction for Parking-Lot Digital Twins",
    "summary": "High-fidelity parking-lot digital twins provide essential priors for path planning, collision checking, and perception validation in Automated Valet Parking (AVP). Yet robot-oriented reconstruction faces a trilemma: sparse forward-facing views cause weak parallax and ill-posed geometry; dynamic occlusions and extreme lighting hinder stable texture fusion; and neural rendering typically needs expensive offline optimization, violating edge-side streaming constraints. We propose ParkingTwin, a training-free, lightweight system for online streaming 3D reconstruction. First, OSM-prior-driven geometric construction uses OpenStreetMap semantic topology to directly generate a metric-consistent TSDF, replacing blind geometric search with deterministic mapping and avoiding costly optimization. Second, geometry-aware dynamic filtering employs a quad-modal constraint field (normal/height/depth consistency) to reject moving vehicles and transient occlusions in real time. Third, illumination-robust fusion in CIELAB decouples luminance and chromaticity via adaptive L-channel weighting and depth-gradient suppression, reducing seams under abrupt lighting changes. ParkingTwin runs at 30+ FPS on an entry-level GTX 1660. On a 68,000 m^2 real-world dataset, it achieves SSIM 0.87 (+16.0%), delivers about 15x end-to-end speedup, and reduces GPU memory by 83.3% compared with state-of-the-art 3D Gaussian Splatting (3DGS) that typically requires high-end GPUs (RTX 4090D). The system outputs explicit triangle meshes compatible with Unity/Unreal digital-twin pipelines. Project page: https://mihoutao-liu.github.io/ParkingTwin/",
    "authors": [
      "Xinhao Liu",
      "Yu Wang",
      "Xiansheng Guo",
      "Gordon Owusu Boateng",
      "Yu Cao",
      "Haonan Si",
      "Xingchen Guo",
      "Nirwan Ansari"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13706v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13706v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.13657v1",
    "title": "Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning",
    "summary": "This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.",
    "authors": [
      "Myong-Yol Choi",
      "Hankyoul Ko",
      "Hanse Cho",
      "Changseung Kim",
      "Seunghwan Kim",
      "Jaemin Seo",
      "Hyondong Oh"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13657v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13657v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.14111v1",
    "title": "PMCE: Probabilistic Multi-Granularity Semantics with Caption-Guided Enhancement for Few-Shot Learning",
    "summary": "Few-shot learning aims to identify novel categories from only a handful of labeled samples, where prototypes estimated from scarce data are often biased and generalize poorly. Semantic-based methods alleviate this by introducing coarse class-level information, but they are mostly applied on the support side, leaving query representations unchanged. In this paper, we present PMCE, a Probabilistic few-shot framework that leverages Multi-granularity semantics with Caption-guided Enhancement. PMCE constructs a nonparametric knowledge bank that stores visual statistics for each category as well as CLIP-encoded class name embeddings of the base classes. At meta-test time, the most relevant base classes are retrieved based on the similarities of class name embeddings for each novel category. These statistics are then aggregated into category-specific prior information and fused with the support set prototypes via a simple MAP update. Simultaneously, a frozen BLIP captioner provides label-free instance-level image descriptions, and a lightweight enhancer trained on base classes optimizes both support prototypes and query features under an inductive protocol with a consistency regularization to stabilize noisy captions. Experiments on four benchmarks show that PMCE consistently improves over strong baselines, achieving up to 7.71% absolute gain over the strongest semantic competitor on MiniImageNet in the 1-shot setting. Our code is available at https://anonymous.4open.science/r/PMCE-275D",
    "authors": [
      "Jiaying Wu",
      "Can Gao",
      "Jinglu Hu",
      "Hui Li",
      "Xiaofeng Cao",
      "Jingcai Guo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14111v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14111v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2601.13874v1",
    "title": "Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample Performance with Imbalanced Data and Exact Acceleration under Null and Alternative Hypotheses",
    "summary": "The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic for two-sample testing, whose inferential accuracy depends critically on variance characterization. Existing work provides various finite-sample estimators of the MMD variance, often differing under the null and alternative hypotheses and across balanced or imbalanced sampling schemes. In this paper, we study the variance of the MMD statistic through its U-statistic representation and Hoeffding decomposition, and establish a unified finite-sample characterization covering different hypotheses and sample configurations. Building on this analysis, we propose an exact acceleration method for the univariate case under the Laplacian kernel, which reduces the overall computational complexity from $\\mathcal O(n^2)$ to $\\mathcal O(n \\log n)$.",
    "authors": [
      "Shijie Zhong",
      "Jiangfeng Fu",
      "Yikun Yang"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13874v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13874v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2601.13649v1",
    "title": "Fairness or Fluency? An Investigation into Language Bias of Pairwise LLM-as-a-Judge",
    "summary": "Recent advances in Large Language Models (LLMs) have incentivized the development of LLM-as-a-judge, an application of LLMs where they are used as judges to decide the quality of a certain piece of text given a certain context. However, previous studies have demonstrated that LLM-as-a-judge can be biased towards different aspects of the judged texts, which often do not align with human preference. One of the identified biases is language bias, which indicates that the decision of LLM-as-a-judge can differ based on the language of the judged texts. In this paper, we study two types of language bias in pairwise LLM-as-a-judge: (1) performance disparity between languages when the judge is prompted to compare options from the same language, and (2) bias towards options written in major languages when the judge is prompted to compare options of two different languages. We find that for same-language judging, there exist significant performance disparities across language families, with European languages consistently outperforming African languages, and this bias is more pronounced in culturally-related subjects. For inter-language judging, we observe that most models favor English answers, and that this preference is influenced more by answer language than question language. Finally, we investigate whether language bias is in fact caused by low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we find that while perplexity is slightly correlated with language bias, language bias cannot be fully explained by perplexity only.",
    "authors": [
      "Xiaolin Zhou",
      "Zheng Luo",
      "Yicheng Gao",
      "Qixuan Chen",
      "Xiyang Hu",
      "Yue Zhao",
      "Ruishan Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13649v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13649v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2601.14161v1",
    "title": "One-Shot Refiner: Boosting Feed-forward Novel View Synthesis via One-Step Diffusion",
    "summary": "We present a novel framework for high-fidelity novel view synthesis (NVS) from sparse images, addressing key limitations in recent feed-forward 3D Gaussian Splatting (3DGS) methods built on Vision Transformer (ViT) backbones. While ViT-based pipelines offer strong geometric priors, they are often constrained by low-resolution inputs due to computational costs. Moreover, existing generative enhancement methods tend to be 3D-agnostic, resulting in inconsistent structures across views, especially in unseen regions. To overcome these challenges, we design a Dual-Domain Detail Perception Module, which enables handling high-resolution images without being limited by the ViT backbone, and endows Gaussians with additional features to store high-frequency details. We develop a feature-guided diffusion network, which can preserve high-frequency details during the restoration process. We introduce a unified training strategy that enables joint optimization of the ViT-based geometric backbone and the diffusion-based refinement module. Experiments demonstrate that our method can maintain superior generation quality across multiple datasets.",
    "authors": [
      "Yitong Dong",
      "Qi Zhang",
      "Minchao Jiang",
      "Zhiqiang Wu",
      "Qingnan Fan",
      "Ying Feng",
      "Huaqi Zhang",
      "Hujun Bao",
      "Guofeng Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14161v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14161v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2601.13871v1",
    "title": "OCCAM: Class-Agnostic, Training-Free, Prior-Free and Multi-Class Object Counting",
    "summary": "Class-Agnostic object Counting (CAC) involves counting instances of objects from arbitrary classes within an image. Due to its practical importance, CAC has received increasing attention in recent years. Most existing methods assume a single object class per image, rely on extensive training of large deep learning models and address the problem by incorporating additional information, such as visual exemplars or text prompts. In this paper, we present OCCAM, the first training-free approach to CAC that operates without the need of any supplementary information. Moreover, our approach addresses the multi-class variant of the problem, as it is capable of counting the object instances in each and every class among arbitrary object classes within an image. We leverage Segment Anything Model 2 (SAM2), a foundation model, and a custom threshold-based variant of the First Integer Neighbor Clustering Hierarchy (FINCH) algorithm to achieve competitive performance on widely used benchmark datasets, FSC-147 and CARPK. We propose a synthetic multi-class dataset and F1 score as a more suitable evaluation metric. The code for our method and the proposed synthetic dataset will be made publicly available at https://mikespanak.github.io/OCCAM_counter.",
    "authors": [
      "Michail Spanakis",
      "Iason Oikonomidis",
      "Antonis Argyros"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13871v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13871v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2601.14232v1",
    "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
    "summary": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.",
    "authors": [
      "Egor Cherepanov",
      "Daniil Zelezetsky",
      "Alexey K. Kovalev",
      "Aleksandr I. Panov"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14232v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14232v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2601.14103v1",
    "title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing",
    "summary": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.",
    "authors": [
      "Xiaolu Liu",
      "Yicong Li",
      "Qiyuan He",
      "Jiayin Zhu",
      "Wei Ji",
      "Angela Yao",
      "Jianke Zhu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14103v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14103v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2601.13839v1",
    "title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes",
    "summary": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.",
    "authors": [
      "Aisha Al-Mohannadi",
      "Ayisha Firoz",
      "Yin Yang",
      "Muhammad Imran",
      "Ferda Ofli"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13839v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13839v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2601.14188v1",
    "title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models",
    "summary": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.",
    "authors": [
      "Liang Shi",
      "Wei Li",
      "Kevin M Beussman",
      "Lin Chen",
      "Yun Fu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14188v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14188v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2601.13974v1",
    "title": "STEC: A Reference-Free Spatio-Temporal Entropy Coverage Metric for Evaluating Sampled Video Frames",
    "summary": "Frame sampling is a fundamental component in video understanding and video--language model pipelines, yet evaluating the quality of sampled frames remains challenging. Existing evaluation metrics primarily focus on perceptual quality or reconstruction fidelity, and are not designed to assess whether a set of sampled frames adequately captures informative and representative video content.   We propose Spatio-Temporal Entropy Coverage (STEC), a simple and non-reference metric for evaluating the effectiveness of video frame sampling. STEC builds upon Spatio-Temporal Frame Entropy (STFE), which measures per-frame spatial information via entropy-based structural complexity, and evaluates sampled frames based on their temporal coverage and redundancy. By jointly modeling spatial information strength, temporal dispersion, and non-redundancy, STEC provides a principled and lightweight measure of sampling quality.   Experiments on the MSR-VTT test-1k benchmark demonstrate that STEC clearly differentiates common sampling strategies, including random, uniform, and content-aware methods. We further show that STEC reveals robustness patterns across individual videos that are not captured by average performance alone, highlighting its practical value as a general-purpose evaluation tool for efficient video understanding.   We emphasize that STEC is not designed to predict downstream task accuracy, but to provide a task-agnostic diagnostic signal for analyzing frame sampling behavior under constrained budgets.",
    "authors": [
      "Shih-Yao Lin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13974v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13974v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2601.13705v1",
    "title": "Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles",
    "summary": "Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.",
    "authors": [
      "Maria Lymperaiou",
      "Vasileios Karampinis",
      "Giorgos Filandrianos",
      "Angelos Vlachos",
      "Chrysoula Zerva",
      "Athanasios Voulodimos"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13705v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13705v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2601.13664v1",
    "title": "VIAFormer: Voxel-Image Alignment Transformer for High-Fidelity Voxel Refinement",
    "summary": "We propose VIAFormer, a Voxel-Image Alignment Transformer model designed for Multi-view Conditioned Voxel Refinement--the task of repairing incomplete noisy voxels using calibrated multi-view images as guidance. Its effectiveness stems from a synergistic design: an Image Index that provides explicit 3D spatial grounding for 2D image tokens, a Correctional Flow objective that learns a direct voxel-refinement trajectory, and a Hybrid Stream Transformer that enables robust cross-modal fusion. Experiments show that VIAFormer establishes a new state of the art in correcting both severe synthetic corruptions and realistic artifacts on the voxel shape obtained from powerful Vision Foundation Models. Beyond benchmarking, we demonstrate VIAFormer as a practical and reliable bridge in real-world 3D creation pipelines, paving the way for voxel-based methods to thrive in large-model, big-data wave.",
    "authors": [
      "Tiancheng Fang",
      "Bowen Pan",
      "Lingxi Chen",
      "Jiangjing Lyu",
      "Chengfei Lyu",
      "Chaoyue Niu",
      "Fan Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13664v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13664v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2601.14030v1",
    "title": "Likelihood-Separable Diffusion Inference for Multi-Image MRI Super-Resolution",
    "summary": "Diffusion models are the current state-of-the-art for solving inverse problems in imaging. Their impressive generative capability allows them to approximate sampling from a prior distribution, which alongside a known likelihood function permits posterior sampling without retraining the model. While recent methods have made strides in advancing the accuracy of posterior sampling, the majority focuses on single-image inverse problems. However, for modalities such as magnetic resonance imaging (MRI), it is common to acquire multiple complementary measurements, each low-resolution along a different axis. In this work, we generalize common diffusion-based inverse single-image problem solvers for multi-image super-resolution (MISR) MRI. We show that the DPS likelihood correction allows an exactly-separable gradient decomposition across independently acquired measurements, enabling MISR without constructing a joint operator, modifying the diffusion model, or increasing network function evaluations. We derive MISR versions of DPS, DMAP, DPPS, and diffusion-based PnP/ADMM, and demonstrate substantial gains over SISR across $4\\times/8\\times/16\\times$ anisotropic degradations. Our results achieve state-of-the-art super-resolution of anisotropic MRI volumes and, critically, enable reconstruction of near-isotropic anatomy from routine 2D multi-slice acquisitions, which are otherwise highly degraded in orthogonal views.",
    "authors": [
      "Samuel W. Remedios",
      "Zhangxing Bian",
      "Shuwen Wei",
      "Aaron Carass",
      "Jerry L. Prince",
      "Blake E. Dewey"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14030v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14030v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2601.14130v1",
    "title": "GIC-DLC: Differentiable Logic Circuits for Hardware-Friendly Grayscale Image Compression",
    "summary": "Neural image codecs achieve higher compression ratios than traditional hand-crafted methods such as PNG or JPEG-XL, but often incur substantial computational overhead, limiting their deployment on energy-constrained devices such as smartphones, cameras, and drones. We propose Grayscale Image Compression with Differentiable Logic Circuits (GIC-DLC), a hardware-aware codec where we train lookup tables to combine the flexibility of neural networks with the efficiency of Boolean operations. Experiments on grayscale benchmark datasets show that GIC-DLC outperforms traditional codecs in compression efficiency while allowing substantial reductions in energy consumption and latency. These results demonstrate that learned compression can be hardware-friendly, offering a promising direction for low-power image compression on edge devices.",
    "authors": [
      "Till Aczel",
      "David F. Jenny",
      "Simon Bührer",
      "Andreas Plesner",
      "Antonio Di Maio",
      "Roger Wattenhofer"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14130v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14130v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2601.14104v1",
    "title": "Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning",
    "summary": "Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.",
    "authors": [
      "Tairan Huang",
      "Qingqing Ye",
      "Yulin Jin",
      "Jiawei Lian",
      "Yi Wang",
      "Haibo Hu"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14104v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14104v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2601.13951v1",
    "title": "VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content",
    "summary": "With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.",
    "authors": [
      "Shengyi Wu",
      "Yan Hong",
      "Shengyao Chen",
      "Zheng Wang",
      "Xianbing Sun",
      "Jiahui Zhan",
      "Jun Lan",
      "Jianfu Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13951v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13951v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2601.13665v1",
    "title": "Transformer based Multi-task Fusion Network for Food Spoilage Detection and Shelf life Forecasting",
    "summary": "Food wastage is one of the critical challenges in the agricultural supply chain, and accurate and effective spoilage detection can help to reduce it. Further, it is highly important to forecast the spoilage information. This aids the longevity of the supply chain management in the agriculture field. This motivated us to propose fusion based architectures by combining CNN with LSTM and DeiT transformer for the following multi-tasks simultaneously: (i) vegetable classification, (ii) food spoilage detection, and (iii) shelf life forecasting. We developed a dataset by capturing images of vegetables from their fresh state until they were completely spoiled. From the experimental analysis it is concluded that the proposed fusion architectures CNN+CNN-LSTM and CNN+DeiT Transformer outperformed several deep learning models such as CNN, VGG16, ResNet50, Capsule Networks, and DeiT Transformers. Overall, CNN + DeiT Transformer yielded F1-score of 0.98 and 0.61 in vegetable classification and spoilage detection respectively and mean squared error (MSE) and symmetric mean absolute percentage error (SMAPE) of 3.58, and 41.66% respectively in spoilage forecasting. Further, the reliability of the fusion models was validated on noisy images and integrated with LIME to visualize the model decisions.",
    "authors": [
      "Mounika Kanulla",
      "Rajasree Dadigi",
      "Sailaja Thota",
      "Vivek Yelleti"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13665v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13665v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2601.13899v1",
    "title": "Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging",
    "summary": "Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.",
    "authors": [
      "Masoumeh Javanbakhat",
      "Piotr Komorowski",
      "Dilyara Bareeva",
      "Wei-Chang Lai",
      "Wojciech Samek",
      "Christoph Lippert"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.13899v1",
    "pdf_url": "https://arxiv.org/pdf/2601.13899v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2601.14246v1",
    "title": "Soft Tail-dropping for Adaptive Visual Tokenization",
    "summary": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.",
    "authors": [
      "Zeyuan Chen",
      "Kai Zhang",
      "Zhuowen Tu",
      "Yuanjun Xiong"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-20",
    "url": "https://arxiv.org/abs/2601.14246v1",
    "pdf_url": "https://arxiv.org/pdf/2601.14246v1.pdf",
    "date": "2026-01-21",
    "source": "arxiv",
    "research_score": 0.42
  }
]