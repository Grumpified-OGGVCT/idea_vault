[
  {
    "arxiv_id": "2602.05748v1",
    "title": "LeakBoost: Perceptual-Loss-Based Membership Inference Attack",
    "summary": "Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.",
    "authors": [
      "Amit Kravchik Taub",
      "Fred M. Grabovski",
      "Guy Amit",
      "Yisroel Mirsky"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05748v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05748v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2602.05794v1",
    "title": "FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem",
    "summary": "We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.",
    "authors": [
      "Aboli Kathar",
      "Aman Kumar",
      "Anusha Kamath",
      "Araveeti Srujan",
      "Ashish Sharma",
      "Chandra Bhushan",
      "Dilip Asbe",
      "Divya Sorate",
      "Duddu Prasanth Kumar",
      "Evan Acharya",
      "Harsh Sharma",
      "Hrithik Kadam",
      "Kanishk Singla",
      "Keyur Doshi",
      "Kiran Praveen",
      "Kolisetty Krishna SK",
      "Krishanu Adhikary",
      "Lokesh MPT",
      "Mayurdeep Sonowal",
      "Nadeem Shaikh",
      "Navya Prakash",
      "Nimit Kothari",
      "Nitin Kukreja",
      "Prashant Devadiga",
      "Rakesh Paul",
      "Ratanjeet Pratap Chauhan",
      "Raunak Kalani",
      "Raviraj Joshi",
      "Shamanth MH",
      "Shantanu Pandey",
      "Shubham Soni",
      "Siddharth Dixit",
      "Smriti Jopat",
      "Sunil Patel",
      "Suraj Singh",
      "Suvradip Paul",
      "Tulasi Pilla",
      "Utkarsh Vaidya",
      "Vineeth Nambiar",
      "Vishal Kanvaty",
      "Yatharth Dedhia"
    ],
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05794v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05794v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.88
  },
  {
    "arxiv_id": "2602.05471v1",
    "title": "Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision",
    "summary": "Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.",
    "authors": [
      "Md. Mithun Hossaina",
      "Mashary N. Alrasheedy",
      "Nirban Bhowmick",
      "Shamim Forhad",
      "Md. Shakil Hossain",
      "Sudipto Chaki",
      "Md Shafiqul Islam"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05471v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05471v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.84
  },
  {
    "arxiv_id": "2602.05617v1",
    "title": "Unified Sensor Simulation for Autonomous Driving",
    "summary": "In this work, we introduce \\textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \\href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.",
    "authors": [
      "Nikolay Patakin",
      "Arsenii Shirokov",
      "Anton Konushin",
      "Dmitry Senushkin"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05617v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05617v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.82
  },
  {
    "arxiv_id": "2602.05550v1",
    "title": "ArkTS-CodeSearch: A Open-Source ArkTS Dataset for Code Retrieval",
    "summary": "ArkTS is a core programming language in the OpenHarmony ecosystem, yet research on ArkTS code intelligence is hindered by the lack of public datasets and evaluation benchmarks. This paper presents a large-scale ArkTS dataset constructed from open-source repositories, targeting code retrieval and code evaluation tasks. We design a single-search task, where natural language comments are used to retrieve corresponding ArkTS functions. ArkTS repositories are crawled from GitHub and Gitee, and comment-function pairs are extracted using tree-sitter-arkts, followed by cross-platform deduplication and statistical analysis of ArkTS function types. We further evaluate all existing open-source code embedding models on the single-search task and perform fine-tuning using both ArkTS and TypeScript training datasets, resulting in a high-performing model for ArkTS code understanding. This work establishes the first systematic benchmark for ArkTS code retrieval. Both the dataset and our fine-tuned model will be released publicly and are available at https://huggingface.co/hreyulog/embedinggemma_arkts and https://huggingface.co/datasets/hreyulog/arkts-code-docstring,establishing the first systematic benchmark for ArkTS code retrieval.",
    "authors": [
      "Yulong He",
      "Artem Ermakov",
      "Sergey Kovalchuk",
      "Artem Aliev",
      "Dmitry Shalymov"
    ],
    "categories": [
      "cs.SE",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05550v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05550v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.82
  },
  {
    "arxiv_id": "2602.05571v1",
    "title": "EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking",
    "summary": "Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\\%, a +3.8 pp improvement over the prior state of the art (74.2\\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/",
    "authors": [
      "Rishabh Bhattacharya",
      "Naresh Manwani"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05571v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05571v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2602.05573v1",
    "title": "Visual Implicit Geometry Transformer for Autonomous Driving",
    "summary": "We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \\href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.",
    "authors": [
      "Arsenii Shirokov",
      "Mikhail Kuznetsov",
      "Danila Stepochkin",
      "Egor Evdokimov",
      "Daniil Glazkov",
      "Nikolay Patakin",
      "Anton Konushin",
      "Dmitry Senushkin"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05573v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05573v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2602.05416v1",
    "title": "Reduced-Order Surrogates for Forced Flexible Mesh Coastal-Ocean Models",
    "summary": "While POD-based surrogates are widely explored for hydrodynamic applications, the use of Koopman Autoencoders for real-world coastal-ocean modelling remains relatively limited. This paper introduces a flexible Koopman autoencoder formulation that incorporates meteorological forcings and boundary conditions, and systematically compares its performance against POD-based surrogates. The Koopman autoencoder employs a learned linear temporal operator in latent space, enabling eigenvalue regularization to promote temporal stability. This strategy is evaluated alongside temporal unrolling techniques for achieving stable and accurate long-term predictions. The models are assessed on three test cases spanning distinct dynamical regimes, with prediction horizons up to one year at 30-minute temporal resolution. Across all cases, the Koopman autoencoder with temporal unrolling yields the best overall accuracy compared to the POD-based surrogates, achieving relative root-mean-squared-errors of 0.01-0.13 and $R^2$-values of 0.65-0.996. Prediction errors are largest for current velocities, and smallest for water surface elevations. Comparing to in-situ observations, the surrogate yields -0.65% to 12% change in water surface elevation prediction error when compared to prediction errors of the physics-based model. These error levels, corresponding to a few centimeters, are acceptable for many practical applications, while inference speed-ups of 300-1400x enables workflows such as ensemble forecasting and long climate simulations for coastal-ocean modelling.",
    "authors": [
      "Freja Høgholm Petersen",
      "Jesper Sandvig Mariegaard",
      "Rocco Palmitessa",
      "Allan P. Engsig-Karup"
    ],
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph",
      "physics.flu-dyn"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05416v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05416v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2602.05830v1",
    "title": "Learning Compact Boolean Networks",
    "summary": "Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.",
    "authors": [
      "Shengpu Wang",
      "Yuhao Mao",
      "Yani Zhang",
      "Martin Vechev"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05830v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05830v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2602.05785v1",
    "title": "ReText: Text Boosts Generalization in Image-Based Person Re-identification",
    "summary": "Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.",
    "authors": [
      "Timur Mamedov",
      "Karina Kvanchiani",
      "Anton Konushin",
      "Vadim Konushin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05785v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05785v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2602.05630v1",
    "title": "Rewards as Labels: Revisiting RLVR from a Classification Perspective",
    "summary": "Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.",
    "authors": [
      "Zepeng Zhai",
      "Meilin Chen",
      "Jiaxuan Zhao",
      "Junlang Qian",
      "Lei Shen",
      "Yuan Lu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05630v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05630v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2602.05667v1",
    "title": "Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection",
    "summary": "Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS",
    "authors": [
      "Ling Zhan",
      "Zhen Li",
      "Junjie Huang",
      "Tao Jia"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05667v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05667v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2602.05625v1",
    "title": "Reactive Knowledge Representation and Asynchronous Reasoning",
    "summary": "Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Julian Eggert",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05625v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05625v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2602.05818v1",
    "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
    "summary": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.",
    "authors": [
      "Zihao Jiang",
      "Miao Peng",
      "Zhenyan Shan",
      "Wenjie Xu",
      "Ben Liu",
      "Gong Chen",
      "Ziqi Gao",
      "Min Peng"
    ],
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05818v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05818v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2602.05454v1",
    "title": "Attention Retention for Continual Learning with Vision Transformers",
    "summary": "Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.",
    "authors": [
      "Yue Lu",
      "Xiangyu Zhou",
      "Shizhou Zhang",
      "Yinghui Xing",
      "Guoqiang Liang",
      "Wencong Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05454v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05454v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2602.05853v1",
    "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
    "summary": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$τ$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.",
    "authors": [
      "Siran Liu",
      "Guoxia Wang",
      "Sa Wang",
      "Jinle Zeng",
      "HaoYang Xie",
      "Siyu Lou",
      "JiaBin Yang",
      "DianHai Yu",
      "Haifeng Wang",
      "Chao Yang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05853v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05853v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2602.05646v1",
    "title": "Empowering Time Series Analysis with Large-Scale Multimodal Pretraining",
    "summary": "While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.",
    "authors": [
      "Peng Chen",
      "Siyuan Wang",
      "Shiyan Hu",
      "Xingjian Wu",
      "Yang Shu",
      "Zhongwen Rao",
      "Meng Wang",
      "Yijie Li",
      "Bin Yang",
      "Chenjuan Guo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05646v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05646v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2602.05535v1",
    "title": "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification",
    "summary": "Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.",
    "authors": [
      "Tao Huang",
      "Rui Wang",
      "Xiaofei Liu",
      "Yi Qin",
      "Li Duan",
      "Liping Jing"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05535v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05535v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2602.05464v1",
    "title": "Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning",
    "summary": "Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.",
    "authors": [
      "Jiaquan Wang",
      "Yan Lyu",
      "Chen Li",
      "Yuheng Jia"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05464v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05464v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2602.05688v1",
    "title": "Mining Generalizable Activation Functions",
    "summary": "The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.",
    "authors": [
      "Alex Vitvitskyi",
      "Michael Boratko",
      "Matej Grcic",
      "Razvan Pascanu",
      "Deep Shah",
      "Petar Veličković"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05688v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05688v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2602.05616v1",
    "title": "Path-Guided Flow Matching for Dataset Distillation",
    "summary": "Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \\emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\\times$ more efficient than the diffusion-based counterparts with 78\\% mode coverage.",
    "authors": [
      "Xuhui Li",
      "Zhengquan Luo",
      "Xiwei Liu",
      "Yongqiang Yu",
      "Zhiqiang Xu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05616v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05616v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2602.05544v1",
    "title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation",
    "summary": "Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\\% in Sports and 4.59\\% in Toys, along with ROUGE-L by 8.02\\% and 3.49\\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\\% in cold-start and 11.9\\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\\% in Beauty and 23.16\\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.",
    "authors": [
      "Fahad Anwaar",
      "Adil Mehmood Khan",
      "Muhammad Khalid",
      "Usman Zia",
      "Kezhi Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05544v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05544v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2602.05429v1",
    "title": "M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining",
    "summary": "Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.",
    "authors": [
      "Rui Lv",
      "Juncheng Mo",
      "Tianyi Chu",
      "Chen Rao",
      "Hongyi Jing",
      "Jiajie Teng",
      "Jiafu Chen",
      "Shiqi Zhang",
      "Liangzi Ding",
      "Shuo Fang",
      "Huaizhong Lin",
      "Ziqiang Dang",
      "Chenguang Ma",
      "Lei Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05429v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05429v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2602.05767v1",
    "title": "PMT Waveform Simulation and Reconstruction with Conditional Diffusion Network",
    "summary": "Photomultiplier tubes (PMTs) are widely employed in particle and nuclear physics experiments. The accuracy of PMT waveform reconstruction directly impacts the detector's spatial and energy resolution. A key challenge arises when multiple photons arrive within a few nanoseconds, making it difficult to resolve individual photoelectrons (PEs). Although supervised deep learning methods have surpassed traditional methods in performance, their practical applicability is limited by the lack of ground-truth PE labels in real data. To address this issue, we propose an innovative weakly supervised waveform simulation and reconstruction approach based on a bidirectional conditional diffusion network framework. The method is fully data-driven and requires only raw waveforms and coarse estimates of PE information as input. It first employs a PE-conditioned diffusion model to simulate realistic waveforms from PE sequences, thereby learning the features of overlapping waveforms. Subsequently, these simulated waveforms are used to train a waveform-conditioned diffusion model to reconstruct the PE sequences from waveforms, reinforcing the learning of features of overlapping waveforms. Through iterative refinement between the two conditional diffusion processes, the model progressively improves reconstruction accuracy. Experimental results demonstrate that the proposed method achieves 99% of the normalized PE-number resolution averaged over 1-5 p.e. and 80% of the timing resolution attained by fully supervised learning.",
    "authors": [
      "Kainan Liu",
      "Jingyu Huang",
      "Guihong Huang",
      "Jianyi Luo"
    ],
    "categories": [
      "hep-ex",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05767v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05767v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2602.05694v1",
    "title": "Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation",
    "summary": "Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.",
    "authors": [
      "Shuting Jiang",
      "Ran Song",
      "Yuxin Huang",
      "Yan Xiang",
      "Yantuan Xian",
      "Shengxiang Gao",
      "Zhengtao Yu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05694v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05694v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2602.05508v1",
    "title": "VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency",
    "summary": "Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.",
    "authors": [
      "Zhuang Xiong",
      "Chen Zhang",
      "Qingshan Xu",
      "Wenbing Tao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05508v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05508v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2602.05415v1",
    "title": "VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection",
    "summary": "Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.",
    "authors": [
      "Ningkang Peng",
      "Qianfeng Yu",
      "Yuhao Zhang",
      "Yafei Liu",
      "Xiaoqian Peng",
      "Peirong Ma",
      "Yi Chen",
      "Peiheng Li",
      "Yanhui Gu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05415v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05415v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2602.05729v1",
    "title": "Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification",
    "summary": "Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.",
    "authors": [
      "Lexiang Hu",
      "Youze Xue",
      "Dian Li",
      "Gang Liu",
      "Zhouchen Lin"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05729v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05729v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.05670v1",
    "title": "HyperPotter: Spell the Charm of High-Order Interactions in Audio Deepfake Detection",
    "summary": "Advances in AIGC technologies have enabled the synthesis of highly realistic audio deepfakes capable of deceiving human auditory perception. Although numerous audio deepfake detection (ADD) methods have been developed, most rely on local temporal/spectral features or pairwise relations, overlooking high-order interactions (HOIs). HOIs capture discriminative patterns that emerge from multiple feature components beyond their individual contributions. We propose HyperPotter, a hypergraph-based framework that explicitly models these synergistic HOIs through clustering-based hyperedges with class-aware prototype initialization. Extensive experiments demonstrate that HyperPotter surpasses its baseline by an average relative gain of 22.15% across 11 datasets and outperforms state-of-the-art methods by 13.96% on 4 challenging cross-domain datasets, demonstrating superior generalization to diverse attacks and speakers.",
    "authors": [
      "Qing Wen",
      "Haohao Li",
      "Zhongjie Ba",
      "Peng Cheng",
      "Miao He",
      "Li Lu",
      "Kui Ren"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05670v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05670v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.05437v1",
    "title": "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models",
    "summary": "Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.",
    "authors": [
      "Basel Mousi",
      "Fahim Dalvi",
      "Shammur Chowdhury",
      "Firoj Alam",
      "Nadir Durrani"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05437v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05437v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2602.05847v1",
    "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
    "summary": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
    "authors": [
      "Zhangquan Chen",
      "Jiale Tao",
      "Ruihuang Li",
      "Yihao Hu",
      "Ruitao Chen",
      "Zhantao Yang",
      "Xinlei Yu",
      "Haodong Jing",
      "Manyuan Zhang",
      "Shuai Shao",
      "Biao Wang",
      "Qinglin Lu",
      "Ruqi Huang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05847v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05847v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05765v1",
    "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
    "summary": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.",
    "authors": [
      "Zhong Guan",
      "Haoran Sun",
      "Yongjian Guo",
      "Shuai Di",
      "Xiaodong Bai",
      "Jing Long",
      "Tianyun Zhao",
      "Mingxi Luo",
      "Chen Zhou",
      "Yucheng Guo",
      "Qiming Yang",
      "Wanting Xu",
      "Wen Huang",
      "Yunxuan Ma",
      "Hongke Zhao",
      "Likang Wu",
      "Xiaotie Deng",
      "Xi Xiao",
      "Sheng Wen",
      "Yicheng Gong",
      "Junwu Xiong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05765v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05765v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05755v1",
    "title": "FMPose3D: monocular 3D pose estimation via flow matching",
    "summary": "Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.",
    "authors": [
      "Ti Wang",
      "Xiaohang Yu",
      "Mackenzie Weygandt Mathis"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05755v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05755v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05711v1",
    "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
    "summary": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
    "authors": [
      "Jingze Shi",
      "Zhangyang Peng",
      "Yizhang Zhu",
      "Yifan Wu",
      "Guang Liu",
      "Yuyu Luo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05711v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05711v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05547v1",
    "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks",
    "summary": "RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.",
    "authors": [
      "Shyam Sundhar Ramesh",
      "Xiaotong Ji",
      "Matthieu Zimmer",
      "Sangwoong Yoon",
      "Zhiyong Wang",
      "Haitham Bou Ammar",
      "Aurelien Lucchi",
      "Ilija Bogunovic"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05547v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05547v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05536v1",
    "title": "When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging",
    "summary": "Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.",
    "authors": [
      "Yayuan Li",
      "Ze Peng",
      "Jian Zhang",
      "Jintao Guo",
      "Yue Duan",
      "Yinghuan Shi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05536v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05536v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05420v1",
    "title": "Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring",
    "summary": "Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the \"divide and conquer\" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, \"Explicit Marking\" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a \"conflict set.\" Second, \"Implicit Disambiguation\" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.",
    "authors": [
      "Rui Sun",
      "Yiwen Yang",
      "Kaiyu Guo",
      "Chen Jiang",
      "Dongli Xu",
      "Zhaonan Liu",
      "Tan Pan",
      "Limei Han",
      "Xue Jiang",
      "Wu Wei",
      "Yuan Cheng"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05420v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05420v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05410v1",
    "title": "Robust Federated Learning via Byzantine Filtering over Encrypted Updates",
    "summary": "Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.",
    "authors": [
      "Adda Akram Bendoukha",
      "Aymen Boudguiga",
      "Nesrine Kaaniche",
      "Renaud Sirdey",
      "Didem Demirag",
      "Sébastien Gambs"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05410v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05410v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2602.05472v1",
    "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
    "summary": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
    "authors": [
      "Yiwen Duan",
      "Jing Ye",
      "Xinpei Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05472v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05472v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2602.05448v1",
    "title": "BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs",
    "summary": "Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\\times$ fewer than pairwise methods at near-identical quality.",
    "authors": [
      "Sheshansh Agrawal",
      "Thien Hang Nguyen",
      "Douwe Kiela"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05448v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05448v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2602.05424v1",
    "title": "THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs",
    "summary": "Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.",
    "authors": [
      "Weijian Yu",
      "Yuhuan Lu",
      "Dingqi Yang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05424v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05424v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2602.05849v1",
    "title": "Visualizing the loss landscapes of physics-informed neural networks",
    "summary": "Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a \"loss landscape\" community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.",
    "authors": [
      "Conor Rowan",
      "Finn Murphy-Blanchard"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05849v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05849v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.05599v1",
    "title": "BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages",
    "summary": "Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.",
    "authors": [
      "Subhadip Maji",
      "Arnab Bhattacharya"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05599v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05599v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.05549v1",
    "title": "Logical Guidance for the Exact Composition of Diffusion Models",
    "summary": "We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.   We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.   First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.   Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.   In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.   Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.   Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.   We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.",
    "authors": [
      "Francesco Alesiani",
      "Jonathan Warrell",
      "Tanja Bien",
      "Henrik Christiansen",
      "Matheus Ferraz",
      "Mathias Niepert"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05549v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05549v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.05524v1",
    "title": "AI Agent Systems for Supply Chains: Structured Decision Prompts and Memory Retrieval",
    "summary": "This study investigates large language model (LLM) -based multi-agent systems (MASs) as a promising approach to inventory management, which is a key component of supply chain management. Although these systems have gained considerable attention for their potential to address the challenges associated with typical inventory management methods, key uncertainties regarding their effectiveness persist. Specifically, it is unclear whether LLM-based MASs can consistently derive optimal ordering policies and adapt to diverse supply chain scenarios. To address these questions, we examine an LLM-based MAS with a fixed-ordering strategy prompt that encodes the stepwise processes of the problem setting and a safe-stock strategy commonly used in inventory management. Our empirical results demonstrate that, even without detailed prompt adjustments, an LLM-based MAS can determine optimal ordering decisions in a restricted scenario. To enhance adaptability, we propose a novel agent called AIM-RM, which leverages similar historical experiences through similarity matching. Our results show that AIM-RM outperforms benchmark methods across various supply chain scenarios, highlighting its robustness and adaptability.",
    "authors": [
      "Konosuke Yoshizato",
      "Kazuma Shimizu",
      "Ryota Higa",
      "Takanobu Otsuka"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05524v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05524v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.05474v1",
    "title": "LMMRec: LLM-driven Motivation-aware Multimodal Recommendation",
    "summary": "Motivation-based recommendation systems uncover user behavior drivers. Motivation modeling, crucial for decision-making and content preference, explains recommendation generation. Existing methods often treat motivation as latent variables from interaction data, neglecting heterogeneous information like review text. In multimodal motivation fusion, two challenges arise: 1) achieving stable cross-modal alignment amid noise, and 2) identifying features reflecting the same underlying motivation across modalities. To address these, we propose LLM-driven Motivation-aware Multimodal Recommendation (LMMRec), a model-agnostic framework leveraging large language models for deep semantic priors and motivation understanding. LMMRec uses chain-of-thought prompting to extract fine-grained user and item motivations from text. A dual-encoder architecture models textual and interaction-based motivations for cross-modal alignment, while Motivation Coordination Strategy and Interaction-Text Correspondence Method mitigate noise and semantic drift through contrastive learning and momentum updates. Experiments on three datasets show LMMRec achieves up to a 4.98\\% performance improvement.",
    "authors": [
      "Yicheng Di",
      "Zhanjie Zhang",
      "Yun Wangc",
      "Jinren Liue",
      "Jiaqi Yanf",
      "Jiyu Wei",
      "Xiangyu Chend",
      "Yuan Liu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05474v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05474v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.05449v1",
    "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
    "summary": "While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.",
    "authors": [
      "Chang Zou",
      "Changlin Li",
      "Yang Li",
      "Patrol Li",
      "Jianbing Wu",
      "Xiao He",
      "Songtao Liu",
      "Zhao Zhong",
      "Kailin Huang",
      "Linfeng Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05449v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05449v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2602.05869v1",
    "title": "Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity",
    "summary": "We introduce Wedge Sampling, a new non-adaptive sampling scheme for low-rank tensor completion. We study recovery of an order-$k$ low-rank tensor of dimension $n \\times \\cdots \\times n$ from a subset of its entries. Unlike the standard uniform entry model (i.e., i.i.d. samples from $[n]^k$), wedge sampling allocates observations to structured length-two patterns (wedges) in an associated bipartite sampling graph. By directly promoting these length-two connections, the sampling design strengthens the spectral signal that underlies efficient initialization, in regimes where uniform sampling is too sparse to generate enough informative correlations.   Our main result shows that this change in sampling paradigm enables polynomial-time algorithms to achieve both weak and exact recovery with nearly linear sample complexity in $n$. The approach is also plug-and-play: wedge-sampling-based spectral initialization can be combined with existing refinement procedures (e.g., spectral or gradient-based methods) using only an additional $\\tilde{O}(n)$ uniformly sampled entries, substantially improving over the $\\tilde{O}(n^{k/2})$ sample complexity typically required under uniform entry sampling for efficient methods. Overall, our results suggest that the statistical-to-computational gap highlighted in Barak and Moitra (2022) is, to a large extent, a consequence of the uniform entry sampling model for tensor completion, and that alternative non-adaptive measurement designs that guarantee a strong initialization can overcome this barrier.",
    "authors": [
      "Hengrui Luo",
      "Anna Ma",
      "Ludovic Stephan",
      "Yizhe Zhu"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.NA",
      "math.PR",
      "math.ST"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05869v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05869v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05817v1",
    "title": "Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows",
    "summary": "The rapid expansion of Internet of Things (IoT) ecosystems has led to increasingly complex and heterogeneous network topologies. Traditional network monitoring and visualization tools rely on aggregated metrics or static representations, which fail to capture the evolving relationships and structural dependencies between devices. Although Graph Neural Networks (GNNs) offer a powerful way to learn from relational data, their internal representations often remain opaque and difficult to interpret for security-critical operations. Consequently, this work introduces an interpretable pipeline that generates directly visualizable low-dimensional representations by mapping high-dimensional embeddings onto a latent manifold. This projection enables the interpretable monitoring and interoperability of evolving network states, while integrated feature attribution techniques decode the specific characteristics shaping the manifold structure. The framework achieves a classification F1-score of 0.830 for intrusion detection while also highlighting phenomena such as concept drift. Ultimately, the presented approach bridges the gap between high-dimensional GNN embeddings and human-understandable network behavior, offering new insights for network administrators and security analysts.",
    "authors": [
      "Enrique Feito-Casares",
      "Francisco M. Melgarejo-Meseguer",
      "Elena Casiraghi",
      "Giorgio Valentini",
      "José-Luis Rojo-Álvarez"
    ],
    "categories": [
      "cs.CR",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05817v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05817v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05787v1",
    "title": "Bagging-Based Model Merging for Robust General Text Embeddings",
    "summary": "General-purpose text embedding models underpin a wide range of NLP and information retrieval applications, and are typically trained on large-scale multi-task corpora to encourage broad generalization. However, it remains unclear how different multi-task training strategies compare in practice, and how to efficiently adapt embedding models as new domains and data types continually emerge. In this work, we present a systematic study of multi-task training for text embeddings from two perspectives: data scheduling and model merging. We compare batch-level shuffling, sequential training variants, two-stage training, and multiple merging granularities, and find that simple batch-level shuffling consistently yields the strongest overall performance, suggesting that task conflicts are limited and training datasets are largely complementary. Despite its effectiveness, batch-level shuffling exhibits two practical limitations: suboptimal out-of-domain (OOD) generalization and poor suitability for incremental learning due to expensive full retraining. To address these issues, we propose Bagging-based rObust mOdel Merging (\\modelname), which trains multiple embedding models on sampled subsets and merges them into a single model, improving robustness while retaining single-model inference efficiency. Moreover, \\modelname naturally supports efficient incremental updates by training lightweight update models on new data with a small historical subset and merging them into the existing model. Experiments across diverse embedding benchmarks demonstrate that \\modelname consistently improves both in-domain and OOD performance over full-corpus batch-level shuffling, while substantially reducing training cost in incremental learning settings.",
    "authors": [
      "Hengran Zhang",
      "Keping Bi",
      "Jiafeng Guo",
      "Jiaming Zhang",
      "Wenbo Yang",
      "Daiting Shi",
      "Xueqi Cheng"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05787v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05787v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05693v1",
    "title": "FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning",
    "summary": "Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.",
    "authors": [
      "Arno Geimer",
      "Beltran Fiz Pontiveros",
      "Radu State"
    ],
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05693v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05693v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05649v1",
    "title": "End-to-End Compression for Tabular Foundation Models",
    "summary": "The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.",
    "authors": [
      "Guri Zabërgja",
      "Rafiq Kamel",
      "Arlind Kadra",
      "Christian M. M. Frey",
      "Josif Grabocka"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05649v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05649v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05523v1",
    "title": "Capture the Flags: Family-Based Evaluation of Agentic LLMs via Semantics-Preserving Transformations",
    "summary": "Agentic large language models (LLMs) are increasingly evaluated on cybersecurity tasks using capture-the-flag (CTF) benchmarks. However, existing pointwise benchmarks have limited ability to shed light on the robustness and generalisation abilities of agents across alternative versions of the source code. We introduce CTF challenge families, whereby a single CTF is used as the basis for generating a family of semantically-equivalent challenges via semantics-preserving program transformations. This enables controlled evaluation of agent robustness to source code transformations while keeping the underlying exploit strategy fixed. We introduce a new tool, Evolve-CTF, that generates CTF families from Python challenges using a range of transformations. Using Evolve-CTF to derive families from Cybench and Intercode challenges, we evaluate 13 agentic LLM configurations with tool access. We find that models are remarkably robust to intrusive renaming and code insertion-based transformations, but that composed transformations and deeper obfuscation affect performance by requiring more sophisticated use of tools. We also find that enabling explicit reasoning has little effect on solution success rates across challenge families. Our work contributes a valuable technique and tool for future LLM evaluations, and a large dataset characterising the capabilities of current state-of-the-art models in this domain.",
    "authors": [
      "Shahin Honarvar",
      "Amber Gorzynski",
      "James Lee-Jones",
      "Harry Coppock",
      "Marek Rei",
      "Joseph Ryan",
      "Alastair F. Donaldson"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05523v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05523v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05496v1",
    "title": "XEmoGPT: An Explainable Multimodal Emotion Recognition Framework with Cue-Level Perception and Reasoning",
    "summary": "Explainable Multimodal Emotion Recognition plays a crucial role in applications such as human-computer interaction and social media analytics. However, current approaches struggle with cue-level perception and reasoning due to two main challenges: 1) general-purpose modality encoders are pretrained to capture global structures and general semantics rather than fine-grained emotional cues, resulting in limited sensitivity to emotional signals; and 2) available datasets usually involve a trade-off between annotation quality and scale, which leads to insufficient supervision for emotional cues and ultimately limits cue-level reasoning. Moreover, existing evaluation metrics are inadequate for assessing cue-level reasoning performance. To address these challenges, we propose eXplainable Emotion GPT (XEmoGPT), a novel EMER framework capable of both perceiving and reasoning over emotional cues. It incorporates two specialized modules: the Video Emotional Cue Bridge (VECB) and the Audio Emotional Cue Bridge (AECB), which enhance the video and audio encoders through carefully designed tasks for fine-grained emotional cue perception. To further support cue-level reasoning, we construct a large-scale dataset, EmoCue, designed to teach XEmoGPT how to reason over multimodal emotional cues. In addition, we introduce EmoCue-360, an automated metric that extracts and matches emotional cues using semantic similarity, and release EmoCue-Eval, a benchmark of 400 expert-annotated samples covering diverse emotional scenarios. Experimental results show that XEmoGPT achieves strong performance in both emotional cue perception and reasoning.",
    "authors": [
      "Hanwen Zhang",
      "Yao Liu",
      "Peiyuan Jiang",
      "Lang Junjie",
      "Xie Jun",
      "Yihui He",
      "Yajiao Deng",
      "Siyu Du",
      "Qiao Liu"
    ],
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05496v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05496v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2602.05859v1",
    "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
    "summary": "Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.",
    "authors": [
      "Xu Wang",
      "Bingqing Jiang",
      "Yu Wan",
      "Baosong Yang",
      "Lingpeng Kong",
      "Difan Zou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05859v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05859v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05855v1",
    "title": "A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion",
    "summary": "Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.",
    "authors": [
      "Dennis Bank",
      "Joost Cordes",
      "Thomas Seel",
      "Simon F. G. Ehlers"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05855v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05855v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05852v1",
    "title": "Exact Recovery in the Data Block Model",
    "summary": "Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and Verdú (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.",
    "authors": [
      "Amir R. Asadi",
      "Akbar Davoodi",
      "Ramin Javadi",
      "Farzad Parvaresh"
    ],
    "categories": [
      "cs.LG",
      "cs.IT",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05852v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05852v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05809v1",
    "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
    "summary": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR",
    "authors": [
      "Enwei Tong",
      "Yuanchao Bai",
      "Yao Zhu",
      "Junjun Jiang",
      "Xianming Liu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05809v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05809v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05783v1",
    "title": "Distributional Reinforcement Learning with Diffusion Bridge Critics",
    "summary": "Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.",
    "authors": [
      "Shutong Ding",
      "Yimiao Zhou",
      "Ke Hu",
      "Mokai Pan",
      "Shan Zhong",
      "Yanwei Fu",
      "Jingya Wang",
      "Ye Shi"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05783v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05783v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05734v1",
    "title": "Evaluating the impact of word embeddings on similarity scoring in practical information retrieval",
    "summary": "Search behaviour is characterised using synonymy and polysemy as users often want to search information based on meaning. Semantic representation strategies represent a move towards richer associative connections that can adequately capture this complex usage of language. Vector Space Modelling (VSM) and neural word embeddings play a crucial role in modern machine learning and Natural Language Processing (NLP) pipelines. Embeddings use distributional semantics to represent words, sentences, paragraphs or entire documents as vectors in high dimensional spaces. This can be leveraged by Information Retrieval (IR) systems to exploit the semantic relatedness between queries and answers.   This paper evaluates an alternative approach to measuring query statement similarity that moves away from the common similarity measure of centroids of neural word embeddings. Motivated by the Word Movers Distance (WMD) model, similarity is evaluated using the distance between individual words of queries and statements. Results from ranked query and response statements demonstrate significant gains in accuracy using the combined approach of similarity ranking through WMD with the word embedding techniques. The top performing WMD + GloVe combination outperforms all other state-of-the-art retrieval models including Doc2Vec and the baseline LSA model. Along with the significant gains in performance of similarity ranking through WMD, we conclude that the use of pre-trained word embeddings, trained on vast amounts of data, result in domain agnostic language processing solutions that are portable to diverse business use-cases.",
    "authors": [
      "Niall McCarroll",
      "Kevin Curran",
      "Eugene McNamee",
      "Angela Clist",
      "Andrew Brammer"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05734v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05734v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05728v1",
    "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
    "summary": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.   In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.   Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
    "authors": [
      "Hao Yang",
      "Zhiyu Yang",
      "Xupeng Zhang",
      "Wei Wei",
      "Yunjie Zhang",
      "Lin Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05728v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05728v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05708v1",
    "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
    "summary": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
    "authors": [
      "Chuangtao Ma",
      "Zeyu Zhang",
      "Arijit Khan",
      "Sebastian Schelter",
      "Paul Groth"
    ],
    "categories": [
      "cs.DB",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05708v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05708v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05679v1",
    "title": "Perception-Based Beliefs for POMDPs with Visual Observations",
    "summary": "Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.",
    "authors": [
      "Miriam Schäfers",
      "Merlijn Krale",
      "Thiago D. Simão",
      "Nils Jansen",
      "Maximilian Weininger"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05679v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05679v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05665v1",
    "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
    "summary": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
    "authors": [
      "Chang Yang",
      "Chuang Zhou",
      "Yilin Xiao",
      "Su Dong",
      "Luyao Zhuang",
      "Yujing Zhang",
      "Zhu Wang",
      "Zijin Hong",
      "Zheng Yuan",
      "Zhishang Xiang",
      "Shengyuan Chen",
      "Huachi Zhou",
      "Qinggang Zhang",
      "Ninghao Liu",
      "Jinsong Su",
      "Xinrun Wang",
      "Yi Chang",
      "Xiao Huang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05665v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05665v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05660v1",
    "title": "Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks",
    "summary": "The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism.   The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.",
    "authors": [
      "Slawek Smyl",
      "Paweł Pełka",
      "Grzegorz Dudek"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05660v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05660v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05591v1",
    "title": "Efficient Algorithms for Robust Markov Decision Processes with $s$-Rectangular Ambiguity Sets",
    "summary": "Robust Markov decision processes (MDPs) have attracted significant interest due to their ability to protect MDPs from poor out-of-sample performance in the presence of ambiguity. In contrast to classical MDPs, which account for stochasticity by modeling the dynamics through a stochastic process with a known transition kernel, a robust MDP additionally accounts for ambiguity by optimizing against the most adverse transition kernel from an ambiguity set constructed via historical data. In this paper, we develop a unified solution framework for a broad class of robust MDPs with $s$-rectangular ambiguity sets, where the most adverse transition probabilities are considered independently for each state. Using our algorithms, we show that $s$-rectangular robust MDPs with $1$- and $2$-norm as well as $φ$-divergence ambiguity sets can be solved several orders of magnitude faster than with state-of-the-art commercial solvers, and often only a logarithmic factor slower than classical MDPs. We demonstrate the favorable scaling properties of our algorithms on a range of synthetically generated as well as standard benchmark instances.",
    "authors": [
      "Chin Pang Ho",
      "Marek Petrik",
      "Wolfram Wiesemann"
    ],
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05591v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05591v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05512v1",
    "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
    "summary": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
    "authors": [
      "Larissa Pusch",
      "Alexandre Courtiol",
      "Tim Conrad"
    ],
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05512v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05512v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05447v1",
    "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
    "summary": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.   Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.   These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.",
    "authors": [
      "Damon McMillan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05447v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05447v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05444v1",
    "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
    "summary": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.",
    "authors": [
      "Yao Zhou",
      "Zeen Song",
      "Wenwen Qiang",
      "Fengge Wu",
      "Shuyi Zhou",
      "Changwen Zheng",
      "Hui Xiong"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05444v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05444v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2602.05863v1",
    "title": "Constrained Group Relative Policy Optimization",
    "summary": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.",
    "authors": [
      "Roger Girgis",
      "Rodrigue de Schaetzen",
      "Luke Rowe",
      "Azalée Robitaille",
      "Christopher Pal",
      "Liam Paull"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.RO"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05863v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05863v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05838v1",
    "title": "FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation",
    "summary": "Data is the lifeblood of AI, yet much of the most valuable data remains locked in silos due to privacy and regulations. As a result, AI remains heavily underutilized in many of the most important domains, including healthcare, education, and finance. Synthetic data generation (SDG), i.e. the generation of artificial data with a synthesizer trained on real data, offers an appealing solution to make data available while mitigating privacy concerns, however existing SDG-as-a-service workflow require data holders to trust providers with access to private data.We propose FHAIM, the first fully homomorphic encryption (FHE) framework for training a marginal-based synthetic data generator on encrypted tabular data. FHAIM adapts the widely used AIM algorithm to the FHE setting using novel FHE protocols, ensuring that the private data remains encrypted throughout and is released only with differential privacy guarantees. Our empirical analysis show that FHAIM preserves the performance of AIM while maintaining feasible runtimes.",
    "authors": [
      "Mayank Kumar",
      "Qian Lou",
      "Paulo Barreto",
      "Martine De Cock",
      "Sikha Pentyala"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05838v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05838v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05704v1",
    "title": "Limitations of SGD for Multi-Index Models Beyond Statistical Queries",
    "summary": "Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.",
    "authors": [
      "Daniel Barzilai",
      "Ohad Shamir"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05704v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05704v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05676v1",
    "title": "ShapeUP: Scalable Image-Conditioned 3D Editing",
    "summary": "Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.",
    "authors": [
      "Inbar Gat",
      "Dana Cohen-Bar",
      "Guy Levy",
      "Elad Richardson",
      "Daniel Cohen-Or"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05676v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05676v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05639v1",
    "title": "Joint Embedding Variational Bayes",
    "summary": "We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.",
    "authors": [
      "Amin Oji",
      "Paul Fieguth"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05639v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05639v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05576v1",
    "title": "OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph",
    "summary": "Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.",
    "authors": [
      "Chenxi Wan",
      "Xunkai Li",
      "Yilong Zuo",
      "Haokun Deng",
      "Sihan Li",
      "Bowen Fan",
      "Hongchao Qin",
      "Ronghua Li",
      "Guoren Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05576v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05576v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05533v1",
    "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
    "summary": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.",
    "authors": [
      "Zhengyi Guo",
      "Wenpin Tang",
      "Renyuan Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05533v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05533v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05515v1",
    "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
    "summary": "Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.",
    "authors": [
      "Ajo Babu George",
      "Anna Mariam John",
      "Athul Anoop",
      "Balu Bhasuran"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05515v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05515v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2602.05811v1",
    "title": "STProtein: predicting spatial protein expression from multi-omics data",
    "summary": "The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological \"Dark Matter\".",
    "authors": [
      "Zhaorui Jiang",
      "Yingfang Yuan",
      "Lei Hu",
      "Wei Pang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05811v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05811v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.05758v1",
    "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards",
    "summary": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.",
    "authors": [
      "Bowen Ping",
      "Zijun Chen",
      "Yiyao Yu",
      "Tingfeng Hui",
      "Junchi Yan",
      "Baobao Chang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05758v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05758v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.05636v1",
    "title": "Generative Ontology: When Structured Knowledge Learns to Create",
    "summary": "Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.   Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional \"anxiety\" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.   We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (\"bioluminescent fungi competing in a cave ecosystem\"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.   The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.",
    "authors": [
      "Benny Cheung"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05636v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05636v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.05605v1",
    "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
    "summary": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
    "authors": [
      "Jiaji Zhang",
      "Hailiang Zhao",
      "Guoxuan Zhu",
      "Ruichao Sun",
      "Jiaju Wu",
      "Xinkui Zhao",
      "Hanlin Tang",
      "Weiyi Lu",
      "Kan Liu",
      "Tao Lan",
      "Lin Qu",
      "Shuiguang Deng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05605v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05605v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2602.05861v1",
    "title": "CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs",
    "summary": "Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.",
    "authors": [
      "Seyedmasoud Mousavi",
      "Ruomeng Xu",
      "Xiaojing Zhu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05861v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05861v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05805v1",
    "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
    "summary": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
    "authors": [
      "Kang Chen",
      "Zhuoka Feng",
      "Sihan Zhao",
      "Kai Xiong",
      "Junjie Nian",
      "Yaoning Wang",
      "Changyi Xiao",
      "Yixin Cao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05805v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05805v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05798v1",
    "title": "Learning False Discovery Rate Control via Model-Based Neural Networks",
    "summary": "Controlling the false discovery rate (FDR) in high-dimensional variable selection requires balancing rigorous error control with statistical power. Existing methods with provable guarantees are often overly conservative, creating a persistent gap between the realized false discovery proportion (FDP) and the target FDR level. We introduce a learning-augmented enhancement of the T-Rex Selector framework that narrows this gap. Our approach replaces the analytical FDP estimator with a neural network trained solely on diverse synthetic datasets, enabling a substantially tighter and more accurate approximation of the FDP. This refinement allows the procedure to operate much closer to the desired FDR level, thereby increasing discovery power while maintaining effective approximate control. Through extensive simulations and a challenging synthetic genome-wide association study (GWAS), we demonstrate that our method achieves superior detection of true variables compared to existing approaches.",
    "authors": [
      "Arnau Vilella",
      "Jasin Machkour",
      "Michael Muma",
      "Daniel P. Palomar"
    ],
    "categories": [
      "stat.ME",
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05798v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05798v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05797v1",
    "title": "Classification Under Local Differential Privacy with Model Reversal and Model Averaging",
    "summary": "Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.",
    "authors": [
      "Caihong Qin",
      "Yang Bai"
    ],
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05797v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05797v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05786v1",
    "title": "Selecting Hyperparameters for Tree-Boosting",
    "summary": "Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.",
    "authors": [
      "Floris Jan Koster",
      "Fabio Sigrist"
    ],
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05786v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05786v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05723v1",
    "title": "Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification",
    "summary": "In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.",
    "authors": [
      "Taoye Yin",
      "Haoyuan Hu",
      "Yaxin Fan",
      "Xinhao Chen",
      "Xinya Wu",
      "Kai Deng",
      "Kezun Zhang",
      "Feng Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05723v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05723v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05598v1",
    "title": "CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion",
    "summary": "Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.",
    "authors": [
      "Aon Safdar",
      "Mohamed Saadeldin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05598v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05598v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05557v1",
    "title": "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds",
    "summary": "We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.",
    "authors": [
      "Michael Schwingshackl",
      "Fabio F. Oberweger",
      "Mario Niedermeyer",
      "Huemer Johannes",
      "Markus Murschitz"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05557v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05557v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05548v1",
    "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
    "authors": [
      "Zhiqi Yu",
      "Zhangquan Chen",
      "Mengting Liu",
      "Heye Zhang",
      "Liangqiong Qu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05548v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05548v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05494v1",
    "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "summary": "Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.",
    "authors": [
      "Qingyuan Wu",
      "Yuhui Wang",
      "Simon Sinong Zhan",
      "Yanning Dai",
      "Shilong Deng",
      "Sarra Habchi",
      "Qi Zhu",
      "Matthias Gallé",
      "Chao Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05494v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05494v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05480v1",
    "title": "SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing",
    "summary": "Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.",
    "authors": [
      "Peihao Wu",
      "Yongxiang Yao",
      "Yi Wan",
      "Wenfei Zhang",
      "Ruipeng Zhao",
      "Jiayuan Li",
      "Yongjun Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05480v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05480v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2602.05846v1",
    "title": "Optimal scaling laws in learning hierarchical multi-index models",
    "summary": "In this work, we provide a sharp theory of scaling laws for two-layer neural networks trained on a class of hierarchical multi-index targets, in a genuinely representation-limited regime. We derive exact information-theoretic scaling laws for subspace recovery and prediction error, revealing how the hierarchical features of the target are sequentially learned through a cascade of phase transitions. We further show that these optimal rates are achieved by a simple, target-agnostic spectral estimator, which can be interpreted as the small learning-rate limit of gradient descent on the first-layer weights. Once an adapted representation is identified, the readout can be learned statistically optimally, using an efficient procedure. As a consequence, we provide a unified and rigorous explanation of scaling laws, plateau phenomena, and spectral structure in shallow neural networks trained on such hierarchical targets.",
    "authors": [
      "Leonardo Defilippis",
      "Florent Krzakala",
      "Bruno Loureiro",
      "Antoine Maillard"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05846v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05846v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05833v1",
    "title": "Synthesizing Realistic Test Data without Breaking Privacy",
    "summary": "There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.",
    "authors": [
      "Laura Plein",
      "Alexi Turcotte",
      "Arina Hallemans",
      "Andreas Zeller"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05833v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05833v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05813v1",
    "title": "Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers",
    "summary": "We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.   Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup",
    "authors": [
      "Artem Riabinin",
      "Andrey Veprikov",
      "Arman Bolatov",
      "Martin Takáč",
      "Aleksandr Beznosikov"
    ],
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05813v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05813v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05789v1",
    "title": "Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation",
    "summary": "With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.",
    "authors": [
      "Hengyi Wang",
      "Ruiqiang Zhang",
      "Chang Liu",
      "Guanjie Wang",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05789v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05789v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05638v1",
    "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
    "summary": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.",
    "authors": [
      "Jinlin Wu",
      "Felix Holm",
      "Chuxi Chen",
      "An Wang",
      "Yaxin Hu",
      "Xiaofan Ye",
      "Zelin Zang",
      "Miao Xu",
      "Lihua Zhou",
      "Huai Liao",
      "Danny T. M. Chan",
      "Ming Feng",
      "Wai S. Poon",
      "Hongliang Ren",
      "Dong Yi",
      "Nassir Navab",
      "Gaofeng Meng",
      "Jiebo Luo",
      "Hongbin Liu",
      "Zhen Lei"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05638v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05638v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05532v1",
    "title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
    "summary": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.",
    "authors": [
      "Florian Dietz",
      "William Wale",
      "Oscar Gilg",
      "Robert McCarthy",
      "Felix Michalak",
      "Gustavo Ewbank Rodrigues Danon",
      "Miguelito de Guzman",
      "Dietrich Klakow"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05532v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05532v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05479v1",
    "title": "Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction",
    "summary": "Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.",
    "authors": [
      "Zhe Wang",
      "Zijing Liu",
      "Chencheng Xu",
      "Yuan Yao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05479v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05479v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05419v1",
    "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation",
    "summary": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.",
    "authors": [
      "Takumi Goto",
      "Yusuke Sakai",
      "Taro Watanabe"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05419v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05419v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2602.05827v1",
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "summary": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
    "authors": [
      "Hai Zhang",
      "Siqi Liang",
      "Li Chen",
      "Yuxian Li",
      "Yukuan Xu",
      "Yichao Zhong",
      "Fu Zhang",
      "Hongyang Li"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05827v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05827v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05822v1",
    "title": "NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects",
    "summary": "We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.",
    "authors": [
      "Musawar Ali",
      "Manuel Carranza-García",
      "Nicola Fioraio",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05822v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05822v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05776v1",
    "title": "Cross-Domain Offline Policy Adaptation via Selective Transition Correction",
    "summary": "It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.",
    "authors": [
      "Mengbei Yan",
      "Jiafei Lyu",
      "Shengjie Sun",
      "Zhongjian Qiao",
      "Jingwen Yang",
      "Zichuan Lin",
      "Deheng Ye",
      "Xiu Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05776v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05776v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05754v1",
    "title": "TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism",
    "summary": "Pipeline parallelism enables training models that exceed single-device memory, but practical throughput remains limited by pipeline bubbles. Although parameter freezing can improve training throughput by adaptively skipping backward computation, existing methods often over-freeze parameters, resulting in unnecessary accuracy degradation. To address this issue, we propose TimelyFreeze, which models the pipeline schedule as a directed acyclic graph and solves a linear program to compute optimal freeze ratios that minimize batch execution time under accuracy constraints. Experiments show that TimelyFreeze achieves up to 40% training throughput improvement on LLaMA-8B with comparable accuracy. Overall, it enables faster large-scale model training without compromising convergence and generalizes across diverse pipeline-parallel settings.",
    "authors": [
      "Seonghye Cho",
      "Jaemin Han",
      "Hyunjin Kim",
      "Euisoo Jung",
      "Jae-Gil Lee"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05754v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05754v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05690v1",
    "title": "Almost Asymptotically Optimal Active Clustering Through Pairwise Observations",
    "summary": "We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.",
    "authors": [
      "Rachel S. Y. Teo",
      "P. N. Karthik",
      "Ramya Korlakai Vinayak",
      "Vincent Y. F. Tan"
    ],
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05690v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05690v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05657v1",
    "title": "Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization",
    "summary": "The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \\in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{β_p}/\\log(t)}$, where $β_p = \\frac{4(p-1)}{3p-2}$ for $p \\in (1,2)$ and $e^{-t/\\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\\sqrt{t}}$ and $e^{-t^{β_p/2}}$, $p \\in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.",
    "authors": [
      "Aleksandar Armacki",
      "Dragana Bajović",
      "Dušan Jakovetić",
      "Soummya Kar",
      "Ali H. Sayed"
    ],
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05657v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05657v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05644v1",
    "title": "UAV Trajectory Optimization via Improved Noisy Deep Q-Network",
    "summary": "This paper proposes an Improved Noisy Deep Q-Network (Noisy DQN) to enhance the exploration and stability of Unmanned Aerial Vehicle (UAV) when applying deep reinforcement learning in simulated environments. This method enhances the exploration ability by combining the residual NoisyLinear layer with an adaptive noise scheduling mechanism, while improving training stability through smooth loss and soft target network updates. Experiments show that the proposed model achieves faster convergence and up to $+40$ higher rewards compared to standard DQN and quickly reach to the minimum number of steps required for the task 28 in the 15 * 15 grid navigation environment set up. The results show that our comprehensive improvements to the network structure of NoisyNet, exploration control, and training stability contribute to enhancing the efficiency and reliability of deep Q-learning.",
    "authors": [
      "Zhang Hengyu",
      "Maryam Cheraghy",
      "Liu Wei",
      "Armin Farhadi",
      "Meysam Soltanpour",
      "Zhong Zhuoqing"
    ],
    "categories": [
      "eess.SY",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05644v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05644v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05633v1",
    "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models",
    "summary": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.",
    "authors": [
      "Rui Jia",
      "Ruiyi Lan",
      "Fengrui Liu",
      "Zhongxiang Dai",
      "Bo Jiang",
      "Jing Shao",
      "Jingyuan Chen",
      "Guandong Xu",
      "Fei Wu",
      "Min Zhang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05633v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05633v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05555v1",
    "title": "IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools",
    "summary": "We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.",
    "authors": [
      "Panagiotis Sapoutzoglou",
      "Orestis Vaggelis",
      "Athina Zacharia",
      "Evangelos Sartinas",
      "Maria Pateraki"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05555v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05555v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05423v1",
    "title": "NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks",
    "summary": "In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.",
    "authors": [
      "Pengcheng Chen",
      "Yue Hu",
      "Wenhao Li",
      "Nicole M Gunderson",
      "Andrew Feng",
      "Zhenglong Sun",
      "Peter Beerel",
      "Eric J Seibel"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05423v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05423v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2602.05842v1",
    "title": "Reinforcement World Model Learning for LLM-based Agents",
    "summary": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $τ^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $τ^2$ Bench respectively, while matching the performance of expert-data training.",
    "authors": [
      "Xiao Yu",
      "Baolin Peng",
      "Ruize Xu",
      "Yelong Shen",
      "Pengcheng He",
      "Suman Nath",
      "Nikhil Singh",
      "Jiangfeng Gao",
      "Zhou Yu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05842v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05842v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05812v1",
    "title": "Principled Confidence Estimation for Deep Computed Tomography",
    "summary": "We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.",
    "authors": [
      "Matteo Gätzner",
      "Johannes Kirschner"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05812v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05812v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05810v1",
    "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents",
    "summary": "Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.",
    "authors": [
      "Quan M. Tran",
      "Zhuo Huang",
      "Wenbin Zhang",
      "Bo Han",
      "Koji Yatani",
      "Masashi Sugiyama",
      "Tongliang Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05810v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05810v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05735v1",
    "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
    "summary": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
    "authors": [
      "Lixuan Guo",
      "Yifei Wang",
      "Tiansheng Wen",
      "Yifan Wang",
      "Aosong Feng",
      "Bo Chen",
      "Stefanie Jegelka",
      "Chenyu You"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.IT"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05735v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05735v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05718v1",
    "title": "Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization",
    "summary": "Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.",
    "authors": [
      "Yunchuan Ma",
      "Laiyun Qing",
      "Guorong Li",
      "Yuqing Liu",
      "Yuankai Qi",
      "Qingming Huang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05718v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05718v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05710v1",
    "title": "Ethology of Latent Spaces",
    "summary": "This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.   Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.   We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault's notion of the archive, Jameson's ideologeme, and Simondon's theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.",
    "authors": [
      "Philippe Boisnard"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05710v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05710v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05692v1",
    "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations",
    "summary": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.",
    "authors": [
      "Congbo Ma",
      "Yichun Zhang",
      "Yousef Al-Jazzazi",
      "Ahamed Foisal",
      "Laasya Sharma",
      "Yousra Sadqi",
      "Khaled Saleh",
      "Jihad Mallat",
      "Farah E. Shamout"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05692v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05692v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05552v1",
    "title": "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator",
    "summary": "This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.",
    "authors": [
      "Bessie Dominguez-Dager",
      "Sergio Suescun-Ferrandiz",
      "Felix Escalona",
      "Francisco Gomez-Donoso",
      "Miguel Cazorla"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05552v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05552v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05499v1",
    "title": "SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration",
    "summary": "Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.",
    "authors": [
      "Hanyu Wei",
      "Zunhai Su",
      "Peng Lu",
      "Chao Li",
      "Spandan Tiwari",
      "Ashish Sirasao",
      "Yuhan Dong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05499v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05499v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05495v1",
    "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models",
    "summary": "Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.",
    "authors": [
      "Chenhang Cui",
      "Binyun Yang",
      "Fei Shen",
      "Yuxin Chen",
      "Jingnan Zheng",
      "Xiang Wang",
      "An Zhang",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05495v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05495v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05441v1",
    "title": "Benchmarking Affordance Generalization with BusyBox",
    "summary": "Vision-Language-Action (VLA) models have been attracting the attention of researchers and practitioners thanks to their promise of generalization. Although single-task policies still offer competitive performance, VLAs are increasingly able to handle commands and environments unseen in their training set. While generalization in vision and language space is undoubtedly important for robust versatile behaviors, a key meta-skill VLAs need to possess is affordance generalization -- the ability to manipulate new objects with familiar physical features.   In this work, we present BusyBox, a physical benchmark for systematic semi-automatic evaluation of VLAs' affordance generalization. BusyBox consists of 6 modules with switches, sliders, wires, buttons, a display, and a dial. The modules can be swapped and rotated to create a multitude of BusyBox variations with different visual appearances but the same set of affordances. We empirically demonstrate that generalization across BusyBox variants is highly challenging even for strong open-weights VLAs such as $π_{0.5}$ and GR00T-N1.6. To encourage the research community to evaluate their own VLAs on BusyBox and to propose new affordance generalization experiments, we have designed BusyBox to be easy to build in most robotics labs. We release the full set of CAD files for 3D-printing its parts as well as a bill of materials for (optionally) assembling its electronics. We also publish a dataset of language-annotated demonstrations that we collected using the common bimanual Mobile Aloha robot on the canonical BusyBox configuration. All of the released materials are available at https://microsoft.github.io/BusyBox.",
    "authors": [
      "Dean Fortier",
      "Timothy Adamson",
      "Tess Hellebrekers",
      "Teresa LaScala",
      "Kofi Ennin",
      "Michael Murray",
      "Andrey Kolobov",
      "Galen Mullins"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05441v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05441v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05413v1",
    "title": "SciDef: Automating Definition Extraction from Academic Literature with Large Language Models",
    "summary": "Definitions are the foundation for any scientific work, but with a significant increase in publication numbers, gathering definitions relevant to any keyword has become challenging. We therefore introduce SciDef, an LLM-based pipeline for automated definition extraction. We test SciDef on DefExtra & DefSim, novel datasets of human-extracted definitions and definition-pairs' similarity, respectively. Evaluating 16 language models across prompting strategies, we demonstrate that multi-step and DSPy-optimized prompting improve extraction performance. To evaluate extraction, we test various metrics and show that an NLI-based method yields the most reliable results. We show that LLMs are largely able to extract definitions from scientific literature (86.4% of definitions from our test-set); yet future work should focus not just on finding definitions, but on identifying relevant ones, as models tend to over-generate them.   Code & datasets are available at https://github.com/Media-Bias-Group/SciDef.",
    "authors": [
      "Filip Kučera",
      "Christoph Mandl",
      "Isao Echizen",
      "Radu Timofte",
      "Timo Spinde"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05413v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05413v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2602.05790v1",
    "title": "Price of universality in vector quantization is at most 0.11 bit",
    "summary": "Fast computation of a matrix product $W^\\top X$ is a workhorse of modern LLMs. To make their deployment more efficient, a popular approach is that of using a low-precision approximation $\\widehat W$ in place of true $W$ (\"weight-only quantization''). Information theory demonstrates that an optimal algorithm for reducing precision of $W$ depends on the (second order) statistics of $X$ and requires a careful alignment of vector quantization codebook with PCA directions of $X$ (a process known as \"waterfilling allocation''). Dependence of the codebook on statistics of $X$, however, is highly impractical. This paper proves that there exist a universal codebook that is simultaneously near-optimal for all possible statistics of $X$, in the sense of being at least as good as an $X$-adapted waterfilling codebook with rate reduced by 0.11 bit per dimension. Such universal codebook would be an ideal candidate for the low-precision storage format, a topic of active modern research, but alas the existence proof is non-constructive.   Equivalently, our result shows existence of a net in $\\mathbb{R}^n$ that is a nearly-optimal covering of a sphere simultaneously with respect to all Hilbert norms.",
    "authors": [
      "Alina Harbuzova",
      "Or Ordentlich",
      "Yury Polyanskiy"
    ],
    "categories": [
      "cs.IT",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05790v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05790v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05774v1",
    "title": "Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance",
    "summary": "Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.",
    "authors": [
      "Xiandong Zou",
      "Jianshu Li",
      "Jing Huang",
      "Pan Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05774v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05774v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05762v1",
    "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
    "summary": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
    "authors": [
      "Andrei Kozyrev",
      "Nikita Khramov",
      "Denis Lochmelis",
      "Valerio Morelli",
      "Gleb Solovev",
      "Anton Podkopaev"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SE"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05762v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05762v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05749v1",
    "title": "How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning",
    "summary": "Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.",
    "authors": [
      "Kai Ming Ting",
      "Wei-Jie Xu",
      "Hang Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05749v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05749v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05725v1",
    "title": "Muon in Associative Memory Learning: Training Dynamics and Scaling Laws",
    "summary": "Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.",
    "authors": [
      "Binghui Li",
      "Kaifei Wang",
      "Han Zhong",
      "Pinyan Lu",
      "Liwei Wang"
    ],
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05725v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05725v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05713v1",
    "title": "Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions",
    "summary": "Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a \"fairness cost\" term $δ_t = \\sqrt{\\mathrm{KL}(w^t \\| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.",
    "authors": [
      "Amir Asiaee",
      "Kaveh Aryan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05713v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05713v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05648v1",
    "title": "Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew",
    "summary": "We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.",
    "authors": [
      "Giuseppe Samo",
      "Paola Merlo"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05648v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05648v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05570v1",
    "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?",
    "summary": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.",
    "authors": [
      "Yikun Zong",
      "Cheston Tan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05570v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05570v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05567v1",
    "title": "MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks",
    "summary": "Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.",
    "authors": [
      "Long D. Nguyen",
      "Binh P. Nguyen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05567v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05567v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05539v1",
    "title": "Steering Large Reasoning Models towards Concise Reasoning via Flow Matching",
    "summary": "Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.",
    "authors": [
      "Yawei Li",
      "Benjamin Bergner",
      "Yinghan Zhao",
      "Vihang Prakash Patil",
      "Bei Chen",
      "Cheng Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05539v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05539v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05522v1",
    "title": "Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification",
    "summary": "Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.",
    "authors": [
      "Jeongbin You",
      "Donggun Kim",
      "Sejun Park",
      "Seungsang Oh"
    ],
    "categories": [
      "cs.CV",
      "math.GT"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05522v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05522v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05513v1",
    "title": "DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter",
    "summary": "Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.",
    "authors": [
      "Xukun Li",
      "Yu Sun",
      "Lei Zhang",
      "Bosheng Huang",
      "Yibo Peng",
      "Yuan Meng",
      "Haojun Jiang",
      "Shaoxuan Xie",
      "Guacai Yao",
      "Alois Knoll",
      "Zhenshan Bing",
      "Xinlong Wang",
      "Zhenguo Sun"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05513v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05513v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05463v1",
    "title": "Thermodynamic Limits of Physical Intelligence",
    "summary": "Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.",
    "authors": [
      "Koichi Takahashi",
      "Yusuke Hayashi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05463v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05463v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2602.05712v1",
    "title": "Towards Green AI: Decoding the Energy of LLM Inference in Software Development",
    "summary": "Context: AI-assisted tools are increasingly integrated into software development workflows, but their reliance on large language models (LLMs) introduces substantial computational and energy costs. Understanding and reducing the energy footprint of LLM inference is therefore essential for sustainable software development. Objective: In this study, we conduct a phase-level analysis of LLM inference energy consumption, distinguishing between the (1) prefill, where the model processes the input and builds internal representations, and (2) decoding, where output tokens are generated using the stored state. Method: We investigate six 6B-7B and four 3B-4B transformer-based models, evaluating them on code-centric benchmarks HumanEval for code generation and LongBench for code understanding. Results: Our findings show that, within both parameter groups, models exhibit distinct energy patterns across phases. Furthermore, we observed that increases in prefill cost amplify the energy cost per token during decoding, with amplifications ranging from 1.3% to 51.8% depending on the model. Lastly, three out of ten models demonstrate babbling behavior, adding excessive content to the output that unnecessarily inflates energy consumption. We implemented babbling suppression for code generation, achieving energy savings ranging from 44% to 89% without affecting generation accuracy. Conclusion: These findings show that prefill costs influence decoding, which dominates energy consumption, and that babbling suppression can yield up to 89% energy savings. Reducing inference energy therefore requires both mitigating babbling behavior and limiting impact of prefill on decoding.",
    "authors": [
      "Lola Solovyeva",
      "Fernando Castor"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05712v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05712v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05695v1",
    "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
    "summary": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
    "authors": [
      "Hiari Pizzini Cavagna",
      "Andrea Proia",
      "Giacomo Madella",
      "Giovanni B. Esposito",
      "Francesco Antici",
      "Daniele Cesarini",
      "Zeynep Kiziltan",
      "Andrea Bartolini"
    ],
    "categories": [
      "cs.AI",
      "cs.PF"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05695v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05695v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05590v1",
    "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
    "summary": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.",
    "authors": [
      "Haojie Cheng",
      "Shaun Jing Heng Ong",
      "Shaoyu Cai",
      "Aiden Tat Yang Koh",
      "Fuxi Ouyang",
      "Eng Tat Khoo"
    ],
    "categories": [
      "cs.CV",
      "cs.ET",
      "cs.GR"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05590v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05590v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05541v1",
    "title": "Reducing the Complexity of Matrix Multiplication to $O(N^2log_2N)$ by an Asymptotically Optimal Quantum Algorithm",
    "summary": "Matrix multiplication is a fundamental classical computing operation whose efficiency becomes a major challenge at scale, especially for machine learning applications. Quantum computing, with its inherent parallelism and exponential storage capacity, offers a potential solution to these limitations. This work presents a quantum kernel-based matrix multiplication algorithm (QKMM) that achieves an asymptotically optimal computational complexity of $ O(N^2 \\log_2 N) $, outperforming the classical optimal complexity of $ O(N^{2.371552}) $, where $N$ denotes the matrix dimension. Through noiseless and noisy quantum simulation experiments, we demonstrate that the proposed algorithm not only exhibits superior theoretical efficiency but also shows practical advantages in runtime performance and stability.",
    "authors": [
      "Jiaqi Yao",
      "Ding Liu"
    ],
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05541v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05541v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05486v1",
    "title": "Sovereign-by-Design A Reference Architecture for AI and Blockchain Enabled Systems",
    "summary": "Digital sovereignty has emerged as a central concern for modern software-intensive systems, driven by the dominance of non-sovereign cloud infrastructures, the rapid adoption of Generative AI, and increasingly stringent regulatory requirements. While existing initiatives address governance, compliance, and security in isolation, they provide limited guidance on how sovereignty can be operationalized at the architectural level. In this paper, we argue that sovereignty must be treated as a first-class architectural property rather than a purely regulatory objective. We introduce a Sovereign Reference Architecture that integrates self-sovereign identity, blockchain-based trust and auditability, sovereign data governance, and Generative AI deployed under explicit architectural control. The architecture explicitly captures the dual role of Generative AI as both a source of governance risk and an enabler of compliance, accountability, and continuous assurance when properly constrained. By framing sovereignty as an architectural quality attribute, our work bridges regulatory intent and concrete system design, offering a coherent foundation for building auditable, evolvable, and jurisdiction-aware AI-enabled systems. The proposed reference architecture provides a principled starting point for future research and practice at the intersection of software architecture, Generative AI, and digital sovereignty.",
    "authors": [
      "Matteo Esposito",
      "Lodovica Marchesi",
      "Roberto Tonelli",
      "Valentina Lenarduzzi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05486v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05486v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05453v1",
    "title": "Towards Segmenting the Invisible: An End-to-End Registration and Segmentation Framework for Weakly Supervised Tumour Analysis",
    "summary": "Liver tumour ablation presents a significant clinical challenge: whilst tumours are clearly visible on pre-operative MRI, they are often effectively invisible on intra-operative CT due to minimal contrast between pathological and healthy tissue. This work investigates the feasibility of cross-modality weak supervision for scenarios where pathology is visible in one modality (MRI) but absent in another (CT). We present a hybrid registration-segmentation framework that combines MSCGUNet for inter-modal image registration with a UNet-based segmentation module, enabling registration-assisted pseudo-label generation for CT images. Our evaluation on the CHAOS dataset demonstrates that the pipeline can successfully register and segment healthy liver anatomy, achieving a Dice score of 0.72. However, when applied to clinical data containing tumours, performance degrades substantially (Dice score of 0.16), revealing the fundamental limitations of current registration methods when the target pathology lacks corresponding visual features in the target modality. We analyse the \"domain gap\" and \"feature absence\" problems, demonstrating that whilst spatial propagation of labels via registration is feasible for visible structures, segmenting truly invisible pathology remains an open challenge. Our findings highlight that registration-based label transfer cannot compensate for the absence of discriminative features in the target modality, providing important insights for future research in cross-modality medical image analysis. Code an weights are available at: https://github.com/BudhaTronix/Weakly-Supervised-Tumour-Detection",
    "authors": [
      "Budhaditya Mukhopadhyay",
      "Chirag Mandal",
      "Pavan Tummala",
      "Naghmeh Mahmoodian",
      "Andreas Nürnberger",
      "Soumick Chatterjee"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "physics.med-ph"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05453v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05453v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05434v1",
    "title": "LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects",
    "summary": "Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.",
    "authors": [
      "Sanghoon Jeon",
      "Gihyun Jung",
      "Suhyeon Ka",
      "Jae-Sang Hyun"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05434v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05434v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2602.05857v1",
    "title": "BABE: Biology Arena BEnchmark",
    "summary": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
    "authors": [
      "Junting Zhou",
      "Jin Chen",
      "Linfeng Hao",
      "Denghui Cao",
      "Zheyu Wang",
      "Qiguang Chen",
      "Chaoyou Fu",
      "Jiaze Chen",
      "Yuchen Wu",
      "Ge Zhang",
      "Mingxuan Wang",
      "Wenhao Huang",
      "Tong Yang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05857v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05857v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05848v1",
    "title": "DARWIN: Dynamic Agentically Rewriting Self-Improving Network",
    "summary": "DARWIN is an evolutionary GPT model, utilizing a genetic-algorithm like optimization structure with several independent GPT agents being trained individually using unique training code. Each iteration, the GPT models are prompted to modify the training code of one another in an attempt to improve their performance in a mutation-like manner, and the best GPT agents are then benchmarked and selected for the next iteration by genetic algorithm. For demonstration purposes and due to budget and time constraints, OpenAI API is used to prompt training code improvements and the nanoGPT framework is used as the training code. DARWIN also utilizes persistent JSON-based memory files to track previous reasoning and changes to code to correlate with improvement to model performance. and a bidirectional interface for HITL intervention allowing the model to request upgrades such as additional datasets, training scripts, and restructuring of file hierarchies. In experiments, DARWIN achieved a 1.26 percent improvement in model FLOPS utilization (MFU) and a 2.07 percent improvement to perplexity in 5 iterations of training over baseline configurations, demonstrating promising capabilities as a foundation for scaling evolutionary GPT training.",
    "authors": [
      "Henry Jiang"
    ],
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05848v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05848v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05832v1",
    "title": "UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents",
    "summary": "Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io",
    "authors": [
      "Han Xiao",
      "Guozhi Wang",
      "Hao Wang",
      "Shilong Liu",
      "Yuxiang Chai",
      "Yue Pan",
      "Yufeng Zhou",
      "Xiaoxin Chen",
      "Yafei Wen",
      "Hongsheng Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05832v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05832v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05780v1",
    "title": "Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes",
    "summary": "Code completion (CC) is a task frequently used by developers when working in collaboration with LLM-based programming assistants. Despite the increased performance of LLMs on public benchmarks, out of the box LLMs still have a hard time generating code that aligns with a private code repository not previously seen by the model's training data. Customizing code LLMs to a private repository provides a way to improve the model performance. In this paper we present our approach for automated LLM customization based on semantic scopes in the code. We evaluate LLMs on real industry cases with two private enterprise code repositories with two customization strategies: Retrieval-Augmented Generation (RAG) and supervised Fine-Tuning (FT). Our mechanism for ingesting the repository's data and formulating the training data pairs with semantic scopes helps models to learn the underlying patterns specific to the repository, providing more precise code to developers and helping to boost their productivity. The code completions of moderately sized customized models can be significantly better than those of uncustomized models of much larger capacity. We also include an analysis of customization on two public benchmarks and present opportunities for future work.",
    "authors": [
      "Ulrich Finkler",
      "Irene Manotas",
      "Wei Zhang",
      "Geert Janssen",
      "Octavian Popescu",
      "Shyam Ramji"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05780v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05780v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05737v1",
    "title": "Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing",
    "summary": "In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.",
    "authors": [
      "Luca Ciampi",
      "Ludovico Iannello",
      "Fabrizio Tonelli",
      "Gabriele Lagani",
      "Angelo Di Garbo",
      "Federico Cremisi",
      "Giuseppe Amato"
    ],
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05737v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05737v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05706v1",
    "title": "Poster: Camera Tampering Detection for Outdoor IoT Systems",
    "summary": "Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.",
    "authors": [
      "Shadi Attarha",
      "Kanaga Shanmugi",
      "Anna Förster"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05706v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05706v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05687v1",
    "title": "Exploring AI-Augmented Sensemaking of Patient-Generated Health Data: A Mixed-Method Study with Healthcare Professionals in Cardiac Risk Reduction",
    "summary": "Individuals are increasingly generating substantial personal health and lifestyle data, e.g. through wearables and smartphones. While such data could transform preventative care, its integration into clinical practice is hindered by its scale, heterogeneity and the time pressure and data literacy of healthcare professionals (HCPs). We explore how large language models (LLMs) can support sensemaking of patient-generated health data (PGHD) with automated summaries and natural language data exploration. Using cardiovascular disease (CVD) risk reduction as a use case, 16 HCPs reviewed multimodal PGHD in a mixed-methods study with a prototype that integrated common charts, LLM-generated summaries, and a conversational interface. Findings show that AI summaries provided quick overviews that anchored exploration, while conversational interaction supported flexible analysis and bridged data-literacy gaps. However, HCPs raised concerns about transparency, privacy, and overreliance. We contribute empirical insights and sociotechnical design implications for integrating AI-driven summarization and conversation into clinical workflows to support PGHD sensemaking.",
    "authors": [
      "Pavithren V S Pakianathan",
      "Rania Islambouli",
      "Diogo Branco",
      "Albrecht Schmidt",
      "Tiago Guerreiro",
      "Jan David Smeddinck"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05687v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05687v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05635v1",
    "title": "Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias",
    "summary": "Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.",
    "authors": [
      "Ojasva Nema",
      "Kaustubh Sharma",
      "Aditya Chauhan",
      "Parikshit Pareek"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05635v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05635v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05578v1",
    "title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation",
    "summary": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.",
    "authors": [
      "Junyang Chen",
      "Xiangbo Lv",
      "Zhiqiang Kou",
      "Xingdong Sheng",
      "Ning Xu",
      "Yiguo Qiao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05578v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05578v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05493v1",
    "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation",
    "summary": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.",
    "authors": [
      "Bingru Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05493v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05493v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05467v1",
    "title": "MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation",
    "summary": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.",
    "authors": [
      "Dekang Qi",
      "Shuang Zeng",
      "Xinyuan Chang",
      "Feng Xiong",
      "Shichao Xie",
      "Xiaolong Wu",
      "Mu Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.RO"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05467v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05467v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2602.05799v1",
    "title": "Non-Stationary Inventory Control with Lead Times",
    "summary": "We study non-stationary single-item, periodic-review inventory control problems in which the demand distribution is unknown and may change over time. We analyze how demand non-stationarity affects learning performance across inventory models, including systems with demand backlogging or lost-sales, both with and without lead times. For each setting, we propose an adaptive online algorithm that optimizes over the class of base-stock policies and establish performance guarantees in terms of dynamic regret relative to the optimal base-stock policy at each time step. Our results reveal a sharp separation across inventory models. In backlogging systems and lost-sales models with zero lead time, we show that it is possible to adapt to demand changes without incurring additional performance loss in stationary environments, even without prior knowledge of the demand distributions or the number of demand shifts. In contrast, for lost-sales systems with positive lead times, we establish weaker guarantees that reflect fundamental limitations imposed by delayed replenishment in combination with censored feedback. Our algorithms leverage the convexity and one-sided feedback structure of inventory costs to enable counterfactual policy evaluation despite demand censoring. We complement the theoretical analysis with simulation results showing that our methods significantly outperform existing benchmarks.",
    "authors": [
      "Nele H. Amiri",
      "Sean R. Sinclair",
      "Maximiliano Udenio"
    ],
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05799v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05799v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05779v1",
    "title": "How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs",
    "summary": "The intermediate layers of deep networks can be characterised as a Gaussian process, in particular the Edge-of-Chaos (EoC) initialisation strategy prescribes the limiting covariance matrix of the Gaussian process. Here we show that the under-utilised chosen variance of the Gaussian process is important in the training of deep networks with sparsity inducing activation, such as a shifted and clipped ReLU, $\\text{CReLU}_{τ,m}(x)=\\min(\\max(x-τ,0),m)$. Specifically, initialisations leading to larger fixed Gaussian process variances, allow for improved expressivity with activation sparsity as large as 90% in DNNs and CNNs, and generally improve the stability of the training process. Enabling full, or near full, accuracy at such high levels of sparsity in the hidden layers suggests a promising mechanism to reduce the energy consumption of machine learning models involving fully connected layers.",
    "authors": [
      "Emily Dent",
      "Jared Tanner"
    ],
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05779v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05779v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05769v1",
    "title": "Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors",
    "summary": "LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.",
    "authors": [
      "Adnan Al Ali",
      "Jindřich Helcl",
      "Jindřich Libovický"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05769v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05769v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05746v1",
    "title": "Learning to Inject: Automated Prompt Injection via Reinforcement Learning",
    "summary": "Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.",
    "authors": [
      "Xin Chen",
      "Jie Zhang",
      "Florian Tramer"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05746v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05746v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05717v1",
    "title": "Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.",
    "authors": [
      "Tianyi Wang",
      "Long Li",
      "Hongcan Guo",
      "Yibiao Chen",
      "Yixia Li",
      "Yong Wang",
      "Yun Chen",
      "Guanhua Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05717v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05717v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05709v1",
    "title": "Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions",
    "summary": "Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.",
    "authors": [
      "Yihao Ouyang",
      "Shiwei Li",
      "Haozhao Wang",
      "Xiandi Luo",
      "Zhuoqi Hu",
      "Yuetong Song",
      "Qiyu Qin",
      "Yichen Li",
      "Ruixuan Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05709v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05709v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05702v1",
    "title": "Broken neural scaling laws in materials science",
    "summary": "In materials science, data are scarce and expensive to generate, whether computationally or experimentally. Therefore, it is crucial to identify how model performance scales with dataset size and model capacity to distinguish between data- and model-limited regimes. Neural scaling laws provide a framework for quantifying this behavior and guide the design of materials datasets and machine learning architectures. Here, we investigate neural scaling laws for a paradigmatic materials science task: predicting the dielectric function of metals, a high-dimensional response that governs how solids interact with light. Using over 200,000 dielectric functions from high-throughput ab initio calculations, we study two multi-objective graph neural networks trained to predict the frequency-dependent complex interband dielectric function and the Drude frequency. We observe broken neural scaling laws with respect to dataset size, whereas scaling with the number of model parameters follows a simple power law that rapidly saturates.",
    "authors": [
      "Max Großmann",
      "Malte Grunert",
      "Erich Runge"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05702v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05702v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05629v1",
    "title": "ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing",
    "summary": "Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.",
    "authors": [
      "Jianlei Chi",
      "Yuzhen Wu",
      "Jiaxuan Hou",
      "Xiaodong Zhang",
      "Ming Fan",
      "Suhui Sun",
      "Weijun Dai",
      "Bo Li",
      "Jianguo Sun",
      "Jun Sun"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05629v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05629v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05619v1",
    "title": "Mode-Dependent Rectification for Stable PPO Training",
    "summary": "Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.",
    "authors": [
      "Mohamad Mohamad",
      "Francesco Ponzio",
      "Xavier Descombes"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05619v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05619v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05572v1",
    "title": "ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors",
    "summary": "We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.",
    "authors": [
      "Zhenxiao Liang",
      "Ning Zhang",
      "Youbao Tang",
      "Ruei-Sung Lin",
      "Qixing Huang",
      "Peng Chang",
      "Jing Xiao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05572v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05572v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05487v1",
    "title": "Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014",
    "summary": "What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).",
    "authors": [
      "Julien Moreau",
      "S. Ambellouis",
      "Yassine Ruichek"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05487v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05487v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05456v1",
    "title": "Ontology-Driven Robotic Specification Synthesis",
    "summary": "This paper addresses robotic system engineering for safety- and mission-critical applications by bridging the gap between high-level objectives and formal, executable specifications. The proposed method, Robotic System Task to Model Transformation Methodology (RSTM2) is an ontology-driven, hierarchical approach using stochastic timed Petri nets with resources, enabling Monte Carlo simulations at mission, system, and subsystem levels. A hypothetical case study demonstrates how the RSTM2 method supports architectural trades, resource allocation, and performance analysis under uncertainty. Ontological concepts further enable explainable AI-based assistants, facilitating fully autonomous specification synthesis. The methodology offers particular benefits to complex multi-robot systems, such as the NASA CADRE mission, representing decentralized, resource-aware, and adaptive autonomous systems of the future.",
    "authors": [
      "Maksym Figat",
      "Ryan M. Mackey",
      "Michel D. Ingham"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SY"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05456v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05456v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05430v1",
    "title": "Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy",
    "summary": "Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.",
    "authors": [
      "Kritchanat Ponyuenyong",
      "Pengyu Tu",
      "Jia Wei Tan",
      "Wei Soon Cheong",
      "Jamie Ng Suat Ling",
      "Lianlian Jiang"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05430v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05430v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2602.05845v1",
    "title": "Self-Supervised Learning with a Multi-Task Latent Space Objective",
    "summary": "Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.",
    "authors": [
      "Pierre-François De Plaen",
      "Abhishek Jha",
      "Luc Van Gool",
      "Tinne Tuytelaars",
      "Marc Proesmans"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05845v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05845v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.05742v1",
    "title": "Fast Rates for Nonstationary Weighted Risk Minimization",
    "summary": "Weighted empirical risk minimization is a common approach to prediction under distribution drift. This article studies its out-of-sample prediction error under nonstationarity. We provide a general decomposition of the excess risk into a learning term and an error term associated with distribution drift, and prove oracle inequalities for the learning error under mixing conditions. The learning bound holds uniformly over arbitrary weight classes and accounts for the effective sample size induced by the weight vector, the complexity of the weight and hypothesis classes, and potential data dependence. We illustrate the applicability and sharpness of our results in (auto-) regression problems with linear models, basis approximations, and neural networks, recovering minimax-optimal rates (up to logarithmic factors) when specialized to unweighted and stationary settings.",
    "authors": [
      "Tobias Brock",
      "Thomas Nagler"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05742v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05742v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.05650v1",
    "title": "Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances",
    "summary": "Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.",
    "authors": [
      "Amir Ansari",
      "Jana Subirana",
      "Bruna Silva",
      "Sergio Escalera",
      "David Gallardo-Pujol",
      "Cristina Palmero"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05650v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05650v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.05489v1",
    "title": "Convergence Rate of the Last Iterate of Stochastic Proximal Algorithms",
    "summary": "We analyze two classical algorithms for solving additively composite convex optimization problems where the objective is the sum of a smooth term and a nonsmooth regularizer: proximal stochastic gradient method for a single regularizer; and the randomized incremental proximal method, which uses the proximal operator of a randomly selected function when the regularizer is given as the sum of many nonsmooth functions. We focus on relaxing the bounded variance assumption that is common, yet stringent, for getting last iterate convergence rates. We prove the $\\widetilde{O}(1/\\sqrt{T})$ rate of convergence for the last iterate of both algorithms under componentwise convexity and smoothness, which is optimal up to log terms. Our results apply directly to graph-guided regularizers that arise in multi-task and federated learning, where the regularizer decomposes as a sum over edges of a collaboration graph.",
    "authors": [
      "Kevin Kurian Thomas Vaidyan",
      "Michael P. Friedlander",
      "Ahmet Alacaoglu"
    ],
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05489v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05489v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.05459v1",
    "title": "When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL",
    "summary": "Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\\approx$ 20\\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.",
    "authors": [
      "Jan Malte Töpperwien",
      "Aditya Mohan",
      "Marius Lindauer"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05459v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05459v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2602.05843v1",
    "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
    "summary": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
    "authors": [
      "Fangzhi Xu",
      "Hang Yan",
      "Qiushi Sun",
      "Jinyang Wu",
      "Zixian Huang",
      "Muye Huang",
      "Jingyang Gong",
      "Zichen Ding",
      "Kanzhi Cheng",
      "Yian Wang",
      "Xinyu Che",
      "Zeyi Sun",
      "Jian Zhang",
      "Zhangyue Yin",
      "Haoran Luo",
      "Xuanjing Huang",
      "Ben Kao",
      "Jun Liu",
      "Qika Lin"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05843v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05843v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05730v1",
    "title": "Depth as Prior Knowledge for Object Detection",
    "summary": "Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.",
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05730v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05730v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05707v1",
    "title": "Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift",
    "summary": "Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent \"fairness helps accuracy\" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.",
    "authors": [
      "Amir Asiaee",
      "Kaveh Aryan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05707v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05707v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05656v1",
    "title": "Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation",
    "summary": "Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.   We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.   Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.   We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.",
    "authors": [
      "Igor Santos-Grueiro"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05656v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05656v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05628v1",
    "title": "AI chatbots versus human healthcare professionals: a systematic review and meta-analysis of empathy in patient care",
    "summary": "Background: Empathy is widely recognized for improving patient outcomes, including reduced pain and anxiety and improved satisfaction, and its absence can cause harm. Meanwhile, use of artificial intelligence (AI)-based chatbots in healthcare is rapidly expanding, with one in five general practitioners using generative AI to assist with tasks such as writing letters. Some studies suggest AI chatbots can outperform human healthcare professionals (HCPs) in empathy, though findings are mixed and lack synthesis.   Sources of data: We searched multiple databases for studies comparing AI chatbots using large language models with human HCPs on empathy measures. We assessed risk of bias with ROBINS-I and synthesized findings using random-effects meta-analysis where feasible, whilst avoiding double counting.   Areas of agreement: We identified 15 studies (2023-2024). Thirteen studies reported statistically significantly higher empathy ratings for AI, with only two studies situated in dermatology favouring human responses. Of the 15 studies, 13 provided extractable data and were suitable for pooling. Meta-analysis of those 13 studies, all utilising ChatGPT-3.5/4, showed a standardized mean difference of 0.87 (95% CI, 0.54-1.20) favouring AI (P < .00001), roughly equivalent to a two-point increase on a 10-point scale.   Areas of controversy: Studies relied on text-based assessments that overlook non-verbal cues and evaluated empathy through proxy raters.   Growing points: Our findings indicate that, in text-only scenarios, AI chatbots are frequently perceived as more empathic than human HCPs.   Areas timely for developing research: Future research should validate these findings with direct patient evaluations and assess whether emerging voice-enabled AI systems can deliver similar empathic advantages.",
    "authors": [
      "Alastair Howcroft",
      "Amber Bennett-Weston",
      "Ahmad Khan",
      "Joseff Griffiths",
      "Simon Gay",
      "Jeremy Howick"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05628v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05628v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05600v1",
    "title": "On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature",
    "summary": "Stochastic Gradient Descent (SGD) introduces anisotropic noise that is correlated with the local curvature of the loss landscape, thereby biasing optimization toward flat minima. Prior work often assumes an equivalence between the Fisher Information Matrix and the Hessian for negative log-likelihood losses, leading to the claim that the SGD noise covariance $\\mathbf{C}$ is proportional to the Hessian $\\mathbf{H}$. We show that this assumption holds only under restrictive conditions that are typically violated in deep neural networks. Using the recently discovered Activity--Weight Duality, we find a more general relationship agnostic to the specific loss formulation, showing that $\\mathbf{C} \\propto \\mathbb{E}_p[\\mathbf{h}_p^2]$, where $\\mathbf{h}_p$ denotes the per-sample Hessian with $\\mathbf{H} = \\mathbb{E}_p[\\mathbf{h}_p]$. As a consequence, $\\mathbf{C}$ and $\\mathbf{H}$ commute approximately rather than coincide exactly, and their diagonal elements follow an approximate power-law relation $C_{ii} \\propto H_{ii}^γ$ with a theoretically bounded exponent $1 \\leq γ\\leq 2$, determined by per-sample Hessian spectra. Experiments across datasets, architectures, and loss functions validate these bounds, providing a unified characterization of the noise-curvature relationship in deep learning.",
    "authors": [
      "Yikuan Zhang",
      "Ning Yang",
      "Yuhai Tu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05600v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05600v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05597v1",
    "title": "Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents",
    "summary": "Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.",
    "authors": [
      "Stephen Pilli",
      "Vivek Nallur"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05597v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05597v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05531v1",
    "title": "Solving Stochastic Variational Inequalities without the Bounded Variance Assumption",
    "summary": "We analyze algorithms for solving stochastic variational inequalities (VI) without the bounded variance or bounded domain assumptions, where our main focus is min-max optimization with possibly unbounded constraint sets. We focus on two classes of problems: monotone VIs; and structured nonmonotone VIs that admit a solution to the weak Minty VI. The latter assumption allows us to solve structured nonconvex-nonconcave min-max problems. For both classes of VIs, to make the expected residual norm less than $\\varepsilon$, we show an oracle complexity of $\\widetilde{O}(\\varepsilon^{-4})$, which is the best-known for constrained VIs. In our setting, this complexity had been obtained with the bounded variance assumption in the literature, which is not even satisfied for bilinear min-max problems with an unbounded domain. We obtain this complexity for stochastic oracles whose variance can grow as fast as the squared norm of the optimization variable.",
    "authors": [
      "Ahmet Alacaoglu",
      "Jun-Hyun Kim"
    ],
    "categories": [
      "math.OC",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05531v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05531v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05407v1",
    "title": "H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration",
    "summary": "Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.",
    "authors": [
      "Jun-Min Lee",
      "Meong Hi Son",
      "Edward Choi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05407v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05407v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2602.05829v1",
    "title": "Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning",
    "summary": "Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.",
    "authors": [
      "Yudi Shi",
      "Shangzhe Di",
      "Qirui Chen",
      "Qinian Wang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Weidi Xie"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05829v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05829v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2602.05574v1",
    "title": "A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features",
    "summary": "Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.",
    "authors": [
      "Mengyu Li",
      "Ingibjörg Kristjánsdóttir",
      "Thilo van Eimeren",
      "Kathrin Giehl",
      "Lotta M. Ellingsen",
      "the ASAP Neuroimaging Initiative"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05574v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05574v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2602.05559v1",
    "title": "Piecewise Deterministic Markov Processes for Bayesian Inference of PDE Coefficients",
    "summary": "We develop a general framework for piecewise deterministic Markov process (PDMP) samplers that enables efficient Bayesian inference in non-linear inverse problems with expensive likelihoods. The key ingredient is a surrogate-assisted thinning scheme in which a surrogate model provides a proposal event rate and a robust correction mechanism enforces an upper bound on the true rate by dynamically adjusting an additive offset whenever violations are detected. This construction is agnostic to the choice of surrogate and PDMP, and we demonstrate it for the Zig-Zag sampler and the Bouncy particle sampler with constant, Laplace, and Gaussian process (GP) surrogates, including gradient-informed and adaptively refined GP variants. As a representative application, we consider Bayesian inference of a spatially varying Young's modulus in a one-dimensional linear elasticity problem. Across dimensions, PDMP samplers equipped with GP-based surrogates achieve substantially higher accuracy and effective sample size per forward model evaluation than Random Walk Metropolis algorithm and the No-U-Turn sampler. The Bouncy particle sampler exhibits the most favorable overall efficiency and scaling, illustrating the potential of the proposed PDMP framework beyond this particular setting.",
    "authors": [
      "Leon Riccius",
      "Iuri B. C. M. Rocha",
      "Joris Bierkens",
      "Hanne Kekkonen",
      "Frans P. van der Meer"
    ],
    "categories": [
      "stat.CO",
      "math.ST",
      "stat.AP",
      "stat.ML"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05559v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05559v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2602.05862v1",
    "title": "Distribution-free two-sample testing with blurred total variation distance",
    "summary": "Two-sample testing, where we aim to determine whether two distributions are equal or not equal based on samples from each one, is challenging if we cannot place assumptions on the properties of the two distributions. In particular, certifying equality of distributions, or even providing a tight upper bound on the total variation (TV) distance between the distributions, is impossible to achieve in a distribution-free regime. In this work, we examine the blurred TV distance, a relaxation of TV distance that enables us to perform inference without assumptions on the distributions. We provide theoretical guarantees for distribution-free upper and lower bounds on the blurred TV distance, and examine its properties in high dimensions.",
    "authors": [
      "Rohan Hore",
      "Rina Foygel Barber"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05862v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05862v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2602.05668v1",
    "title": "Stable but Wrong: When More Data Degrades Scientific Conclusions",
    "summary": "Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.",
    "authors": [
      "Zhipeng Zhang",
      "Kai Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05668v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05668v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2602.05588v1",
    "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
    "summary": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.",
    "authors": [
      "Haojie Cheng",
      "Chang Liu",
      "Abhiram Kanneganti",
      "Mahesh Arjandas Choolani",
      "Arundhati Tushar Gosavi",
      "Eng Tat Khoo"
    ],
    "categories": [
      "cs.CV",
      "cs.ET",
      "cs.GR"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05588v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05588v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2602.05426v1",
    "title": "Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications",
    "summary": "Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.",
    "authors": [
      "Wahyu Rahmaniar",
      "Kenji Suzuki"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05426v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05426v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2602.05602v1",
    "title": "Multi-instance robust fitting for non-classical geometric models",
    "summary": "Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.",
    "authors": [
      "Zongliang Zhang",
      "Shuxiang Li",
      "Xingwang Huang",
      "Zongyue Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05602v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05602v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.05582v1",
    "title": "Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation",
    "summary": "We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).   The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.   The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.",
    "authors": [
      "Joe-Mei Feng",
      "Sheng-Wei Yu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05582v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05582v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.05466v1",
    "title": "Optimization is Not Enough: Why Problem Formulation Deserves Equal Attention",
    "summary": "Black-box optimization is increasingly used in engineering design problems where simulation-based evaluations are costly and gradients are unavailable. In this context, the optimization community has largely analyzed algorithm performance in context-free setups, while not enough attention has been devoted to how problem formulation and domain knowledge may affect the optimization outcomes. We address this gap through a case study in the topology optimization of laminated composite structures, formulated as a black-box optimization problem. Specifically, we consider the design of a cantilever beam under a volume constraint, intending to minimize compliance while optimizing both the structural topology and fiber orientations. To assess the impact of problem formulation, we explicitly separate topology and material design variables and compare two strategies: a concurrent approach that optimizes all variables simultaneously without leveraging physical insight, and a sequential approach that optimizes variables of the same nature in stages. Our results show that context-agnostic strategies consistently lead to suboptimal or non-physical designs. In contrast, the sequential strategy yields better-performing and more interpretable solutions. These findings underscore the value of incorporating, when available, domain knowledge into the optimization process and motivate the development of new black-box benchmarks that reward physically informed and context-aware optimization strategies.",
    "authors": [
      "Iván Olarte Rodríguez",
      "Gokhan Serhat",
      "Mariusz Bujny",
      "Fabian Duddeck",
      "Thomas Bäck",
      "Elena Raponi"
    ],
    "categories": [
      "cs.NE",
      "cs.CE"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05466v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05466v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2602.05538v1",
    "title": "A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments",
    "summary": "Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.",
    "authors": [
      "Malaz Tamim",
      "Andrea Matic-Flierl",
      "Karsten Roscher"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05538v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05538v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2602.05435v1",
    "title": "Stable Velocity: A Variance Perspective on Flow Matching",
    "summary": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
    "authors": [
      "Donglin Yang",
      "Yongxing Zhang",
      "Xin Yu",
      "Liang Hou",
      "Xin Tao",
      "Pengfei Wan",
      "Xiaojuan Qi",
      "Renjie Liao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05435v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05435v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2602.05738v1",
    "title": "Disc-Centric Contrastive Learning for Lumbar Spine Severity Grading",
    "summary": "This work examines a disc-centric approach for automated severity grading of lumbar spinal stenosis from sagittal T2-weighted MRI. The method combines contrastive pretraining with disc-level fine-tuning, using a single anatomically localized region of interest per intervertebral disc. Contrastive learning is employed to help the model focus on meaningful disc features and reduce sensitivity to irrelevant differences in image appearance. The framework includes an auxiliary regression task for disc localization and applies weighted focal loss to address class imbalance. Experiments demonstrate a 78.1% balanced accuracy and a reduced severe-to-normal misclassification rate of 2.13% compared with supervised training from scratch. Detecting discs with moderate severity can still be challenging, but focusing on disc-level features provides a practical way to assess the lumbar spinal stenosis.",
    "authors": [
      "Sajjan Acharya",
      "Pralisha Kansakar"
    ],
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05738v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05738v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2602.05551v1",
    "title": "FastVMT: Eliminating Redundancy in Video Motion Transfer",
    "summary": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.",
    "authors": [
      "Yue Ma",
      "Zhikai Wang",
      "Tianhao Ren",
      "Mingzhe Zheng",
      "Hongyu Liu",
      "Jiayi Guo",
      "Mark Fong",
      "Yuxuan Xue",
      "Zixiang Zhao",
      "Konrad Schindler",
      "Qifeng Chen",
      "Linfeng Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05551v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05551v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2602.05440v1",
    "title": "Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations",
    "summary": "In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.",
    "authors": [
      "Natascha Jeziorski",
      "Petra Gospodnetić",
      "Claudia Redenbach"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05440v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05440v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.46
  },
  {
    "arxiv_id": "2602.05414v1",
    "title": "TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions",
    "summary": "Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.",
    "authors": [
      "Ngoc Doan-Minh Huynh",
      "Duong Nguyen-Ngoc Tran",
      "Long Hoang Pham",
      "Tai Huu-Phuong Tran",
      "Hyung-Joon Jeon",
      "Huy-Hung Nguyen",
      "Duong Khac Vu",
      "Hyung-Min Jeon",
      "Son Hong Phan",
      "Quoc Pham-Nam Ho",
      "Chi Dai Tran",
      "Trinh Le Ba Khanh",
      "Jae Wook Jeon"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05414v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05414v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2602.05675v1",
    "title": "Variable Search Stepsize for Randomized Local Search in Multi-Objective Combinatorial Optimization",
    "summary": "Over the past two decades, research in evolutionary multi-objective optimization has predominantly focused on continuous domains, with comparatively limited attention given to multi-objective combinatorial optimization problems (MOCOPs). Combinatorial problems differ significantly from continuous ones in terms of problem structure and landscape. Recent studies have shown that on MOCOPs multi-objective evolutionary algorithms (MOEAs) can even be outperformed by simple randomised local search. Starting with a randomly sampled solution in search space, randomised local search iteratively draws a random solution (from an archive) to perform local variation within its neighbourhood. However, in most existing methods, the local variation relies on a fixed neighbourhood, which limits exploration and makes the search easy to get trapped in local optima. In this paper, we present a simple yet effective local search method, called variable stepsize randomized local search (VS-RLS), which adjusts the stepsize during the search. VS-RLS transitions gradually from a broad, exploratory search in the early phases to a more focused, fine-grained search as the search progresses. We demonstrate the effectiveness and generalizability of VS-RLS through extensive evaluations against local search and MOEAs methods on diverse MOCOPs.",
    "authors": [
      "Xuepeng Ren",
      "Maocai Wang",
      "Guangming Dai",
      "Zimin Liang",
      "Qianrong Liu",
      "Shengxiang Yang",
      "Miqing Li"
    ],
    "categories": [
      "cs.NE"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05675v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05675v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2602.05577v1",
    "title": "LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization",
    "summary": "Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.",
    "authors": [
      "Shiyu Wu",
      "Shuyan Li",
      "Jing Li",
      "Jing Liu",
      "Yequan Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05577v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05577v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2602.05527v1",
    "title": "Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains",
    "summary": "Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \\pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \\pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.",
    "authors": [
      "Ben Isselmann",
      "Dilara Göksu",
      "Andreas Weinmann"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05527v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05527v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2602.05534v1",
    "title": "SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation",
    "summary": "Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.",
    "authors": [
      "Youngwoo Shin",
      "Jiwan Hur",
      "Junmo Kim"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-02-05",
    "url": "https://arxiv.org/abs/2602.05534v1",
    "pdf_url": "https://arxiv.org/pdf/2602.05534v1.pdf",
    "date": "2026-02-06",
    "source": "arxiv",
    "research_score": 0.42
  }
]