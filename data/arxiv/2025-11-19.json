[
  {
    "arxiv_id": "2511.13655v1",
    "title": "OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation",
    "summary": "Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\\href{https://github.com/allenai/olmoearth_pretrain}{\\text{https://github.com/allenai/olmoearth_pretrain}}$.",
    "authors": [
      "Henry Herzog",
      "Favyen Bastani",
      "Yawen Zhang",
      "Gabriel Tseng",
      "Joseph Redmon",
      "Hadrien Sablon",
      "Ryan Park",
      "Jacob Morrison",
      "Alexandra Buraczynski",
      "Karen Farley",
      "Joshua Hansen",
      "Andrew Howe",
      "Patrick Alan Johnson",
      "Mark Otterlee",
      "Ted Schmitt",
      "Hunter Pitelka",
      "Stephen Daspit",
      "Rachel Ratner",
      "Christopher Wilhelm",
      "Sebastian Wood",
      "Mike Jacobi",
      "Hannah Kerner",
      "Evan Shelhamer",
      "Ali Farhadi",
      "Ranjay Krishna",
      "Patrick Beukema"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13655v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13655v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.91
  },
  {
    "arxiv_id": "2511.13219v1",
    "title": "FoleyBench: A Benchmark For Video-to-Audio Models",
    "summary": "Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench",
    "authors": [
      "Satvik Dixit",
      "Koichi Saito",
      "Zhi Zhong",
      "Yuki Mitsufuji",
      "Chris Donahue"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13219v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13219v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2511.13621v1",
    "title": "Alpha Divergence Losses for Biometric Verification",
    "summary": "Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.",
    "authors": [
      "Dimitrios Koutsianos",
      "Ladislav Mosner",
      "Yannis Panagakis",
      "Themos Stafylakis"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13621v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13621v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.83
  },
  {
    "arxiv_id": "2511.13548v1",
    "title": "ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models",
    "summary": "The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \\textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.",
    "authors": [
      "Siyang Cheng",
      "Gaotian Liu",
      "Rui Mei",
      "Yilin Wang",
      "Kejia Zhang",
      "Kaishuo Wei",
      "Yuqi Yu",
      "Weiping Wen",
      "Xiaojie Wu",
      "Junhua Liu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13548v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13548v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.13391v1",
    "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning",
    "summary": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.",
    "authors": [
      "Chengdong Ma",
      "Théo Tao Zhaowei",
      "Pengyu Li",
      "Minghao Liu",
      "Haojun Chen",
      "Zihao Mao",
      "Yuan Cheng",
      "Yuan Qi",
      "Yaodong Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13391v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13391v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.13193v1",
    "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction",
    "summary": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.",
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Chengpei Tang",
      "Jian Wang",
      "Keze Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13193v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13193v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2511.13198v1",
    "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer",
    "summary": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.",
    "authors": [
      "Zhixin Ou",
      "Peng Liang",
      "Jianchen Han",
      "Baihui Liu",
      "Linbo Qiao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13198v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13198v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2511.13444v1",
    "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes",
    "summary": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.",
    "authors": [
      "Zhipeng Ma",
      "Bo Nørregaard Jørgensen",
      "Zheng Grace Ma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13444v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13444v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2511.13335v1",
    "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects",
    "summary": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.",
    "authors": [
      "Maram Alharbi",
      "Salmane Chafik",
      "Saad Ezzini",
      "Ruslan Mitkov",
      "Tharindu Ranasinghe",
      "Hansi Hettiarachchi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13335v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13335v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.13319v1",
    "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
    "summary": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.   In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
    "authors": [
      "Chelsea McMurray",
      "Hayder Tirmazi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13319v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13319v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.13399v1",
    "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
    "summary": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
    "authors": [
      "Yuchen Bao",
      "Yiting Wang",
      "Wenjian Huang",
      "Haowei Wang",
      "Shen Chen",
      "Taiping Yao",
      "Shouhong Ding",
      "Jianguo Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13399v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13399v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.13640v1",
    "title": "Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures",
    "summary": "The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.",
    "authors": [
      "Haohui Wang",
      "Jingyuan Qi",
      "Jianpeng Chen",
      "Jun Wu",
      "Lifu Huang",
      "Lecheng Zheng",
      "Kevin Choi",
      "Balaji Veeramani",
      "Edward Bowen",
      "Alison Hu",
      "Tyler Cody",
      "Dawei Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13640v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13640v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.13356v1",
    "title": "Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping",
    "summary": "Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .",
    "authors": [
      "Lei Wang",
      "Yulong Tian",
      "Hao Han",
      "Fengyuan Xu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13356v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13356v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.13676v1",
    "title": "T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization",
    "summary": "Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.",
    "authors": [
      "Hyunwoo Oh",
      "KyungIn Nam",
      "Rajat Bhattacharjya",
      "Hanning Chen",
      "Tamoghno Das",
      "Sanggeon Yun",
      "Suyeon Jang",
      "Andrew Ding",
      "Nikil Dutt",
      "Mohsen Imani"
    ],
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13676v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13676v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.13351v1",
    "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning",
    "summary": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.",
    "authors": [
      "Xinlan Wu",
      "Bin Zhu",
      "Feng Han",
      "Pengkun Jiao",
      "Jingjing Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13351v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13351v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.13254v1",
    "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
    "authors": [
      "Shalini Maiti",
      "Amar Budhiraja",
      "Bhavul Gauri",
      "Gaurav Chaurasia",
      "Anton Protopopov",
      "Alexis Audran-Reiss",
      "Michael Slater",
      "Despoina Magka",
      "Tatiana Shavrina",
      "Roberta Raileanu",
      "Yoram Bachrach"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13254v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13254v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.13561v1",
    "title": "RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise",
    "summary": "Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.",
    "authors": [
      "Shihao Dong",
      "Yue Liu",
      "Xiaotong Zhou",
      "Yuhui Zheng",
      "Huiying Xu",
      "Xinzhong Zhu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13561v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13561v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.71
  },
  {
    "arxiv_id": "2511.13593v1",
    "title": "Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents",
    "summary": "Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.",
    "authors": [
      "Piaohong Wang",
      "Motong Tian",
      "Jiaxian Li",
      "Yuan Liang",
      "Yuqing Wang",
      "Qianben Chen",
      "Tiannan Wang",
      "Zhicong Lu",
      "Jiawei Ma",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13593v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13593v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13541v1",
    "title": "Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries",
    "summary": "A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.",
    "authors": [
      "Yue Hou",
      "Ruomei Liu",
      "Yingke Su",
      "Junran Wu",
      "Ke Xu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13541v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13541v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13539v1",
    "title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse",
    "summary": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.",
    "authors": [
      "Yuanchao Wang",
      "Tian Qin",
      "Eduardo Valle",
      "Bruno Abrahao"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13539v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13539v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13478v1",
    "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
    "summary": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
    "authors": [
      "Adam Hazimeh",
      "Ke Wang",
      "Mark Collier",
      "Gilles Baechler",
      "Efi Kokiopoulou",
      "Pascal Frossard"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13478v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13478v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13442v1",
    "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
    "summary": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
    "authors": [
      "Rui Zuo",
      "Qinyue Tong",
      "Zhe-Ming Lu",
      "Ziqian Lu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13442v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13442v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13373v1",
    "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
    "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
    "authors": [
      "Prakrit Timilsina",
      "Anuj Nepal",
      "Rajan Kadel",
      "Robin Doss"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13373v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13373v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13322v1",
    "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
    "summary": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
    "authors": [
      "Senne Deproost",
      "Dennis Steckelmacher",
      "Ann Nowé"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13322v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13322v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13234v1",
    "title": "MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing",
    "summary": "Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.",
    "authors": [
      "Boris Kriuk"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13234v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13234v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13701v1",
    "title": "Learning stochasticity: a nonparametric framework for intrinsic noise estimation",
    "summary": "Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.",
    "authors": [
      "Gianluigi Pillonetto",
      "Alberto Giaretta",
      "Mauro Bisiacco"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13701v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13701v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13545v1",
    "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
    "summary": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
    "authors": [
      "Md. Iqbal Hossain",
      "Afia Sajeeda",
      "Neeresh Kumar Perla",
      "Ming Shao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13545v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13545v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13394v1",
    "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo",
    "summary": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",
    "authors": [
      "Vasilis Gkolemis",
      "Christos Diou",
      "Michael Gutmann"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13394v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13394v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13223v1",
    "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
    "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.",
    "authors": [
      "Yuxiang Zhang",
      "Zhengxu Yu",
      "Weihang Pan",
      "Zhongming Jin",
      "Qiang Fu",
      "Deng Cai",
      "Binbin Lin",
      "Jieping Ye"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13223v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13223v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13702v1",
    "title": "ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification",
    "summary": "Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.",
    "authors": [
      "Luyao Niu",
      "Nuoxian Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13702v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13702v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13612v1",
    "title": "P1: Mastering Physics Olympiads with Reinforcement Learning",
    "summary": "Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.",
    "authors": [
      "Jiacheng Chen",
      "Qianjia Cheng",
      "Fangchen Yu",
      "Haiyuan Wan",
      "Yuchen Zhang",
      "Shenghe Zheng",
      "Junchi Yao",
      "Qingyang Zhang",
      "Haonan He",
      "Yun Luo",
      "Yufeng Zhao",
      "Futing Wang",
      "Li Sheng",
      "Chengxing Xie",
      "Yuxin Zuo",
      "Yizhuo Li",
      "Wenxauan Zeng",
      "Yulun Wu",
      "Rui Huang",
      "Dongzhan Zhou",
      "Kai Chen",
      "Yu Qiao",
      "Lei Bai",
      "Yu Cheng",
      "Ning Ding",
      "Bowen Zhou",
      "Peng Ye",
      "Ganqu Cui"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13612v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13612v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13389v1",
    "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
    "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
    "authors": [
      "Zhipeng Ma",
      "Bo Nørregaard Jørgensen",
      "Zheng Grace Ma"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13389v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13389v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13293v1",
    "title": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval",
    "summary": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.",
    "authors": [
      "Chuang Zhao",
      "Hui Tang",
      "Hongke Zhao",
      "Xiaofang Zhou",
      "Xiaomeng Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13293v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13293v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13719v1",
    "title": "Scaling Spatial Intelligence with Multimodal Foundation Models",
    "summary": "Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.",
    "authors": [
      "Zhongang Cai",
      "Ruisi Wang",
      "Chenyang Gu",
      "Fanyi Pu",
      "Junxiang Xu",
      "Yubo Wang",
      "Wanqi Yin",
      "Zhitao Yang",
      "Chen Wei",
      "Qingping Sun",
      "Tongxi Zhou",
      "Jiaqi Li",
      "Hui En Pang",
      "Oscar Qian",
      "Yukun Wei",
      "Zhiqian Lin",
      "Xuanke Shi",
      "Kewang Deng",
      "Xiaoyang Han",
      "Zukai Chen",
      "Xiangyu Fan",
      "Hanming Deng",
      "Lewei Lu",
      "Liang Pan",
      "Bo Li",
      "Ziwei Liu",
      "Quan Wang",
      "Dahua Lin",
      "Lei Yang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13719v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13719v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13646v1",
    "title": "Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?",
    "summary": "Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.",
    "authors": [
      "Chunqiu Steven Xia",
      "Zhe Wang",
      "Yan Yang",
      "Yuxiang Wei",
      "Lingming Zhang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13646v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13646v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13615v1",
    "title": "Tissue Aware Nuclei Detection and Classification Model for Histopathology Images",
    "summary": "Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.",
    "authors": [
      "Kesi Xu",
      "Eleni Chiou",
      "Ali Varamesh",
      "Laura Acqualagna",
      "Nasir Rajpoot"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13615v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13615v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13575v1",
    "title": "Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification",
    "summary": "Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.",
    "authors": [
      "Linhan Zhou",
      "Shuang Li",
      "Neng Dong",
      "Yonghang Tai",
      "Yafei Zhang",
      "Huafeng Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13575v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13575v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13540v1",
    "title": "Fairness-Aware Graph Representation Learning with Limited Demographic Information",
    "summary": "Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.",
    "authors": [
      "Zichong Wang",
      "Zhipeng Yin",
      "Liping Yang",
      "Jun Zhuang",
      "Rui Yu",
      "Qingzhao Kong",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13540v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13540v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13290v1",
    "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment",
    "summary": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.",
    "authors": [
      "Jea Kwon",
      "Luiz Felipe Vecchietti",
      "Sungwon Park",
      "Meeyoung Cha"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13290v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13290v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13237v1",
    "title": "Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification",
    "summary": "Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\\geq10\\%$ higher confidence while improving sparsity in $\\geq40\\%$.",
    "authors": [
      "Alan G. Paredes Cetina",
      "Kaouther Benguessoum",
      "Raoni Lourenço",
      "Sylvain Kubler"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13237v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13237v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13221v1",
    "title": "Likelihood-guided Regularization in Attention Based Models",
    "summary": "The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.",
    "authors": [
      "Mohamed Salem",
      "Inyoung Kim"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13221v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13221v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13685v1",
    "title": "Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers",
    "summary": "In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.",
    "authors": [
      "Disha Varshney",
      "Samarth Garg",
      "Sarthak Tyagi",
      "Deeksha Varshney",
      "Nayan Deep",
      "Asif Ekbal"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13685v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13685v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13465v1",
    "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate",
    "summary": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.",
    "authors": [
      "Meng Zhu",
      "Quan Xiao",
      "Weidong Min"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13465v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13465v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13306v1",
    "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving",
    "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.",
    "authors": [
      "Bowen Ye",
      "Bin Zhang",
      "Hang Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13306v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13306v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13283v1",
    "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing",
    "summary": "Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.",
    "authors": [
      "Jongha Kim",
      "Minseong Bae",
      "Sanghyeok Lee",
      "Jinsung Yoon",
      "Hyunwoo J. Kim"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13283v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13283v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13208v1",
    "title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer",
    "summary": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \\textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet",
    "authors": [
      "Yonghui Yu",
      "Jiahang Cai",
      "Xun Wang",
      "Wenwu Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13208v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13208v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13608v1",
    "title": "A Gentle Introduction to Conformal Time Series Forecasting",
    "summary": "Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.",
    "authors": [
      "M. Stocker",
      "W. Małgorzewicz",
      "M. Fontana",
      "S. Ben Taieb"
    ],
    "categories": [
      "stat.ME",
      "cs.LG",
      "econ.EM"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13608v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13608v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13590v1",
    "title": "Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation",
    "summary": "Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.",
    "authors": [
      "Hao Wang",
      "Yuanfeng Song",
      "Xiaoming Yin",
      "Xing Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13590v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13590v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13571v1",
    "title": "Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation",
    "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.",
    "authors": [
      "Ziyang Huang",
      "Jiagang Chen",
      "Jin Liu",
      "Shunping Ji"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13571v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13571v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13418v1",
    "title": "Exploring Multi-Table Retrieval Through Iterative Search",
    "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
    "authors": [
      "Allaa Boutaleb",
      "Bernd Amann",
      "Rafael Angarita",
      "Hubert Naacke"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13418v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13418v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13408v1",
    "title": "Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility",
    "summary": "Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus\" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.",
    "authors": [
      "Zhenyu Chen",
      "Yuguo Shao",
      "Zhengwei Liu",
      "Zhaohui Wei"
    ],
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.IT",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13408v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13408v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13353v1",
    "title": "Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images",
    "summary": "Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.",
    "authors": [
      "Lucas Gabriel Telesco",
      "Danila Nejamkin",
      "Estefanía Mata",
      "Francisco Filizzola",
      "Kevin Wignall",
      "Lucía Franco Troilo",
      "María de los Angeles Cenoz",
      "Melissa Thompson",
      "Mercedes Leguía",
      "Ignacio Larrabide",
      "José Ignacio Orlando"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13353v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13353v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13288v1",
    "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
    "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
    "authors": [
      "Haoyang Hong",
      "Jiajun Yin",
      "Yuan Wang",
      "Jingnan Liu",
      "Zhe Chen",
      "Ailing Yu",
      "Ji Li",
      "Zhiling Ye",
      "Hansong Xiao",
      "Yefei Chen",
      "Hualei Zhou",
      "Yun Yue",
      "Minghui Yang",
      "Chunxiao Guo",
      "Junwei Liu",
      "Peng Wei",
      "Jinjie Gu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13288v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13288v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13714v1",
    "title": "UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity",
    "summary": "The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\\text{NoC}_{90}$ (5.69 $\\rightarrow$ 4.75), 1-IoU (58.0 $\\rightarrow$ 73.1), and $\\text{AR}_{1000}$ (49.6 $\\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.",
    "authors": [
      "Junwei Yu",
      "Trevor Darrell",
      "XuDong Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13714v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13714v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13684v1",
    "title": "Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting",
    "summary": "We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.",
    "authors": [
      "Jiangnan Ye",
      "Jiedong Zhuang",
      "Lianrui Mu",
      "Wenjie Zheng",
      "Jiaqi Hu",
      "Xingze Zou",
      "Jing Wang",
      "Haoji Hu"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13684v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13684v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13625v1",
    "title": "Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization",
    "summary": "Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.",
    "authors": [
      "Kaichi Irie",
      "Shuhei Watanabe",
      "Masaki Onishi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13625v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13625v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13592v1",
    "title": "Power Homotopy for Zeroth-Order Non-Convex Optimizations",
    "summary": "We introduce GS-PowerHP, a novel zeroth-order method for non-convex optimization problems of the form $\\max_{x \\in \\mathbb{R}^d} f(x)$. Our approach leverages two key components: a power-transformed Gaussian-smoothed surrogate $F_{N,σ}(μ) = \\mathbb{E}_{x\\sim\\mathcal{N}(μ,σ^2 I_d)}[e^{N f(x)}]$ whose stationary points cluster near the global maximizer $x^*$ of $f$ for sufficiently large $N$, and an incrementally decaying $σ$ for enhanced data efficiency. Under mild assumptions, we prove convergence in expectation to a small neighborhood of $x^*$ with the iteration complexity of $O(d^2 \\varepsilon^{-2})$. Empirical results show our approach consistently ranks among the top three across a suite of competing algorithms. Its robustness is underscored by the final experiment on a substantially high-dimensional problem ($d=150,528$), where it achieved first place on least-likely targeted black-box attacks against images from ImageNet, surpassing all competing methods.",
    "authors": [
      "Chen Xu"
    ],
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13592v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13592v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13510v1",
    "title": "Naga: Vedic Encoding for Deep State Space Models",
    "summary": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.",
    "authors": [
      "Melanie Schaller",
      "Nick Janssen",
      "Bodo Rosenhahn"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13510v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13510v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13469v1",
    "title": "GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction",
    "summary": "Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.",
    "authors": [
      "Shiyuan Luo",
      "Chonghao Qiu",
      "Runlong Yu",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13469v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13469v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13467v1",
    "title": "Non-Linear Scoring Model for Translation Quality Evaluation",
    "summary": "Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.   Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.   Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model   E(x) = a * ln(1 + b * x), a, b > 0,   anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.   The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.",
    "authors": [
      "Serge Gladkoff",
      "Lifeng Han",
      "Katerina Gasova"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13467v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13467v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13419v1",
    "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction",
    "summary": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data",
    "authors": [
      "Shaheen Mohammed Saleh Ahmed",
      "Hakan Hakan Guneyli"
    ],
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13419v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13419v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13414v1",
    "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation",
    "summary": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.",
    "authors": [
      "Hanwen Hu",
      "Zimo Wen",
      "Shiyou Qian",
      "Jian Co"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13414v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13414v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13315v1",
    "title": "Computer Vision based group activity detection and action spotting",
    "summary": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.",
    "authors": [
      "Narthana Sivalingam",
      "Santhirarajah Sivasthigan",
      "Thamayanthi Mahendranathan",
      "G. M. R. I. Godaliyadda",
      "M. P. B. Ekanayake",
      "H. M. V. R. Herath"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13315v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13315v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13282v1",
    "title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space",
    "summary": "Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.",
    "authors": [
      "Kaiwen Wang",
      "Kaili Zheng",
      "Yiming Shi",
      "Chenyi Guo",
      "Ji Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13282v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13282v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13274v1",
    "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators",
    "summary": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.   We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.",
    "authors": [
      "Taras Sereda",
      "Tom St. John",
      "Burak Bartan",
      "Natalie Serrino",
      "Sachin Katti",
      "Zain Asgar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.PF",
      "cs.SE"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13274v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13274v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13679v1",
    "title": "QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention",
    "summary": "Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.",
    "authors": [
      "Hyunwoo Oh",
      "Hanning Chen",
      "Sanggeon Yun",
      "Yang Ni",
      "Wenjun Huang",
      "Tamoghno Das",
      "Suyeon Jang",
      "Mohsen Imani"
    ],
    "categories": [
      "cs.AR",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13679v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13679v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13598v1",
    "title": "Robust Client-Server Watermarking for Split Federated Learning",
    "summary": "Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\\%$ watermark detection rate ($p-value \\lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.",
    "authors": [
      "Jiaxiong Tang",
      "Zhengchunmin Dai",
      "Liantao Wu",
      "Peng Sun",
      "Honglong Chen",
      "Zhenfu Cao"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13598v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13598v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13587v1",
    "title": "VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping",
    "summary": "Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its \"draft one step, then verify one step\" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.",
    "authors": [
      "Haotian Dong",
      "Ye Li",
      "Rongwei Lu",
      "Chen Tang",
      "Shu-Tao Xia",
      "Zhi Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13587v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13587v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13514v1",
    "title": "A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data",
    "summary": "Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box\" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.",
    "authors": [
      "Pragatheeswaran Vipulananthan",
      "Kamal Premaratne",
      "Dilip Sarkar",
      "Manohar N. Murthi"
    ],
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13514v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13514v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13505v1",
    "title": "Applying Large Language Models to Characterize Public Narratives",
    "summary": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.",
    "authors": [
      "Elinor Poole-Dayan",
      "Daniel T Kessler",
      "Hannah Chiou",
      "Margaret Hughes",
      "Emily S Lin",
      "Marshall Ganz",
      "Deb Roy"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13505v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13505v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13410v1",
    "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction",
    "summary": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.",
    "authors": [
      "Zhaopei Huang",
      "Qifeng Dai",
      "Guozheng Wu",
      "Xiaopeng Wu",
      "Kehan Chen",
      "Chuan Yu",
      "Xubin Li",
      "Tiezheng Ge",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13410v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13410v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13365v1",
    "title": "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference",
    "summary": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.",
    "authors": [
      "Ruijun Deng",
      "Zhihui Lu",
      "Qiang Duan"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13365v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13365v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13338v1",
    "title": "Tab-PET: Graph-Based Positional Encodings for Tabular Transformers",
    "summary": "Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.",
    "authors": [
      "Yunze Leng",
      "Rohan Ghosh",
      "Mehul Motani"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13338v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13338v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13309v1",
    "title": "DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving",
    "summary": "The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.",
    "authors": [
      "Kaiwen Cai",
      "Xinze Liu",
      "Xia Zhou",
      "Hengtong Hu",
      "Jie Xiang",
      "Luyao Zhang",
      "Xueyang Zhang",
      "Kun Zhan",
      "Yifei Zhan",
      "Xianpeng Lang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13309v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13309v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13273v1",
    "title": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs",
    "summary": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.",
    "authors": [
      "Zhe Sun",
      "Yujun Cai",
      "Jiayu Yao",
      "Yiwei Wang"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13273v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13273v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13271v1",
    "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
    "summary": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
    "authors": [
      "Rufeng Chen",
      "Shuaishuai Jiang",
      "Jiyun Shen",
      "AJung Moon",
      "Lili Wei"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13271v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13271v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13238v1",
    "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms",
    "summary": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.",
    "authors": [
      "Patrick Parschan",
      "Charlott Jakob"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13238v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13238v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13195v1",
    "title": "Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection",
    "summary": "Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.",
    "authors": [
      "Soyul Lee",
      "Seungmin Baek",
      "Dongbo Min"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13195v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13195v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13170v1",
    "title": "THIR: Topological Histopathological Image Retrieval",
    "summary": "According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.   Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.",
    "authors": [
      "Zahra Tabatabaei",
      "Jon Sporring"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13170v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13170v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13710v1",
    "title": "From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands",
    "summary": "Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision",
    "authors": [
      "Jianglong Ye",
      "Lai Wei",
      "Guangqi Jiang",
      "Changwei Jing",
      "Xueyan Zou",
      "Xiaolong Wang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13710v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13710v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13699v1",
    "title": "Efficient Calibration for Decision Making",
    "summary": "A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.   We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.",
    "authors": [
      "Parikshit Gopalan",
      "Konstantinos Stavropoulos",
      "Kunal Talwar",
      "Pranay Tankala"
    ],
    "categories": [
      "cs.LG",
      "cs.DS",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13699v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13699v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13675v1",
    "title": "Scientific Data Compression and Super-Resolution Sampling",
    "summary": "Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.",
    "authors": [
      "Minh Vu",
      "Andrey Lokhov"
    ],
    "categories": [
      "cs.LG",
      "physics.data-an",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13675v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13675v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13527v1",
    "title": "Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images",
    "summary": "Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.",
    "authors": [
      "Ihab Asaad",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13527v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13527v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13476v1",
    "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
    "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
    "authors": [
      "Zhipeng Ma",
      "Ali Rida Bahja",
      "Andreas Burgdorf",
      "André Pomp",
      "Tobias Meisen",
      "Bo Nørregaard Jørgensen",
      "Zheng Grace Ma"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13476v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13476v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13361v1",
    "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
    "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
    "authors": [
      "Jiyang Zheng",
      "Islam Nassar",
      "Thanh Vu",
      "Xu Zhong",
      "Yang Lin",
      "Tongliang Liu",
      "Long Duong",
      "Yuan-Fang Li"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13361v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13361v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13295v1",
    "title": "Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection",
    "summary": "Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.",
    "authors": [
      "Chaowang Lan",
      "Jingxin Wu",
      "Yulong Yuan",
      "Chuxun Liu",
      "Huangyi Kang",
      "Caihua Liu"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13295v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13295v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13244v1",
    "title": "Seek and You Shall Fold",
    "summary": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.",
    "authors": [
      "Nadav Bojan Sellam",
      "Meital Bojan",
      "Paul Schanda",
      "Alex Bronstein"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13244v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13244v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13242v1",
    "title": "MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection",
    "summary": "Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.",
    "authors": [
      "Junjie Wu",
      "Guohong Fu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13242v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13242v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13222v1",
    "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation",
    "summary": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.",
    "authors": [
      "Qida Tan",
      "Hongyu Yang",
      "Wenchao Du"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13222v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13222v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13190v1",
    "title": "Video Spatial Reasoning with Object-Centric 3D Rollout",
    "summary": "Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).",
    "authors": [
      "Haoran Tang",
      "Meng Cao",
      "Ruyang Liu",
      "Xiaoxi Liang",
      "Linglong Li",
      "Ge Li",
      "Xiaodan Liang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13190v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13190v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13189v1",
    "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
    "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
    "authors": [
      "Diego Ortego",
      "Marlon Rodríguez",
      "Mario Almagro",
      "Kunal Dahiya",
      "David Jiménez",
      "Juan C. SanMiguel"
    ],
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13189v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13189v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13630v1",
    "title": "Beyond Mimicry: Preference Coherence in LLMs",
    "summary": "We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.",
    "authors": [
      "Luhan Mikaelson",
      "Derek Shiller",
      "Hayley Clatterbuck"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13630v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13630v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13524v1",
    "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
    "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
    "authors": [
      "Yuhang Peng",
      "Yizhou Pan",
      "Xinning He",
      "Jihaoyu Yang",
      "Xinyu Yin",
      "Han Wang",
      "Xiaoji Zheng",
      "Chao Gao",
      "Jiangtao Gong"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13524v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13524v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13415v1",
    "title": "Attention Grounded Enhancement for Visual Document Retrieval",
    "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
    "authors": [
      "Wanqing Cui",
      "Wei Huang",
      "Yazhi Guo",
      "Yibo Hu",
      "Meiguang Jin",
      "Junfeng Ma",
      "Keping Bi"
    ],
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13415v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13415v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13368v1",
    "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
    "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
    "authors": [
      "Kajetan Dymkiewicz",
      "Ivan Vulic",
      "Helen Yannakoudakis",
      "Eilam Shapira",
      "Roi Reichart",
      "Anna Korhonen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13368v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13368v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13261v1",
    "title": "Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges",
    "summary": "Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant",
    "authors": [
      "Junlong Li",
      "Huaiyuan Xu",
      "Sijie Cheng",
      "Kejun Wu",
      "Kim-Hui Yap",
      "Lap-Pui Chau",
      "Yi Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13261v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13261v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13243v1",
    "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing",
    "summary": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.",
    "authors": [
      "Xiaoqi Han",
      "Ru Li",
      "Ran Yi",
      "Hongye Tan",
      "Zhuomin Liang",
      "Víctor Gutiérrez-Basulto",
      "Jeff Z. Pan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13243v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13243v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13240v1",
    "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models",
    "summary": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.",
    "authors": [
      "Arka Pal",
      "Teo Kitanovski",
      "Arthur Liang",
      "Akilesh Potti",
      "Micah Goldblum"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13240v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13240v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13186v1",
    "title": "DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play",
    "summary": "Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\\times$ faster convergence and 30$\\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations",
    "authors": [
      "Akash Karthikeyan",
      "Yash Vardhan Pant"
    ],
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13186v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13186v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13183v1",
    "title": "GenTract: Generative Global Tractography",
    "summary": "Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.",
    "authors": [
      "Alec Sargood",
      "Lemuel Puglisi",
      "Elinor Thompson",
      "Mirco Musolesi",
      "Daniel C. Alexander"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13183v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13183v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13703v1",
    "title": "Generalist Foundation Models Are Not Clinical Enough for Hospital Operations",
    "summary": "Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.",
    "authors": [
      "Lavender Y. Jiang",
      "Angelica Chen",
      "Xu Han",
      "Xujin Chris Liu",
      "Radhika Dua",
      "Kevin Eaton",
      "Frederick Wolff",
      "Robert Steele",
      "Jeff Zhang",
      "Anton Alyakin",
      "Qingkai Pan",
      "Yanbing Chen",
      "Karl L. Sangwon",
      "Daniel A. Alber",
      "Jaden Stryker",
      "Jin Vivian Lee",
      "Yindalon Aphinyanaphongs",
      "Kyunghyun Cho",
      "Eric Karl Oermann"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13703v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13703v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13529v1",
    "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
    "summary": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
    "authors": [
      "Máté Gedeon",
      "Piroska Zsófia Barta",
      "Péter Mihajlik",
      "Tekla Etelka Gráczi",
      "Anna Kohári",
      "Katalin Mády"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13529v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13529v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13481v1",
    "title": "Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns",
    "summary": "Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.",
    "authors": [
      "Attapol T. Rutherford",
      "Sirisak Chueykamhang",
      "Thachaparn Bunditlurdruk",
      "Nanthicha Angsuwichitkul"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13481v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13481v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13457v1",
    "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure",
    "summary": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.",
    "authors": [
      "Bin Liu",
      "Qinghao Zhao",
      "Yuxi Zhou",
      "Zhejun Sun",
      "Kaijie Lei",
      "Deyun Zhang",
      "Shijia Geng",
      "Shenda Hong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13457v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13457v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13397v1",
    "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)",
    "summary": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.",
    "authors": [
      "Nikos Theodoridis",
      "Tim Brophy",
      "Reenu Mohandas",
      "Ganesh Sistu",
      "Fiachra Collins",
      "Anthony Scanlan",
      "Ciaran Eising"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13397v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13397v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13381v1",
    "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts",
    "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.",
    "authors": [
      "Siyu Zhu",
      "Mouxiao Bian",
      "Yue Xie",
      "Yongyu Tang",
      "Zhikang Yu",
      "Tianbin Li",
      "Pengcheng Chen",
      "Bing Han",
      "Jie Xu",
      "Xiaoyan Dong"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13381v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13381v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13378v1",
    "title": "Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models",
    "summary": "Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.",
    "authors": [
      "Carlo Teo Pedretti",
      "Davide Picca",
      "Dario Rodighiero"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13378v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13378v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13312v1",
    "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation",
    "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.",
    "authors": [
      "Jonas Bode",
      "Raphael Memmesheimer",
      "Sven Behnke"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13312v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13312v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13262v1",
    "title": "Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application",
    "summary": "This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.",
    "authors": [
      "Jack B. Coughlin",
      "Archis Joglekar",
      "Jonathan Brodrick",
      "Alexander Lavin"
    ],
    "categories": [
      "physics.comp-ph",
      "cs.CE",
      "cs.LG",
      "cs.MS",
      "physics.plasm-ph"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13262v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13262v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13168v1",
    "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration",
    "summary": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.",
    "authors": [
      "Haodong Wang",
      "Tao Zhuo",
      "Xiuwei Zhang",
      "Hanlin Yin",
      "Wencong Wu",
      "Yanning Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13168v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13168v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13715v1",
    "title": "Segment Anything Across Shots: A Method and Benchmark",
    "summary": "This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.",
    "authors": [
      "Hengrui Hu",
      "Kaining Ying",
      "Henghui Ding"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13715v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13715v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13648v1",
    "title": "PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image",
    "summary": "3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.",
    "authors": [
      "Ziang Cao",
      "Fangzhou Hong",
      "Zhaoxi Chen",
      "Liang Pan",
      "Ziwei Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13648v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13648v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13644v1",
    "title": "CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding",
    "summary": "Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.",
    "authors": [
      "Shrenik Patel",
      "Daivik Patel"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13644v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13644v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13626v1",
    "title": "CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product",
    "summary": "Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.",
    "authors": [
      "Kaiwen Xue",
      "Chenglong Li",
      "Zhonghong Ou",
      "Guoxin Zhang",
      "Kaoyan Lu",
      "Shuai Lyu",
      "Yifan Zhu",
      "Ping Zong Junpeng Ding",
      "Xinyu Liu",
      "Qunlin Chen",
      "Weiwei Qin",
      "Yiran Shen",
      "Jiayi Cen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13626v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13626v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13525v1",
    "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions",
    "summary": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.",
    "authors": [
      "Zichong Wang",
      "Zhipeng Yin",
      "Roland H. C. Yap",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13525v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13525v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13488v1",
    "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE",
    "summary": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.",
    "authors": [
      "Lipeng Wang",
      "Hongxing Fan",
      "Haohua Chen",
      "Zehuan Huang",
      "Lu Sheng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13488v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13488v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13453v1",
    "title": "Hardware optimization on Android for inference of AI models",
    "summary": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.",
    "authors": [
      "Iulius Gherasim",
      "Carlos García Sánchez"
    ],
    "categories": [
      "cs.LG",
      "cs.PF"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13453v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13453v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13387v1",
    "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model",
    "summary": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.",
    "authors": [
      "Fei Kong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13387v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13387v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13185v1",
    "title": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction",
    "summary": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.",
    "authors": [
      "Aishwarya Venkataramanan",
      "Sai Karthikeya Vemuri",
      "Adithya Ashok Chalain Valapil",
      "Joachim Denzler"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13185v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13185v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13178v1",
    "title": "Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach",
    "summary": "With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.",
    "authors": [
      "Mingxuan Tian",
      "Haochen Mu",
      "Donghong Ding",
      "Mengjiao Li",
      "Yuhan Ding",
      "Jianping Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13178v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13178v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13174v1",
    "title": "Warm-starting active-set solvers using graph neural networks",
    "summary": "Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.",
    "authors": [
      "Ella J. Schmidtobreick",
      "Daniel Arnström",
      "Paul Häusner",
      "Jens Sjölund"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13174v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13174v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13160v1",
    "title": "InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions",
    "summary": "Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque \"black boxes\". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a \"what-if\" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.",
    "authors": [
      "TC Singh",
      "Sougata Mukherjea"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13160v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13160v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13689v1",
    "title": "Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation",
    "summary": "Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.",
    "authors": [
      "Sofia Jamil",
      "Kotla Sai Charan",
      "Sriparna Saha",
      "Koustava Goswami",
      "Joseph K J"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13689v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13689v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13663v1",
    "title": "Cost-Driven Synthesis of Sound Abstract Interpreters",
    "summary": "Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.",
    "authors": [
      "Qiuhan Gu",
      "Avaljot Singh",
      "Gagandeep Singh"
    ],
    "categories": [
      "cs.PL",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13663v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13663v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13653v1",
    "title": "Weight-sparse transformers have interpretable circuits",
    "summary": "Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.",
    "authors": [
      "Leo Gao",
      "Achyuta Rajaram",
      "Jacob Coxon",
      "Soham V. Govande",
      "Bowen Baker",
      "Dan Mossing"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13653v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13653v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13341v1",
    "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains",
    "summary": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.",
    "authors": [
      "Zihe Yan",
      "Kai Luo",
      "Haoyu Yang",
      "Yang Yu",
      "Zhuosheng Zhang",
      "Guancheng Li"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13341v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13341v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13333v1",
    "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
    "summary": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
    "authors": [
      "Alexandru-Mihai Apostu",
      "Andrei Preda",
      "Alexandra Daniela Damir",
      "Diana Bolocan",
      "Radu Tudor Ionescu",
      "Ioana Croitoru",
      "Mihaela Gaman"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13333v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13333v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13285v1",
    "title": "SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design",
    "summary": "Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.",
    "authors": [
      "Yunjie Yu",
      "Jingchen Wu",
      "Junchen Zhu",
      "Chunze Lin",
      "Guibin Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13285v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13285v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13225v1",
    "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms",
    "summary": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.",
    "authors": [
      "Tyler Loakman",
      "Joseph James",
      "Chenghua Lin"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13225v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13225v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13609v1",
    "title": "AtlasMorph: Learning conditional deformable templates for brain MRI",
    "summary": "Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.",
    "authors": [
      "Marianne Rakic",
      "Andrew Hoopes",
      "S. Mazdak Abulnaga",
      "Mert R. Sabuncu",
      "John V. Guttag",
      "Adrian V. Dalca"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13609v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13609v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13565v1",
    "title": "Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction",
    "summary": "Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.",
    "authors": [
      "Jingyi Zhao",
      "Daqian Shi",
      "Zhengda Wang",
      "Xiongfeng Tang",
      "Yanguo Qin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13565v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13565v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13497v1",
    "title": "Quantum Machine Learning via Contrastive Training",
    "summary": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.",
    "authors": [
      "Liudmila A. Zhukas",
      "Vivian Ni Zhang",
      "Qiang Miao",
      "Qingfeng Wang",
      "Marko Cetina",
      "Jungsang Kim",
      "Lawrence Carin",
      "Christopher Monroe"
    ],
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13497v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13497v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13421v1",
    "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
    "summary": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
    "authors": [
      "Tingkai Yan",
      "Haodong Wen",
      "Binghui Li",
      "Kairong Luo",
      "Wenguang Chen",
      "Kaifeng Lyu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13421v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13421v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13411v1",
    "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence",
    "summary": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.",
    "authors": [
      "Przemyslaw Chojecki"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13411v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13411v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13259v1",
    "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
    "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
    "authors": [
      "Yushuo Zheng",
      "Jiangyong Ying",
      "Huiyu Duan",
      "Chunyi Li",
      "Zicheng Zhang",
      "Jing Liu",
      "Xiaohong Liu",
      "Guangtao Zhai"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13259v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13259v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13654v1",
    "title": "Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning",
    "summary": "In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.",
    "authors": [
      "Pascal Zimmer",
      "Ghassan Karame"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13654v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13654v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13607v1",
    "title": "ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement",
    "summary": "Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.",
    "authors": [
      "Xin Xu",
      "Hao Liu",
      "Wei Liu",
      "Wei Wang",
      "Jiayi Wu",
      "Kui Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13607v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13607v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13530v1",
    "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
    "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
    "authors": [
      "Vesna Poprcova",
      "Iulia Lefter",
      "Matthias Wieser",
      "Martijn Warnier",
      "Frances Brazier"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13530v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13530v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13526v1",
    "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
    "summary": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
    "authors": [
      "Zhengda Wang",
      "Daqian Shi",
      "Jingyi Zhao",
      "Xiaolei Diao",
      "Xiongfeng Tang",
      "Yanguo Qin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13526v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13526v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13507v1",
    "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
    "summary": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
    "authors": [
      "Wenyu Zhang",
      "Yao Tong",
      "Yiqiu Liu",
      "Rui Cao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13507v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13507v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13487v1",
    "title": "Systematic evaluation of time-frequency features for binaural sound source localization",
    "summary": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.",
    "authors": [
      "Davoud Shariat Panah",
      "Alessandro Ragano",
      "Dan Barry",
      "Jan Skoglund",
      "Andrew Hines"
    ],
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13487v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13487v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13431v1",
    "title": "FUSE: A Flow-based Mapping Between Shapes",
    "summary": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.",
    "authors": [
      "Lorenzo Olearo",
      "Giulio Viganò",
      "Daniele Baieri",
      "Filippo Maggioli",
      "Simone Melzi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13431v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13431v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13371v1",
    "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning",
    "summary": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.",
    "authors": [
      "Caroline Baumgartner",
      "Eleanor Spens",
      "Neil Burgess",
      "Petru Manescu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13371v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13371v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13245v1",
    "title": "Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems",
    "summary": "This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center.    In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.",
    "authors": [
      "Matt Luckcuck",
      "Maike Schwammberger",
      "Mengwei Xu"
    ],
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13245v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13245v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13211v1",
    "title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale",
    "summary": "Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.",
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Jian Wang",
      "Keze Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13211v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13211v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13207v1",
    "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection",
    "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.",
    "authors": [
      "Cheng Peng",
      "Zhenzhe Zhang",
      "Cheng Chi",
      "Xiaobao Wei",
      "Yanhao Zhang",
      "Heng Wang",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Jing Liu",
      "Shanghang Zhang"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13207v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13207v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13180v1",
    "title": "Translation Entropy: A Statistical Framework for Evaluating Translation Systems",
    "summary": "The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.",
    "authors": [
      "Ronit D. Gross",
      "Yanir Harel",
      "Ido Kanter"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13180v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13180v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13166v1",
    "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
    "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
    "authors": [
      "Zhaoxin Shen",
      "Dan Wu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13166v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13166v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13680v1",
    "title": "Cross-Learning from Scarce Data via Multi-Task Constrained Optimization",
    "summary": "A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \\emph{cross-learning} framework to overcome data scarcity by jointly estimating \\emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.",
    "authors": [
      "Leopoldo Agorio",
      "Juan Cerviño",
      "Miguel Calvo-Fullana",
      "Alejandro Ribeiro",
      "Juan Andrés Bazerque"
    ],
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13680v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13680v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13466v1",
    "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology",
    "summary": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.",
    "authors": [
      "Jaclyn Ocumpaugh",
      "Luc Paquette",
      "Ryan S. Baker",
      "Amanda Barany",
      "Jeff Ginger",
      "Nathan Casano",
      "Andres F. Zambrano",
      "Xiner Liu",
      "Zhanlan Wei",
      "Yiqui Zhou",
      "Qianhui Liu",
      "Stephen Hutt",
      "Alexandra M. A. Andres",
      "Nidhi Nasiar",
      "Camille Giordano",
      "Martin van Velsen",
      "Micheal Mogessi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13466v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13466v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13458v1",
    "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
    "summary": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
    "authors": [
      "Agnese Chiatti",
      "Lara Piccolo",
      "Sara Bernardini",
      "Matteo Matteucci",
      "Viola Schiaffonati"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13458v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13458v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13359v1",
    "title": "Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms",
    "summary": "The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.",
    "authors": [
      "Yuhang Wang",
      "Yanxu Zhu",
      "Jitao Sang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13359v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13359v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13329v1",
    "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
    "summary": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
    "authors": [
      "Shufan Yang",
      "Zifeng Cheng",
      "Zhiwei Jiang",
      "Yafeng Yin",
      "Cong Wang",
      "Shiping Ge",
      "Yuchen Fu",
      "Qing Gu"
    ],
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13329v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13329v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13229v1",
    "title": "Laplace Learning in Wasserstein Space",
    "summary": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning   methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical   notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to   an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator   on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through   numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.",
    "authors": [
      "Mary Chriselda Antony Oliver",
      "Michael Roberts",
      "Carola-Bibiane Schönlieb",
      "Matthew Thorpe"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13229v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13214v1",
    "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
    "summary": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
    "authors": [
      "Guillaume Infantes",
      "Stéphanie Roussel",
      "Antoine Jacquet",
      "Emmanuel Benazera"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13214v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13214v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13204v1",
    "title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection",
    "summary": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.",
    "authors": [
      "Junhee Lee",
      "ChaeBeen Bang",
      "MyoungChul Kim",
      "MyeongAh Cho"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13204v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13204v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13712v1",
    "title": "From Black Box to Insight: Explainable AI for Extreme Event Preparedness",
    "summary": "As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.",
    "authors": [
      "Kiana Vu",
      "İsmet Selçuk Özer",
      "Phung Lai",
      "Zheng Wu",
      "Thilanka Munasinghe",
      "Jennifer Wei"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13712v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13712v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13670v1",
    "title": "Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process",
    "summary": "This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.",
    "authors": [
      "Agnieszka Bieńkowska",
      "Jacek Małecki",
      "Alexander Mathiesen-Ohman",
      "Katarzyna Tworek"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13670v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13670v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13637v1",
    "title": "Towards Multimodal Representation Learning in Paediatric Kidney Disease",
    "summary": "Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.",
    "authors": [
      "Ana Durica",
      "John Booth",
      "Ivana Drobnjak"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13637v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13637v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13463v1",
    "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
    "summary": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
    "authors": [
      "Hussein Rajabu",
      "Lijun Qian",
      "Xishuang Dong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13463v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13463v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13417v1",
    "title": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source",
    "summary": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.",
    "authors": [
      "Mykola Lavreniuk",
      "Nataliia Kussul",
      "Andrii Shelestov",
      "Yevhenii Salii",
      "Volodymyr Kuzin",
      "Sergii Skakun",
      "Zoltan Szantoi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13417v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13417v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13269v1",
    "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation",
    "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.",
    "authors": [
      "Lingfeng Zhang",
      "Yuchen Zhang",
      "Hongsheng Li",
      "Haoxiang Fu",
      "Yingbo Tang",
      "Hangjun Ye",
      "Long Chen",
      "Xiaojun Liang",
      "Xiaoshuai Hao",
      "Wenbo Ding"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13269v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13269v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13264v1",
    "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression",
    "summary": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \\textbf{\\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \\textbf{\\color{cyan}{symgs.github.io}}",
    "authors": [
      "Keshav Gupta",
      "Akshat Sanghvi",
      "Shreyas Reddy Palley",
      "Astitva Srivastava",
      "Charu Sharma",
      "Avinash Sharma"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13264v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13264v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13250v1",
    "title": "Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs",
    "summary": "We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.",
    "authors": [
      "Aleksandar Stanković",
      "Dejan Lisica"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13250v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13250v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13249v1",
    "title": "Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention",
    "summary": "Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.",
    "authors": [
      "Yu Wen",
      "Shuyong Gao",
      "Shuping Zhang",
      "Miao Huang",
      "Lili Tao",
      "Han Yang",
      "Haozhe Xing",
      "Lihe Zhang",
      "Boxue Hou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13249v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13249v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13182v1",
    "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study",
    "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.",
    "authors": [
      "Mihai Dan Nadas",
      "Laura Diosan"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13182v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13182v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13169v1",
    "title": "TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine",
    "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\\_r1 and gemini\\_2\\_5\\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the \"In-depth Challenge for Comprehensive TCM Abilities\" special track.",
    "authors": [
      "Tianai Huang",
      "Jiayuan Chen",
      "Lu Lu",
      "Pengcheng Chen",
      "Tianbin Li",
      "Bing Han",
      "Wenchao Tang",
      "Jie Xu",
      "Ming Li"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13169v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13169v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13658v1",
    "title": "Why is \"Chicago\" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues",
    "summary": "Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.",
    "authors": [
      "Jiaming Qu",
      "Mengtian Guo",
      "Yue Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13658v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13658v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13602v1",
    "title": "Nonparametric Estimation of Joint Entropy through Partitioned Sample-Spacing Method",
    "summary": "We propose a nonparametric estimator of multivariate joint entropy based on partitioned sample spacings (PSS). The method extends univariate spacing ideas to multivariate settings by partitioning the sample space into localized cells and aggregating within-cell statistics, with strong consistency guarantees under mild conditions. In benchmarks across diverse distributions, PSS consistently outperforms k-nearest neighbor estimators and achieves accuracy competitive with recent normalizing flow-based methods, while requiring no training or auxiliary density modeling. The estimator scales favorably in moderately high dimensions (d = 10 to 40) and shows particular robustness to correlated or skewed distributions. These properties position PSS as a practical alternative to normalizing flow-based approaches, with broad potential in information-theoretic machine learning applications.",
    "authors": [
      "Jungwoo Ho",
      "Sangun Park",
      "Soyeong Oh"
    ],
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13602v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13602v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13595v1",
    "title": "Physics-Informed Neural Networks for Nonlinear Output Regulation",
    "summary": "This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.",
    "authors": [
      "Sebastiano Mengozzi",
      "Giovanni B. Esposito",
      "Michelangelo Bin",
      "Andrea Acquaviva",
      "Andrea Bartolini",
      "Lorenzo Marconi"
    ],
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13595v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13595v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13588v1",
    "title": "Data-driven Acceleration of MPC with Guarantees",
    "summary": "Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.",
    "authors": [
      "Agustin Castellano",
      "Shijie Pan",
      "Enrique Mallada"
    ],
    "categories": [
      "eess.SY",
      "cs.AI",
      "math.DS"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13588v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13588v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13542v1",
    "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
    "summary": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
    "authors": [
      "Amirreza Mehrabi",
      "Jason Wade Morphew",
      "Breejha Quezada",
      "N. Sanjay Rebello"
    ],
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13542v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13542v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13339v1",
    "title": "Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model",
    "summary": "Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.",
    "authors": [
      "Han Meng",
      "Gang Mei",
      "Hong Tian",
      "Nengxiong Xu",
      "Jianbing Peng"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13339v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13339v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13191v1",
    "title": "Birth of a Painting: Differentiable Brushstroke Reconstruction",
    "summary": "Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.",
    "authors": [
      "Ying Jiang",
      "Jiayin Lu",
      "Yunuo Chen",
      "Yumeng He",
      "Kui Wu",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13191v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13191v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13705v1",
    "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering",
    "summary": "Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI \"Gene Expression Cancer RNA-Seq\" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.",
    "authors": [
      "Alaa Mezghiche"
    ],
    "categories": [
      "cs.LG",
      "q-bio.GN"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13705v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13705v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13647v1",
    "title": "Part-X-MLLM: Part-aware 3D Multimodal Large Language Model",
    "summary": "We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/",
    "authors": [
      "Chunshi Wang",
      "Junliang Ye",
      "Yunhan Yang",
      "Yang Li",
      "Zizhuo Lin",
      "Jun Zhu",
      "Zhuo Chen",
      "Yawei Luo",
      "Chunchao Guo"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13647v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13647v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13503v1",
    "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business",
    "summary": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.",
    "authors": [
      "Ioannis Diamantis"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13503v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13503v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13480v1",
    "title": "A Lexical Analysis of online Reviews on Human-AI Interactions",
    "summary": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.",
    "authors": [
      "Parisa Arbab",
      "Xiaowen Fang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13480v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13480v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13226v1",
    "title": "Informative Communication of Robot Plans",
    "summary": "When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.",
    "authors": [
      "Michele Persiani",
      "Thomas Hellstrom"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13226v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13226v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13713v1",
    "title": "Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine",
    "summary": "Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.",
    "authors": [
      "Xincheng Shuai",
      "Zhenyuan Qin",
      "Henghui Ding",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13713v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13713v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13649v1",
    "title": "Distribution Matching Distillation Meets Reinforcement Learning",
    "summary": "Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.",
    "authors": [
      "Dengyang Jiang",
      "Dongyang Liu",
      "Zanyi Wang",
      "Qilong Wu",
      "Xin Jin",
      "David Liu",
      "Zhen Li",
      "Mengmeng Wang",
      "Peng Gao",
      "Harry Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13649v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13649v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13232v1",
    "title": "MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI",
    "summary": "Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.",
    "authors": [
      "Malek Al Abed",
      "Sebiha Demir",
      "Anne Groteklaes",
      "Elodie Germani",
      "Shahrooz Faghihroohi",
      "Hemmen Sabir",
      "Shadi Albarqouni"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13232v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13232v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13175v1",
    "title": "HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution",
    "summary": "Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.",
    "authors": [
      "Chao Yang",
      "Boqian Zhang",
      "Jinghao Xu",
      "Guang Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13175v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13175v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13704v1",
    "title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
    "summary": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.",
    "authors": [
      "Harold Haodong Chen",
      "Disen Lan",
      "Wen-Jie Shu",
      "Qingyang Liu",
      "Zihan Wang",
      "Sirui Chen",
      "Wenkai Cheng",
      "Kanghao Chen",
      "Hongfei Zhang",
      "Zixin Zhang",
      "Rongjin Guo",
      "Yu Cheng",
      "Ying-Cong Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13704v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13704v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.13645v1",
    "title": "FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs",
    "summary": "We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.",
    "authors": [
      "Aleksandar Stanković"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13645v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13645v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.13535v1",
    "title": "Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew",
    "summary": "As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.",
    "authors": [
      "Farhin Farhad Riya",
      "Shahinul Hoque",
      "Jinyuan Stella Sun",
      "Olivera Kotevska"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13535v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13535v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.13297v1",
    "title": "CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving",
    "summary": "End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.",
    "authors": [
      "Enhui Ma",
      "Lijun Zhou",
      "Tao Tang",
      "Jiahuan Zhang",
      "Junpeng Jiang",
      "Zhan Zhang",
      "Dong Han",
      "Kun Zhan",
      "Xueyang Zhang",
      "XianPeng Lang",
      "Haiyang Sun",
      "Xia Zhou",
      "Di Lin",
      "Kaicheng Yu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13297v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13297v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.13586v1",
    "title": "Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images",
    "summary": "Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.   To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.   To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.",
    "authors": [
      "Yinuo Xu",
      "Yan Cui",
      "Mingyao Li",
      "Zhi Huang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13586v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13586v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.13420v1",
    "title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task",
    "summary": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.",
    "authors": [
      "Xingming Long",
      "Jie Zhang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13420v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13420v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.13400v1",
    "title": "What Color Is It? A Text-Interference Multimodal Hallucination Benchmark",
    "summary": "With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the \"What Color Is It\" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.",
    "authors": [
      "Jinkun Zhao",
      "Lei Huang",
      "Wenjun Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13400v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13400v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.13618v1",
    "title": "A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio",
    "summary": "One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).",
    "authors": [
      "Ashlesha G. Sawant",
      "Shreyash S. Kamble",
      "Raj S. Kanade",
      "Raunak N. Kanugo",
      "Tanishq A. Kapse",
      "Karan A. Bhapse"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13618v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13618v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.13344v1",
    "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
    "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
    "authors": [
      "Ori Meiraz",
      "Sharon Shalev",
      "Avishai Weizman"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13344v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13344v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.13278v1",
    "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
    "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
    "authors": [
      "Zihan Li",
      "Tengfei Wang",
      "Wentian Gan",
      "Hao Zhan",
      "Xin Wang",
      "Zongqian Zhan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13278v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13278v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.13552v1",
    "title": "TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images",
    "summary": "Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.",
    "authors": [
      "Sining Chen",
      "Xiao Xiang Zhu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13552v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13552v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.13197v1",
    "title": "Self-Supervised Ultrasound Screen Detection",
    "summary": "Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.",
    "authors": [
      "Alberto Gomez",
      "Jorge Oliveira",
      "Ramon Casero",
      "Agis Chartsias"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13197v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13197v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.13533v1",
    "title": "Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems",
    "summary": "In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.",
    "authors": [
      "Jeffrey Wen",
      "Rizwan Ahmad",
      "Philip Schniter"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13533v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13533v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.13276v1",
    "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
    "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
    "authors": [
      "Noam Tsfaty",
      "Avishai Weizman",
      "Liav Cohen",
      "Moshe Tshuva",
      "Yehudit Aperstein"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13276v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13276v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.13199v1",
    "title": "Asymptotic confidence bands for centered purely random forests",
    "summary": "In a multivariate nonparametric regression setting we construct explicit asymptotic uniform confidence bands for centered purely random forests. Since the most popular example in this class of random forests, namely the uniformly centered purely random forests, is well known to suffer from suboptimal rates, we propose a new type of purely random forests, called the Ehrenfest centered purely random forests, which achieve minimax optimal rates. Our main confidence band theorem applies to both random forests. The proof is based on an interpretation of random forests as generalized U-Statistics together with a Gaussian approximation of the supremum of empirical processes. Our theoretical findings are illustrated in simulation examples.",
    "authors": [
      "Natalie Neumeyer",
      "Jan Rabe",
      "Mathias Trabs"
    ],
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13199v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13199v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.13720v1",
    "title": "Back to Basics: Let Denoising Generative Models Denoise",
    "summary": "Today's denoising diffusion models do not \"denoise\" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than \"$\\textbf{Just image Transformers}$\", or $\\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.",
    "authors": [
      "Tianhong Li",
      "Kaiming He"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13720v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13720v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2511.13494v1",
    "title": "Language-Guided Invariance Probing of Vision-Language Models",
    "summary": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.   Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.",
    "authors": [
      "Jae Joong Lee"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13494v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13494v1.pdf",
    "date": "2025-11-19",
    "source": "arxiv",
    "research_score": 0.43
  }
]