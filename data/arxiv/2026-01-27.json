[
  {
    "arxiv_id": "2601.17645v1",
    "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking",
    "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public",
    "authors": [
      "Xilin Jiang",
      "Qiaolin Wang",
      "Junkai Wu",
      "Xiaomin He",
      "Zhongweiyang Xu",
      "Yinghao Ma",
      "Minshuo Piao",
      "Kaiyi Yang",
      "Xiuwen Zheng",
      "Riki Shimizu",
      "Yicong Chen",
      "Arsalan Firoozi",
      "Gavin Mischler",
      "Sukru Samet Dindar",
      "Richard Antonello",
      "Linyang He",
      "Tsun-An Hsieh",
      "Xulin Fan",
      "Yulun Wu",
      "Yuesheng Ma",
      "Chaitanya Amballa",
      "Weixiong Chen",
      "Jiarui Hai",
      "Ruisi Li",
      "Vishal Choudhari",
      "Cong Han",
      "Yinghao Aaron Li",
      "Adeen Flinker",
      "Mounya Elhilali",
      "Emmanouil Benetos",
      "Mark Hasegawa-Johnson",
      "Romit Roy Choudhury",
      "Nima Mesgarani"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17645v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17645v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.83
  },
  {
    "arxiv_id": "2601.17952v1",
    "title": "A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models",
    "summary": "Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.",
    "authors": [
      "Michail Mamalakis",
      "Tiago Azevedo",
      "Cristian Cosentino",
      "Chiara D'Ercoli",
      "Subati Abulikemu",
      "Zhongtian Sun",
      "Richard Bethlehem",
      "Pietro Lio"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17952v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17952v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.81
  },
  {
    "arxiv_id": "2601.17737v1",
    "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation",
    "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.",
    "authors": [
      "Chenyu Mu",
      "Xin He",
      "Qu Yang",
      "Wanshun Chen",
      "Jiadi Yao",
      "Huang Liu",
      "Zihao Yi",
      "Bo Zhao",
      "Xingyu Chen",
      "Ruotian Ma",
      "Fanghua Ye",
      "Erkun Yang",
      "Cheng Deng",
      "Zhaopeng Tu",
      "Xiaolong Li",
      " Linus"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17737v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17737v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2601.17705v1",
    "title": "Distance-to-Distance Ratio: A Similarity Measure for Sentences Based on Rate of Change in LLM Embeddings",
    "summary": "A measure of similarity between text embeddings can be considered adequate only if it adheres to the human perception of similarity between texts. In this paper, we introduce the distance-to-distance ratio (DDR), a novel measure of similarity between LLM sentence embeddings. Inspired by Lipschitz continuity, DDR measures the rate of change in similarity between the pre-context word embeddings and the similarity between post-context LLM embeddings, thus measuring the semantic influence of context. We evaluate the performance of DDR in experiments designed as a series of perturbations applied to sentences drawn from a sentence dataset. For each sentence, we generate variants by replacing one, two, or three words with either synonyms, which constitute semantically similar text, or randomly chosen words, which constitute semantically dissimilar text. We compare the performance of DDR with other prevailing similarity metrics and demonstrate that DDR consistently provides finer discrimination between semantically similar and dissimilar texts, even under minimal, controlled edits.",
    "authors": [
      "Abdullah Qureshi",
      "Kenneth Rice",
      "Alexander Wolpert"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17705v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17705v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2601.17687v1",
    "title": "Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis",
    "summary": "Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.",
    "authors": [
      "Hao Li",
      "He Cao",
      "Shenyao Peng",
      "Zijing Liu",
      "Bin Feng",
      "Yu Wang",
      "Zhiyuan Yan",
      "Yonghong Tian",
      "Yu Li",
      "Li Yuan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17687v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17687v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2601.17699v1",
    "title": "SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL",
    "summary": "While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.",
    "authors": [
      "Harper Hua",
      "Zhen Han",
      "Zhengyuan Shen",
      "Jeremy Lee",
      "Patrick Guan",
      "Qi Zhu",
      "Sullam Jeoung",
      "Yueyan Chen",
      "Yunfei Bai",
      "Shuai Wang",
      "Vassilis Ioannidis",
      "Huzefa Rangwala"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17699v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17699v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2601.17668v1",
    "title": "Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction",
    "summary": "Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.",
    "authors": [
      "Jang-Hyun Kim",
      "Dongyoon Han",
      "Sangdoo Yun"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17668v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17668v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2601.17858v1",
    "title": "MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging",
    "summary": "Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \\textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.",
    "authors": [
      "Jiapeng Wang",
      "Changxin Tian",
      "Kunlong Chen",
      "Ziqi Liu",
      "Jiaxin Mao",
      "Wayne Xin Zhao",
      "Zhiqiang Zhang",
      "Jun Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17858v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17858v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2601.17728v1",
    "title": "Unsupervised Elicitation of Moral Values from Language Models",
    "summary": "As AI systems become pervasive, grounding their behavior in human values is critical. Prior work suggests that language models (LMs) exhibit limited inherent moral reasoning, leading to calls for explicit moral teaching. However, constructing ground truth data for moral evaluation is difficult given plural frameworks and pervasive biases. We investigate unsupervised elicitation as an alternative, asking whether pretrained (base) LMs possess intrinsic moral reasoning capability that can be surfaced without human supervision. Using the Internal Coherence Maximization (ICM) algorithm across three benchmark datasets and four LMs, we test whether ICM can reliably label moral judgments, generalize across moral frameworks, and mitigate social bias. Results show that ICM outperforms all pre-trained and chatbot baselines on the Norm Bank and ETHICS benchmarks, while fine-tuning on ICM labels performs on par with or surpasses those of human labels. Across theoretically motivated moral frameworks, ICM yields its largest relative gains on Justice and Commonsense morality. Furthermore, although chatbot LMs exhibit social bias failure rates comparable to their pretrained ones, ICM reduces such errors by more than half, with the largest improvements in race, socioeconomic status, and politics. These findings suggest that pretrained LMs possess latent moral reasoning capacities that can be elicited through unsupervised methods like ICM, providing a scalable path for AI alignment.",
    "authors": [
      "Meysam Alizadeh",
      "Fabrizio Gilardi",
      "Zeynab Samei"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17728v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17728v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2601.17706v1",
    "title": "A Computational Approach to Visual Metonymy",
    "summary": "Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines' ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.",
    "authors": [
      "Saptarshi Ghosh",
      "Linfeng Liu",
      "Tianyu Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17706v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17706v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2601.17927v1",
    "title": "RemEdit: Efficient Diffusion Editing with Riemannian Geometry",
    "summary": "Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.",
    "authors": [
      "Eashan Adhikarla",
      "Brian D. Davison"
    ],
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17927v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17927v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.17814v1",
    "title": "MMR-Bench: A Comprehensive Benchmark for Multimodal LLM Routing",
    "summary": "Multimodal large language models (MLLMs) have advanced rapidly, yet heterogeneity in architecture, alignment strategies, and efficiency means that no single model is uniformly superior across tasks. In practical deployments, workloads span lightweight OCR to complex multimodal reasoning; using one MLLM for all queries either over-provisions compute on easy instances or sacrifices accuracy on hard ones. Query-level model selection (routing) addresses this tension, but extending routing from text-only LLMs to MLLMs is nontrivial due to modality fusion, wide variation in computational cost across models, and the absence of a standardized, budget-aware evaluation. We present MMR-Bench, a unified benchmark that isolates the multimodal routing problem and enables comparison under fixed candidate sets and cost models. MMR-Bench provides (i) a controlled environment with modality-aware inputs and variable compute budgets, (ii) a broad suite of vision-language tasks covering OCR, general VQA, and multimodal math reasoning, and (iii) strong single-model reference, oracle upper bounds, and representative routing policies. Using MMR-Bench, we show that incorporating multimodal signals improves routing quality. Empirically, these cues improve the cost-accuracy frontier and enable the routed system to exceed the strongest single model's accuracy at roughly 33% of its cost. Furthermore, policies trained on a subset of models and tasks generalize zero-shot to new datasets and text-only benchmarks without retuning, establishing MMR-Bench as a foundation for studying adaptive multimodal model selection and efficient MLLM deployment. The code will be available at: https://github.com/Hunter-Wrynn/MMR-Bench.",
    "authors": [
      "Haoxuan Ma",
      "Guannan Lai",
      "Han-Jia Ye"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17814v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17814v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.17702v1",
    "title": "S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference",
    "summary": "Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.   We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.   At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.",
    "authors": [
      "Qingsen Ma",
      "Dianyun Wang",
      "Yaoye Wang",
      "Lechen Ning",
      "Sujie Zhu",
      "Xiaohang Zhang",
      "Jiaming Lyu",
      "Linhao Ren",
      "Zhenbo Xu",
      "Zhaofeng He"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17702v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17702v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.17664v1",
    "title": "UrduLM: A Resource-Efficient Monolingual Urdu Language Model",
    "summary": "Urdu, spoken by 230 million people worldwide, lacks dedicated transformer-based language models and curated corpora. While multilingual models provide limited Urdu support, they suffer from poor performance, high computational costs, and cultural inaccuracies due to insufficient training data. To address these challenges, we present UrduLM, a pretrained Urdu monolingual language model trained in low-resource settings. We curate a 33GB Urdu corpus from diverse sources, develop a custom BPE tokenizer that reduces tokenization overhead by atleast 20-30% compared to multilingual alternatives, and pretrain a 100M-parameter decoder-only model. In few-shot evaluations, UrduLM achieves competitive performance with multilingual models up to 30x its size, reaching 66.6% accuracy on sentiment classification and BLEU scores exceeding 30 on grammar correction tasks. The complete methodology -- including corpus, tokenizer, model weights, and evaluation benchmarks -- is released openly to establish a baseline for Urdu NLP research and provide a scalable framework for other underrepresented languages.",
    "authors": [
      "Syed Muhammad Ali",
      "Hammad Sajid",
      "Zainab Haider",
      "Ali Muhammad Asad",
      "Haya Fatima",
      "Abdul Samad"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17664v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17664v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2601.17942v1",
    "title": "LLM-Based SQL Generation: Prompting, Self-Refinement, and Adaptive Weighted Majority Voting",
    "summary": "Text-to-SQL has emerged as a prominent research area, particularly with the rapid advancement of large language models (LLMs). By enabling users to query databases through natural language rather than SQL, this technology significantly lowers the barrier to data analysis. However, generating accurate SQL from natural language remains challenging due to ambiguity in user queries, the complexity of schema linking, limited generalization across SQL dialects, and the need for domain-specific understanding. In this study, we propose a Single-Agent Self-Refinement with Ensemble Voting (SSEV) pipeline built on PET-SQL that operates without ground-truth data, integrating self-refinement with Weighted Majority Voting (WMV) and its randomized variant (RWMA). Experimental results show that the SSEV achieves competitive performance across multiple benchmarks, attaining execution accuracies of 85.5% on Spider 1.0-Dev, 86.4% on Spider 1.0-Test, and 66.3% on BIRD-Dev. Building on insights from the SSEV pipeline, we further propose ReCAPAgent-SQL (Refinement-Critique-Act-Plan agent-based SQL framework) to address the growing complexity of enterprise databases and real-world Text-to-SQL tasks. The framework integrates multiple specialized agents for planning, external knowledge retrieval, critique, action generation, self-refinement, schema linking, and result validation, enabling iterative refinement of SQL predictions through agent collaboration. ReCAPAgent-SQL's WMA results achieve 31% execution accuracy on the first 100 queries of Spider 2.0-Lite, demonstrating significant improvements in handling real-world enterprise scenarios. Overall, our work facilitates the deployment of scalable Text-to-SQL systems in practical settings, supporting better data-driven decision-making at lower cost and with greater efficiency.",
    "authors": [
      "Yu-Jie Yang",
      "Hung-Fu Chang",
      "Po-An Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17942v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17942v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.17907v1",
    "title": "FARM: Few-shot Adaptive Malware Family Classification under Concept Drift",
    "summary": "Malware classification models often face performance degradation due to concept drift, arising from evolving threat landscapes and the emergence of novel malware families. This paper presents FARM (Few-shot Adaptive Recognition of Malware), a framework designed to detect and adapt to both covariate and label drift in Windows Portable Executable (PE) malware classification. FARM leverages a triplet autoencoder to project samples into a discriminative latent space, enabling unsupervised drift detection via DBSCAN clustering and dynamic thresholding. For rapid adaptation, it employs few-shot learning using prototype-based classification, requiring only a handful of labeled samples. FARM also supports full retraining when enough drifted samples accumulate, updating the latent space for long-term integration. Experiments on the BenchMFC dataset demonstrate that FARM improves classification performance under covariate drift by 5.6\\%, and achieves an average F1 score of 0.85 on unseen malware families using only few-shot adaptation, which further increases to 0.94 after retraining. These results highlight FARM's robustness and adaptability in dynamic malware detection environments under limited supervision.",
    "authors": [
      "Numan Halit Guldemir",
      "Oluwafemi Olukoya",
      "Jesús Martínez-del-Rincón"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17907v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17907v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.17868v1",
    "title": "VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding",
    "summary": "Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.",
    "authors": [
      "Zhihao He",
      "Tieyuan Chen",
      "Kangyu Wang",
      "Ziran Qin",
      "Yang Shao",
      "Chaofan Gan",
      "Shijie Li",
      "Zuxuan Wu",
      "Weiyao Lin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17868v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17868v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.17828v1",
    "title": "Aligning Medical Conversational AI through Online Reinforcement Learning with Information-Theoretic Rewards",
    "summary": "We present Information Gain Fine-Tuning (IGFT), a novel approach for training medical conversational AI to conduct effective patient interviews and generate comprehensive History of Present Illness (HPI) without requiring pre-collected human conversations. IGFT combines online Group Relative Policy Optimization (GRPO) with information-theoretic rewards, enabling models to learn from self-generated conversations with simulated patients. Unlike existing approaches that rely on expensive expert-annotated conversations or static datasets, our online RL framework allows models to discover effective questioning strategies through exploration. Our key innovation is an information gain reward function that tracks which clinical entities such as symptoms, temporal patterns, and medical history, are revealed during conversation. Each question's reward is computed based on its expected information gain combined with GPT-4o-mini quality assessments across dimensions including clinical relevance, patient engagement, and specificity. This hybrid approach ensures models learn to ask targeted, clinically appropriate questions that efficiently gather diagnostic information. We fine-tune two models using LoRA: Llama-3.1-8B-Instruct and DeepSeek-R1-Distill-Qwen-7B (a reasoning-optimized model). Training exclusively on Avey data containing concise HPIs, we evaluate generalization to MIMIC data with longer, more elaborate HPIs. DeepSeek-R1-Distill-Qwen-7B (IGFT) achieves F1 scores of 0.408 on Avey (10.9% improvement over base) and 0.289 on MIMIC (12.9% improvement), while Llama-3.1-8B-Instruct (IGFT) reaches 0.384 and 0.336 respectively. Both models outperform OpenAI's model on MIMIC and surpass medical domain-specific baselines like HuatuoGPT and UltraMedical, which were optimized for single-turn medical QA rather than multi-turn conversations.",
    "authors": [
      "Tanvi Verma",
      "Yang Zhou",
      "Rick Siow Mong Goh",
      "Yong Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17828v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17828v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2601.17934v1",
    "title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images",
    "summary": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.",
    "authors": [
      "Vi Vu",
      "Thanh-Huy Nguyen",
      "Tien-Thinh Nguyen",
      "Ba-Thinh Lam",
      "Hoang-Thien Nguyen",
      "Tianyang Wang",
      "Xingjian Li",
      "Min Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17934v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17934v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.17916v1",
    "title": "UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR",
    "summary": "Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.",
    "authors": [
      "Jialu Tang",
      "Tong Xia",
      "Yuan Lu",
      "Aaqib Saeed"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17916v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17916v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.17844v1",
    "title": "RAICL: Retrieval-Augmented In-Context Learning for Vision-Language-Model Based EEG Seizure Detection",
    "summary": "Electroencephalogram (EEG) decoding is a critical component of medical diagnostics, rehabilitation engineering, and brain-computer interfaces. However, contemporary decoding methodologies remain heavily dependent on task-specific datasets to train specialized neural network architectures. Consequently, limited data availability impedes the development of generalizable large brain decoding models. In this work, we propose a paradigm shift from conventional signal-based decoding by leveraging large-scale vision-language models (VLMs) to analyze EEG waveform plots. By converting multivariate EEG signals into stacked waveform images and integrating neuroscience domain expertise into textual prompts, we demonstrate that foundational VLMs can effectively differentiate between different patterns in the human brain. To address the inherent non-stationarity of EEG signals, we introduce a Retrieval-Augmented In-Context Learning (RAICL) approach, which dynamically selects the most representative and relevant few-shot examples to condition the autoregressive outputs of the VLM. Experiments on EEG-based seizure detection indicate that state-of-the-art VLMs under RAICL achieved better or comparable performance with traditional time series based approaches. These findings suggest a new direction in physiological signal processing that effectively bridges the modalities of vision, language, and neural activities. Furthermore, the utilization of off-the-shelf VLMs, without the need for retraining or downstream architecture construction, offers a readily deployable solution for clinical applications.",
    "authors": [
      "Siyang Li",
      "Zhuoya Wang",
      "Xiyan Gui",
      "Xiaoqing Chen",
      "Ziwei Wang",
      "Yaozhi Wen",
      "Dongrui Wu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17844v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17844v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2601.17897v1",
    "title": "UniCog: Uncovering Cognitive Abilities of LLMs through Latent Mind Space Analysis",
    "summary": "A growing body of research suggests that the cognitive processes of large language models (LLMs) differ fundamentally from those of humans. However, existing interpretability methods remain limited in explaining how cognitive abilities are engaged during LLM reasoning. In this paper, we propose UniCog, a unified framework that analyzes LLM cognition via a latent mind space. Formulated as a latent variable model, UniCog encodes diverse abilities from dense model activations into sparse, disentangled latent dimensions. Through extensive analysis on six advanced LLMs, including DeepSeek-V3.2 and GPT-4o, we reveal a Pareto principle of LLM cognition, where a shared reasoning core is complemented by ability-specific signatures. Furthermore, we discover that reasoning failures often manifest as anomalous intensity in latent activations. These findings opens a new paradigm in LLM analysis, providing a cognition grounded view of reasoning dynamics. Finally, leveraging these insights, we introduce a latent-informed candidate prioritization strategy, which improves reasoning performance by up to 7.5% across challenging benchmarks. Our code is available at https://github.com/milksalute/unicog.",
    "authors": [
      "Jiayu Liu",
      "Yinhe Long",
      "Zhenya Huang",
      "Enhong Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17897v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17897v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.17756v1",
    "title": "MV-S2V: Multi-View Subject-Consistent Video Generation",
    "summary": "Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href=\"https://szy-young.github.io/mv-s2v\">this URL</a>",
    "authors": [
      "Ziyang Song",
      "Xinyu Gong",
      "Bangya Liu",
      "Zelin Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17756v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17756v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.17658v1",
    "title": "Beyond the Rabbit Hole: Mapping the Relational Harms of QAnon Radicalization",
    "summary": "The rise of conspiracy theories has created far-reaching societal harm in the public discourse by eroding trust and fueling polarization. Beyond this public impact lies a deeply personal toll on the friends and families of conspiracy believers, a dimension often overlooked in large-scale computational research. This study fills this gap by systematically mapping radicalization journeys and quantifying the associated emotional toll inflicted on loved ones. We use the prominent case of QAnon as a case study, analyzing 12747 narratives from the r/QAnonCasualties support community through a novel mixed-methods approach. First, we use topic modeling (BERTopic) to map the radicalization trajectories, identifying key pre-existing conditions, triggers, and post-radicalization characteristics. From this, we apply an LDA-based graphical model to uncover six recurring archetypes of QAnon adherents, which we term \"radicalization personas.\" Finally, using LLM-assisted emotion detection and regression modeling, we link these personas to the specific emotional toll reported by narrators. Our findings reveal that these personas are not just descriptive; they are powerful predictors of the specific emotional harms experienced by narrators. Radicalization perceived as a deliberate ideological choice is associated with narrator anger and disgust, while those marked by personal and cognitive collapse are linked to fear and sadness. This work provides the first empirical framework for understanding radicalization as a relational phenomenon, offering a vital roadmap for researchers and practitioners to navigate its interpersonal fallout.",
    "authors": [
      "Bich Ngoc",
      " Doan",
      "Giuseppe Russo",
      "Gianmarco De Francisci Morales",
      "Robert West"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17658v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17658v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2601.17761v1",
    "title": "AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation",
    "summary": "Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of \"Omni\" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.",
    "authors": [
      "Dongjie Cheng",
      "Ruifeng Yuan",
      "Yongqi Li",
      "Runyang You",
      "Wenjie Wang",
      "Liqiang Nie",
      "Lei Zhang",
      "Wenjie Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17761v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17761v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.17741v1",
    "title": "Frequency-aware Neural Representation for Videos",
    "summary": "Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.",
    "authors": [
      "Jun Zhu",
      "Xinfeng Zhang",
      "Lv Tang",
      "Junhao Jiang",
      "Gai Zhang",
      "Jia Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17741v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17741v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.17673v1",
    "title": "Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing",
    "summary": "Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.",
    "authors": [
      "Weiyu Zhang",
      "Yuan Hu",
      "Yong Li",
      "Yu Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17673v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17673v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.17647v1",
    "title": "Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics",
    "summary": "Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\\% reduction in estimation error.",
    "authors": [
      "Akila Sampath",
      "Vandana Janeja",
      "Jianwu Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17647v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17647v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2601.17915v1",
    "title": "Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation",
    "summary": "LLM agents excel when environments are mostly static and the needed information fits in a model's context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.   We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.",
    "authors": [
      "Saurabh Jha",
      "Rohan Arora",
      " Bhavya",
      "Noah Zheutlin",
      "Paulina Toro Isaza",
      "Laura Shwartz",
      "Yu Deng",
      "Daby Sow",
      "Ruchi Mahindru",
      "Ruchir Puri"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17915v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17915v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.17789v1",
    "title": "Neuro-Symbolic Verification on Instruction Following of LLMs",
    "summary": "A fundamental problem of applying Large Language Models (LLMs) to important applications is that LLMs do not always follow instructions, and violations are often hard to observe or check. In LLM-based agentic workflows, such violations can propagate and amplify along reasoning chains, causing task failures and system incidents. This paper presents NSVIF, a neuro-symbolic framework for verifying whether an LLM's output follows the instructions used to prompt the LLM. NSVIF is a universal, general-purpose verifier; it makes no assumption about the instruction or the LLM. NSVIF formulates instruction-following verification as a constraint-satisfaction problem by modeling user instructions as constraints. NSVIF models both logical and semantic constraints; constraint solving is done by a unified solver that orchestrates logical reasoning and semantic analysis. To evaluate NSVIF, we develop VIFBENCH, a new benchmark for instruction-following verifiers with fine-grained data labels. Experiments show that NSVIF significantly outperforms LLM-based approaches and provides interpretable feedback. We also show that feedback from NSVIF helps improve LLMs' instruction-following capability without post-training.",
    "authors": [
      "Yiming Su",
      "Kunzhao Xu",
      "Yanjie Gao",
      "Fan Yang",
      "Cheng Li",
      "Mao Yang",
      "Tianyin Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17789v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17789v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2601.17935v1",
    "title": "FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering",
    "summary": "Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.",
    "authors": [
      "Daniel Commey",
      "Matilda Nkoom",
      "Yousef Alsenani",
      "Sena G. Hounsinou",
      "Garth V. Crosby"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.SI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17935v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17935v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17918v1",
    "title": "Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models",
    "summary": "Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.",
    "authors": [
      "Dain Kim",
      "Jiwoo Lee",
      "Jaehoon Yun",
      "Yong Hoe Koo",
      "Qingyu Chen",
      "Hyunjae Kim",
      "Jaewoo Kang"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17918v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17918v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17905v1",
    "title": "Feature-Space Generative Models for One-Shot Class-Incremental Learning",
    "summary": "Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.",
    "authors": [
      "Jack Foster",
      "Kirill Paramonov",
      "Mete Ozay",
      "Umberto Michieli"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17905v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17905v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17782v1",
    "title": "Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics",
    "summary": "The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.",
    "authors": [
      "Md Sahidullah",
      "Hye-jin Shim",
      "Rosa Gonzalez Hautamäki",
      "Tomi H. Kinnunen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17782v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17782v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17767v1",
    "title": "HyCARD-Net: A Synergistic Hybrid Intelligence Framework for Cardiovascular Disease Diagnosis",
    "summary": "Cardiovascular disease (CVD) remains the foremost cause of mortality worldwide, underscoring the urgent need for intelligent and data-driven diagnostic tools. Traditional predictive models often struggle to generalize across heterogeneous datasets and complex physiological patterns. To address this, we propose a hybrid ensemble framework that integrates deep learning architectures, Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM), with classical machine learning algorithms, including K-Nearest Neighbor (KNN) and Extreme Gradient Boosting (XGB), using an ensemble voting mechanism. This approach combines the representational power of deep networks with the interpretability and efficiency of traditional models. Experiments on two publicly available Kaggle datasets demonstrate that the proposed model achieves superior performance, reaching 82.30 percent accuracy on Dataset I and 97.10 percent on Dataset II, with consistent gains in precision, recall, and F1-score. These findings underscore the robustness and clinical potential of hybrid AI frameworks for predicting cardiovascular disease and facilitating early intervention. Furthermore, this study directly supports the United Nations Sustainable Development Goal 3 (Good Health and Well-being) by promoting early diagnosis, prevention, and management of non-communicable diseases through innovative, data-driven healthcare solutions.",
    "authors": [
      "Rajan Das Gupta",
      "Xiaobin Wu",
      "Xun Liu",
      "Jiaqi He"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17767v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17767v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17753v1",
    "title": "Hylog: A Hybrid Approach to Logging Text Production in Non-alphabetic Scripts",
    "summary": "Research keyloggers are essential for cognitive studies of text production, yet most fail to capture the on-screen transformations performed by Input Method Editors (IMEs) for non-alphabetic scripts. To address this methodological gap, we present Hylog, a novel hybrid logging system that combines analytical keylogging with ecological text logging for a more complete and finer-grained analysis. Our modular, open-source system uses plug-ins for standard applications (Microsoft Word, Google Chrome) to capture both keyboard output and rendered text, which a hybridizer module then synchronizes into a dual trace. To validate the system's technical feasibility and demonstrate its analytical capabilities, we conducted a proof-of-concept study where two volunteers translated a text into simplified Chinese. Hylog successfully captured keypresses and temporal intervals between Latin letters, Chinese characters, and IME confirmations -- some measurements invisible to traditional keyloggers. The resulting data enable the formulation of new, testable hypotheses about the cognitive restrictions and affordances at different linguistic layers in IME-mediated typing. Our plug-in architecture enables extension to other IME systems and fosters more inclusive multilingual text-production research.",
    "authors": [
      "Roberto Crotti",
      "Giovanni Denaro",
      "Zhiqiang Du",
      "Ricardo Muñoz Martín"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17753v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17753v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17689v1",
    "title": "REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization",
    "summary": "Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.",
    "authors": [
      "Shanu Saklani",
      "Tushar M. Athawale",
      "Nairita Pal",
      "David Pugmire",
      "Christopher R. Johnson",
      "Soumya Dutta"
    ],
    "categories": [
      "cs.LG",
      "cs.GR"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17689v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17689v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17679v1",
    "title": "BanglaRobustNet: A Hybrid Denoising-Attention Architecture for Robust Bangla Speech Recognition",
    "summary": "Bangla, one of the most widely spoken languages, remains underrepresented in state-of-the-art automatic speech recognition (ASR) research, particularly under noisy and speaker-diverse conditions. This paper presents BanglaRobustNet, a hybrid denoising-attention framework built on Wav2Vec-BERT, designed to address these challenges. The architecture integrates a diffusion-based denoising module to suppress environmental noise while preserving Bangla-specific phonetic cues, and a contextual cross-attention module that conditions recognition on speaker embeddings for robustness across gender, age, and dialects. Trained end-to-end with a composite objective combining CTC loss, phonetic consistency, and speaker alignment, BanglaRobustNet achieves substantial reductions in word error rate (WER) and character error rate (CER) compared to Wav2Vec-BERT and Whisper baselines. Evaluations on Mozilla Common Voice Bangla and augmented noisy speech confirm the effectiveness of our approach, establishing BanglaRobustNet as a robust ASR system tailored to low-resource, noise-prone linguistic settings.",
    "authors": [
      "Md Sazzadul Islam Ridoy",
      "Mubaswira Ibnat Zidney",
      "Sumi Akter",
      "Md. Aminur Rahman"
    ],
    "categories": [
      "cs.SD",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17679v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17679v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17642v1",
    "title": "Health-ORSC-Bench: A Benchmark for Measuring Over-Refusal and Safety Completion in Health Context",
    "summary": "Safety alignment in Large Language Models is critical for healthcare; however, reliance on binary refusal boundaries often results in \\emph{over-refusal} of benign queries or \\emph{unsafe compliance} with harmful ones. While existing benchmarks measure these extremes, they fail to evaluate Safe Completion: the model's ability to maximise helpfulness on dual-use or borderline queries by providing safe, high-level guidance without crossing into actionable harm. We introduce \\textbf{Health-ORSC-Bench}, the first large-scale benchmark designed to systematically measure \\textbf{Over-Refusal} and \\textbf{Safe Completion} quality in healthcare. Comprising 31,920 benign boundary prompts across seven health categories (e.g., self-harm, medical misinformation), our framework uses an automated pipeline with human validation to test models at varying levels of intent ambiguity. We evaluate 30 state-of-the-art LLMs, including GPT-5 and Claude-4, revealing a significant tension: safety-optimised models frequently refuse up to 80\\% of \"Hard\" benign prompts, while domain-specific models often sacrifice safety for utility. Our findings demonstrate that model family and size significantly influence calibration: larger frontier models (e.g., GPT-5, Llama-4) exhibit \"safety-pessimism\" and higher over-refusal than smaller or MoE-based counterparts (e.g., Qwen-3-Next), highlighting that current LLMs struggle to balance refusal and compliance. Health-ORSC-Bench provides a rigorous standard for calibrating the next generation of medical AI assistants toward nuanced, safe, and helpful completions. The code and data will be released upon acceptance. \\textcolor{red}{Warning: Some contents may include toxic or undesired contents.}",
    "authors": [
      "Zhihao Zhang",
      "Liting Huang",
      "Guanghao Wu",
      "Preslav Nakov",
      "Heng Ji",
      "Usman Naseem"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17642v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17642v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2601.17830v1",
    "title": "VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training",
    "summary": "Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \\textbf{\\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \\name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \\name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \\name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\\% extra GFLOPs with zero additional cost for external guidance models.",
    "authors": [
      "Mengmeng Wang",
      "Dengyang Jiang",
      "Liuzhuozheng Li",
      "Yucheng Lin",
      "Guojiang Shen",
      "Xiangjie Kong",
      "Yong Liu",
      "Guang Dai",
      "Jingdong Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17830v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17830v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.17826v1",
    "title": "RegGuard: AI-Powered Retrieval-Enhanced Assistant for Pharmaceutical Regulatory Compliance",
    "summary": "The increasing frequency and complexity of regulatory updates present a significant burden for multinational pharmaceutical companies. Compliance teams must interpret evolving rules across jurisdictions, formats, and agencies, often manually, at high cost and risk of error. We introduce RegGuard, an industrial-scale AI assistant designed to automate the interpretation of heterogeneous regulatory texts and align them with internal corporate policies. The system ingests heterogeneous document sources through a secure pipeline and enhances retrieval and generation quality with two novel components: HiSACC (Hierarchical Semantic Aggregation for Contextual Chunking) semantically segments long documents into coherent units while maintaining consistency across non-contiguous sections. ReLACE (Regulatory Listwise Adaptive Cross-Encoder for Reranking), a domain-adapted cross-encoder built on an open-source model, jointly models user queries and retrieved candidates to improve ranking relevance. Evaluations in enterprise settings demonstrate that RegGuard improves answer quality specifically in terms of relevance, groundedness, and contextual focus, while significantly mitigating hallucination risk. The system architecture is built for auditability and traceability, featuring provenance tracking, access control, and incremental indexing, making it highly responsive to evolving document sources and relevant for any domain with stringent compliance demands.",
    "authors": [
      "Siyuan Yang",
      "Xihan Bian",
      "Jiayin Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17826v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17826v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.17755v1",
    "title": "ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation",
    "summary": "Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.",
    "authors": [
      "Jinyoung Park",
      "Sanghyeok Lee",
      "Omar Zia Khan",
      "Hyunwoo J. Kim",
      "Joo-Kyung Kim"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17755v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17755v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.17671v1",
    "title": "Align to the Pivot: Dual Alignment with Self-Feedback for Multilingual Math Reasoning",
    "summary": "Despite the impressive reasoning abilities demonstrated by large language models (LLMs), empirical evidence indicates that they are not language agnostic as expected, leading to performance declines in multilingual settings, especially for low-resource languages. We attribute the decline to the model's inconsistent multilingual understanding and reasoning alignment. To address this, we present Pivot-Aligned Self-Feedback Multilingual Reasoning (PASMR), aiming to improve the alignment of multilingual math reasoning abilities in LLMs. This approach designates the model's primary language as the pivot language. During training, the model first translates questions into the pivot language to facilitate better alignment of reasoning patterns. The reasoning process in the target language is then supervised by the pivot language's reasoning answers, thereby establishing a cross-lingual self-feedback mechanism without relying on external correct answers or reward models. Extensive experimental results demonstrate that our method enhances both the model's understanding of questions and its reasoning capabilities, leading to notable task improvements.",
    "authors": [
      "Chunxu Zhao",
      "Xin Huang",
      "Xue Han",
      "Shujian Huang",
      "Chao Deng",
      "Junlan Feng"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17671v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17671v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.17657v1",
    "title": "SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip",
    "authors": [
      "Taewan Cho",
      "Taeryang Kim",
      "Andrew Jaeyong Choi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17657v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17657v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2601.17917v1",
    "title": "treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding",
    "summary": "Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.",
    "authors": [
      "Zhongyu Xiao",
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Yong Luo",
      "Jia Liu",
      "Jie Xu",
      "Han Hu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17917v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17917v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17899v1",
    "title": "Evolving Interdependent Operators with Large Language Models for Multi-Objective Combinatorial Optimization",
    "summary": "Neighborhood search operators are critical to the performance of Multi-Objective Evolutionary Algorithms (MOEAs) and rely heavily on expert design. Although recent LLM-based Automated Heuristic Design (AHD) methods have made notable progress, they primarily optimize individual heuristics or components independently, lacking explicit exploration and exploitation of dynamic coupling relationships between multiple operators. In this paper, multi-operator optimization in MOEAs is formulated as a Markov decision process, enabling the improvement of interdependent operators through sequential decision-making. To address this, we propose the Evolution of Operator Combination (E2OC) framework for MOEAs, which achieves the co-evolution of design strategies and executable codes. E2OC employs Monte Carlo Tree Search to progressively search combinations of operator design strategies and adopts an operator rotation mechanism to identify effective operator configurations while supporting the integration of mainstream AHD methods as the underlying designer. Experimental results across AHD tasks with varying objectives and problem scales show that E2OC consistently outperforms state-of-the-art AHD and other multi-heuristic co-design frameworks, demonstrating strong generalization and sustained optimization capability.",
    "authors": [
      "Junhao Qiu",
      "Xin Chen",
      "Liang Ge",
      "Liyong Lin",
      "Zhichao Lu",
      "Qingfu Zhang"
    ],
    "categories": [
      "cs.NE"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17899v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17899v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17842v1",
    "title": "EFT-CoT: A Multi-Agent Chain-of-Thought Framework for Emotion-Focused Therapy",
    "summary": "Leveraging Large Language Models (LLMs) for Mental Health Question Answering (MHQA) is promising for mitigating resource shortages. However, existing Cognitive Behavioral Therapy (CBT)-based approaches predominantly favor a \"top-down\" rational restructuring, often neglecting clients' embodied experiences and primary emotion processing. To address this, we propose an Emotion-Focused Therapy (EFT)-based Multi-Agent Chain-of-Thought framework (EFT-CoT). Adopting a \"bottom-up\" trajectory, it deconstructs the intervention into a three-stage reasoning flow: \"Embodied Perception - Cognitive Exploration - Narrative Intervention.\" Utilizing eight specialized agents, the system explicitly executes critical components such as somatic awareness mapping, adaptive assessment, core belief extraction, and narrative restructuring. We further constructed \"EFT-Instruct,\" a high-quality dataset via Chain-of-Thought distillation of approximately 67,000 authentic texts, and fine-tuned a specialized model, EFT-LLM. Experimental evaluations demonstrate that EFT-LLM outperforms strong baselines and human responses across metrics like empathy depth and structural professionalism. Ablation studies confirm the necessity of the multi-agent mechanism. The model exhibits superior psychological reasoning, offering an effective pathway for interpretable, high-empathy counseling systems.",
    "authors": [
      "Lanqing Du",
      "Yunong Li",
      "YuJie Long",
      "Shihong Chen"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17842v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17842v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17818v1",
    "title": "ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning",
    "summary": "Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.",
    "authors": [
      "Wen Luo",
      "Peng Chen",
      "Xiaotao Huang",
      "LiQun Huang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17818v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17818v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17770v1",
    "title": "Context-Aware Iterative Token Detection and Masked Transmission for Wireless Token Communication",
    "summary": "The success of large-scale language models has established tokens as compact and meaningful units for natural-language representation, which motivates token communication over wireless channels, where tokens are considered fundamental units for wireless transmission. We propose a context-aware token communication framework that uses a pretrained masked language model (MLM) as a shared contextual probability model between the transmitter (Tx) and receiver (Rx). At Rx, we develop an iterative token detection method that jointly exploits MLM-guided contextual priors and channel observations based on a Bayesian perspective. At Tx, we additionally introduce a context-aware masking strategy which skips highly predictable token transmission to reduce transmission rate. Simulation results demonstrate that the proposed framework substantially improves reconstructed sentence quality and supports effective rate adaptation under various channel conditions.",
    "authors": [
      "Junyong Shin",
      "Joohyuk Park",
      "Jihong Park",
      "Jinho Choi",
      "Yo-Seb Jeon"
    ],
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17770v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17770v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17722v1",
    "title": "EntWorld: A Holistic Environment and Benchmark for Verifiable Enterprise GUI Agents",
    "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled agents to operate in open-ended web and operating system environments. However, existing benchmarks predominantly target consumer-oriented scenarios (e.g., e-commerce and travel booking), failing to capture the complexity and rigor of professional enterprise workflows. Enterprise systems pose distinct challenges, including high-density user interfaces, strict business logic constraints, and a strong reliance on precise, state-consistent information retrieval-settings in which current generalist agents often struggle. To address this gap, we introduce EntWorld, a large-scale benchmark consisting of 1,756 tasks across six representative enterprise domains, including customer relationship management (CRM), information technology infrastructure library (ITIL), and enterprise resource planning (ERP) systems. Unlike previous datasets that depend on fragile execution traces or extensive manual annotation, EntWorld adopts a schema-grounded task generation framework that directly reverse-engineers business logic from underlying database schemas, enabling the synthesis of realistic, long-horizon workflows. Moreover, we propose a SQL-based deterministic verification mechanism in building datasets that replaces ambiguous visual matching with rigorous state-transition validation. Experimental results demonstrate that state-of-the-art models (e.g., GPT-4.1) achieve 47.61% success rate on EntWorld, substantially lower than the human performance, highlighting a pronounced enterprise gap in current agentic capabilities and the necessity of developing domain-specific agents. We release EntWorld as a rigorous testbed to facilitate the development and evaluation of the next generation of enterprise-ready digital agents.",
    "authors": [
      "Ying Mo",
      "Yu Bai",
      "Dapeng Sun",
      "Yuqian Shi",
      "Yukai Miao",
      "Li Chen",
      "Dan Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17722v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17722v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17717v1",
    "title": "The LLM Data Auditor: A Metric-oriented Survey on Quality and Trustworthiness in Evaluating Synthetic Data",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for generating data across various modalities. By transforming data from a scarce resource into a controllable asset, LLMs mitigate the bottlenecks imposed by the acquisition costs of real-world data for model training, evaluation, and system iteration. However, ensuring the high quality of LLM-generated synthetic data remains a critical challenge. Existing research primarily focuses on generation methodologies, with limited direct attention to the quality of the resulting data. Furthermore, most studies are restricted to single modalities, lacking a unified perspective across different data types. To bridge this gap, we propose the \\textbf{LLM Data Auditor framework}. In this framework, we first describe how LLMs are utilized to generate data across six distinct modalities. More importantly, we systematically categorize intrinsic metrics for evaluating synthetic data from two dimensions: quality and trustworthiness. This approach shifts the focus from extrinsic evaluation, which relies on downstream task performance, to the inherent properties of the data itself. Using this evaluation system, we analyze the experimental evaluations of representative generation methods for each modality and identify substantial deficiencies in current evaluation practices. Based on these findings, we offer concrete recommendations for the community to improve the evaluation of data generation. Finally, the framework outlines methodologies for the practical application of synthetic data across different modalities.",
    "authors": [
      "Kaituo Zhang",
      "Mingzhi Hu",
      "Hoang Anh Duy Le",
      "Fariha Kabir Torsha",
      "Zhimeng Jiang",
      "Minh Khai Bui",
      "Chia-Yuan Chang",
      "Yu-Neng Chuang",
      "Zhen Xiong",
      "Ying Lin",
      "Guanchu Wang",
      "Na Zou"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17717v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17717v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17641v1",
    "title": "RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding",
    "summary": "Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.",
    "authors": [
      "Hao Fang",
      "Ryan A. Canfield",
      "Tomohiro Ouchi",
      "Beatrice Macagno",
      "Eli Shlizerman",
      "Amy L. Orsborn"
    ],
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17641v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17641v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2601.17887v1",
    "title": "When Personalization Legitimizes Risks: Uncovering Safety Vulnerabilities in Personalized Dialogue Agents",
    "summary": "Long-term memory enables large language model (LLM) agents to support personalized and sustained interactions. However, most work on personalized agents prioritizes utility and user experience, treating memory as a neutral component and largely overlooking its safety implications. In this paper, we reveal intent legitimation, a previously underexplored safety failure in personalized agents, where benign personal memories bias intent inference and cause models to legitimize inherently harmful queries. To study this phenomenon, we introduce PS-Bench, a benchmark designed to identify and quantify intent legitimation in personalized interactions. Across multiple memory-augmented agent frameworks and base LLMs, personalization increases attack success rates by 15.8%-243.7% relative to stateless baselines. We further provide mechanistic evidence for intent legitimation from internal representations space, and propose a lightweight detection-reflection method that effectively reduces safety degradation. Overall, our work provides the first systematic exploration and evaluation of intent legitimation as a safety failure mode that naturally arises from benign, real-world personalization, highlighting the importance of assessing safety under long-term personal context. WARNING: This paper may contain harmful content.",
    "authors": [
      "Jiahe Guo",
      "Xiangran Guo",
      "Yulin Hu",
      "Zimo Long",
      "Xingyu Sui",
      "Xuda Zhi",
      "Yongbo Huang",
      "Hao He",
      "Weixiang Zhao",
      "Yanyan Zhao",
      "Bing Qin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17887v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17887v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17823v1",
    "title": "DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation",
    "summary": "In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation",
    "authors": [
      "Pranav Kasela",
      "Marco Braga",
      "Alessandro Ghiotto",
      "Andrea Pilzer",
      "Marco Viviani",
      "Alessandro Raganato"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17823v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17823v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17786v1",
    "title": "Beyond a Single Perspective: Text Anomaly Detection with Multi-View Language Representations",
    "summary": "Text anomaly detection (TAD) plays a critical role in various language-driven real-world applications, including harmful content moderation, phishing detection, and spam review filtering. While two-step \"embedding-detector\" TAD methods have shown state-of-the-art performance, their effectiveness is often limited by the use of a single embedding model and the lack of adaptability across diverse datasets and anomaly types. To address these limitations, we propose to exploit the embeddings from multiple pretrained language models and integrate them into $MCA^2$, a multi-view TAD framework. $MCA^2$ adopts a multi-view reconstruction model to effectively extract normal textual patterns from multiple embedding perspectives. To exploit inter-view complementarity, a contrastive collaboration module is designed to leverage and strengthen the interactions across different views. Moreover, an adaptive allocation module is developed to automatically assign the contribution weight of each view, thereby improving the adaptability to diverse datasets. Extensive experiments on 10 benchmark datasets verify the effectiveness of $MCA^2$ against strong baselines. The source code of $MCA^2$ is available at https://github.com/yankehan/MCA2.",
    "authors": [
      "Yixin Liu",
      "Kehan Yan",
      "Shiyuan Li",
      "Qingfeng Chen",
      "Shirui Pan"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17786v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17786v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17777v1",
    "title": "DPI: Exploiting Parameter Heterogeneity for Interference-Free Fine-Tuning",
    "summary": "Supervised fine-tuning (SFT) is a crucial step for adapting large language models (LLMs) to downstream tasks. However, conflicting objectives across heterogeneous SFT tasks often induce the \"seesaw effect\": optimizing for one task may degrade performance on others, particularly when model parameters are updated indiscriminately. In this paper, we propose a principled approach to disentangle and isolate task-specific parameter regions, motivated by the hypothesis that parameter heterogeneity underlies cross-task interference. Specifically, we first independently fine-tune LLMs on diverse SFT tasks and identify each task's core parameter region as the subset of parameters exhibiting the largest updates. Tasks with highly overlapping core parameter regions are merged for joint training, while disjoint tasks are organized into different stages. During multi-stage SFT, core parameters acquired in prior tasks are frozen, thereby preventing overwriting by subsequent tasks. To verify the effectiveness of our method, we conducted intensive experiments on multiple public datasets. The results showed that our dynamic parameter isolation strategy consistently reduced data conflicts and achieved consistent performance improvements compared to multi-stage and multi-task tuning baselines.",
    "authors": [
      "Xiaoyu Liu",
      "Xiaoyu Guan",
      "Di Liang",
      "Xianjie Wu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17777v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17777v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17747v1",
    "title": "Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection",
    "summary": "Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.",
    "authors": [
      "Kaixuan Jiang",
      "Chen Wu",
      "Zhenghui Zhao",
      "Chengxi Han"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17747v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17747v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17735v1",
    "title": "ReFuGe: Feature Generation for Prediction Tasks on Relational Databases with LLM Agents",
    "summary": "Relational databases (RDBs) play a crucial role in many real-world web applications, supporting data management across multiple interconnected tables. Beyond typical retrieval-oriented tasks, prediction tasks on RDBs have recently gained attention. In this work, we address this problem by generating informative relational features that enhance predictive performance. However, generating such features is challenging: it requires reasoning over complex schemas and exploring a combinatorially large feature space, all without explicit supervision. To address these challenges, we propose ReFuGe, an agentic framework that leverages specialized large language model agents: (1) a schema selection agent identifies the tables and columns relevant to the task, (2) a feature generation agent produces diverse candidate features from the selected schema, and (3) a feature filtering agent evaluates and retains promising features through reasoning-based and validation-based filtering. It operates within an iterative feedback loop until performance converges. Experiments on RDB benchmarks demonstrate that ReFuGe substantially improves performance on various RDB prediction tasks. Our code and datasets are available at https://github.com/K-Kyungho/REFUGE.",
    "authors": [
      "Kyungho Kim",
      "Geon Lee",
      "Juyeon Kim",
      "Dongwon Choi",
      "Shinhwan Kang",
      "Kijung Shin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17735v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17735v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17692v1",
    "title": "LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval",
    "summary": "Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.",
    "authors": [
      "Yunhan Li",
      "Mingjie Xie",
      "Gaoli Kang",
      "Zihan Gong",
      "Gengshen Wu",
      "Min Yang"
    ],
    "categories": [
      "cs.IR",
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17692v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17692v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17684v1",
    "title": "A Model-Driven Lossless Compression Algorithm Resistant to Mismatch",
    "summary": "Due to the fundamental connection between next-symbol prediction and compression, modern predictive models, such as large language models (LLMs), can be combined with entropy coding to achieve compression rates that surpass those of standard compression algorithms. However, this approach relies on the assumption that the predictive model produces identical output distributions at both the encoder and decoder, since even small mismatches can cause the decoding to fail. This assumption often fails with complex predictive models, particularly those based on neural networks, a phenomenon referred to as non-determinism.   In this work, we propose a new compression algorithm based on next-token prediction that is robust to arbitrarily large, but structured, prediction mismatches. We prove the correctness of the proposed scheme under a formal mismatch certification, characterize its theoretical performance, and validate it experimentally on real datasets. Our results demonstrate reliable operation within the certified mismatch regime while achieving compression ratios that exceed those of commonly used compression methods.",
    "authors": [
      "Cordelia Hu",
      "Jennifer Tang"
    ],
    "categories": [
      "cs.IT",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17684v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17684v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2601.17954v1",
    "title": "Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms",
    "summary": "We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.",
    "authors": [
      "Nikos Georgoudios",
      "Konstantinos Spiliopoulos",
      "Justin Sirignano"
    ],
    "categories": [
      "cs.LG",
      "math.PR"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17954v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17954v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17921v1",
    "title": "ShapLoRA: Allocation of Low-rank Adaption on Large Language Models via Shapley Value Inspired Importance Estimation",
    "summary": "Low-rank adaption (LoRA) is a representative method in the field of parameter-efficient fine-tuning (PEFT), and is key to Democratizating the modern large language models (LLMs). The vanilla LoRA is implemented with uniform ranks, and the recent literature have found that properly allocating ranks on the LLM backbones results in performance boosts. However, the previous rank allocation methods have limitations since they rely on inexplanable and unreliable importance measures for the LoRA ranks. To address the above issues, we propose the ShapLoRA framework. Inspired by the explanable attribution measure Shapley Value, we combine the sensitivity-based measures with the idea of coalitions in the collaborative games among LoRA ranks, and propose a more explainable importance measure called Shapley sensitivity. In addition, we optimize the workflow of the existing works by: (a) calculating Shapley sensitivity on a separate validation set; (b) Setting up the allocating-retraining procedures for fair comparisons. We have conducted experiments on various challenging tasks, and the experimental results demonstrate that our ShapLoRA method can outperform the recent baselines with comparable tunable parameters.\\footnote{Codes and fine-tuned models will be open-sourced to facilitate future research.",
    "authors": [
      "Yi Zhao",
      "Qinghua Yao",
      "Xinyuan song",
      "Wei Zhu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17921v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17921v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17885v1",
    "title": "PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation",
    "summary": "Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.   In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.   On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.   Project website: https://peafowlvla.github.io/.",
    "authors": [
      "Qingyu Fan",
      "Zhaoxiang Li",
      "Yi Lu",
      "Wang Chen",
      "Qiu Shen",
      "Xiao-xiao Long",
      "Yinghao Cai",
      "Tao Lu",
      "Shuo Wang",
      "Xun Cao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17885v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17885v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17869v1",
    "title": "On the Emergence and Test-Time Use of Structural Information in Large Language Models",
    "summary": "Learning structural information from observational data is central to producing new knowledge outside the training corpus. This holds for mechanistic understanding in scientific discovery as well as flexible test-time compositional generation. We thus study how language models learn abstract structures and utilize the learnt structural information at test-time. To ensure a controlled setup, we design a natural language dataset based on linguistic structural transformations. We empirically show that the emergence of learning structural information correlates with complex reasoning tasks, and that the ability to perform test-time compositional generation remains limited.",
    "authors": [
      "Michelle Chao Chen",
      "Moritz Miller",
      "Bernhard Schölkopf",
      "Siyuan Guo"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17869v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17869v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17857v1",
    "title": "SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction",
    "summary": "Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.",
    "authors": [
      "Lan Yang",
      "Minghan Yang",
      "Ke Li",
      "Honggang Zhang",
      "Kaiyue Pang",
      "Yi-Zhe Song"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17857v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17857v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17829v1",
    "title": "Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents",
    "summary": "The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \\texttt{city\\_name}, \\texttt{stock\\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.",
    "authors": [
      "Dan Greenstein",
      "Zohar Karnin",
      "Chen Amiraz",
      "Oren Somekh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17829v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17829v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17764v1",
    "title": "Cross-Lingual Probing and Community-Grounded Analysis of Gender Bias in Low-Resource Bengali",
    "summary": "Large Language Models (LLMs) have achieved significant success in recent years; yet, issues of intrinsic gender bias persist, especially in non-English languages. Although current research mostly emphasizes English, the linguistic and cultural biases inherent in Global South languages, like Bengali, are little examined. This research seeks to examine the characteristics and magnitude of gender bias in Bengali, evaluating the efficacy of current approaches in identifying and alleviating bias. We use several methods to extract gender-biased utterances, including lexicon-based mining, computational classification models, translation-based comparison analysis, and GPT-based bias creation. Our research indicates that the straight application of English-centric bias detection frameworks to Bengali is severely constrained by language disparities and socio-cultural factors that impact implicit biases. To tackle these difficulties, we executed two field investigations inside rural and low-income areas, gathering authentic insights on gender bias. The findings demonstrate that gender bias in Bengali presents distinct characteristics relative to English, requiring a more localized and context-sensitive methodology. Additionally, our research emphasizes the need of integrating community-driven research approaches to identify culturally relevant biases often neglected by automated systems. Our research enhances the ongoing discussion around gender bias in AI by illustrating the need to create linguistic tools specifically designed for underrepresented languages. This study establishes a foundation for further investigations into bias reduction in Bengali and other Indic languages, promoting the development of more inclusive and fair NLP systems.",
    "authors": [
      "Md Asgor Hossain Reaj",
      "Rajan Das Gupta",
      "Jui Saha Pritha",
      "Abdullah Al Noman",
      "Abir Ahmed",
      "Golam Md Mohiuddin",
      "Tze Hui Liew"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17764v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17764v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2601.17950v1",
    "title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
    "summary": "The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.",
    "authors": [
      "Matthew Walmer",
      "Saksham Suri",
      "Anirud Aggarwal",
      "Abhinav Shrivastava"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17950v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17950v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.17898v1",
    "title": "Assessment of Generative Named Entity Recognition in the Era of Large Language Models",
    "summary": "Named entity recognition (NER) is evolving from a sequence labeling task into a generative paradigm with the rise of large language models (LLMs). We conduct a systematic evaluation of open-source LLMs on both flat and nested NER tasks. We investigate several research questions including the performance gap between generative NER and traditional NER models, the impact of output formats, whether LLMs rely on memorization, and the preservation of general capabilities after fine-tuning. Through experiments across eight LLMs of varying scales and four standard NER datasets, we find that: (1) With parameter-efficient fine-tuning and structured formats like inline bracketed or XML, open-source LLMs achieve performance competitive with traditional encoder-based models and surpass closed-source LLMs like GPT-3; (2) The NER capability of LLMs stems from instruction-following and generative power, not mere memorization of entity-label pairs; and (3) Applying NER instruction tuning has minimal impact on general capabilities of LLMs, even improving performance on datasets like DROP due to enhanced entity understanding. These findings demonstrate that generative NER with LLMs is a promising, user-friendly alternative to traditional methods. We release the data and code at https://github.com/szu-tera/LLMs4NER.",
    "authors": [
      "Qi Zhan",
      "Yile Wang",
      "Hui Huang"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17898v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17898v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.17883v1",
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "summary": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.",
    "authors": [
      "Dingkun Liu",
      "Yuheng Chen",
      "Zhu Chen",
      "Zhenyao Cui",
      "Yaozhi Wen",
      "Jiayu An",
      "Jingwei Luo",
      "Dongrui Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17883v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17883v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.17749v1",
    "title": "Over-The-Air Extreme Learning Machines with XL Reception via Nonlinear Cascaded Metasurfaces",
    "summary": "The recently envisioned goal-oriented communications paradigm calls for the application of inference on wirelessly transferred data via Machine Learning (ML) tools. An emerging research direction deals with the realization of inference ML models directly in the physical layer of Multiple-Input Multiple-Output (MIMO) systems, which, however, entails certain significant challenges. In this paper, leveraging the technology of programmable MetaSurfaces (MSs), we present an eXtremely Large (XL) MIMO system that acts as an Extreme Learning Machine (ELM) performing binary classification tasks completely Over-The-Air (OTA), which can be trained in closed form. The proposed system comprises a receiver architecture consisting of densely parallel placed diffractive layers of XL MSs followed by a single reception radio-frequency chain. The front layer facing the MIMO channel consists of identical unit cells of a fixed NonLinear (NL) response, while the remaining layers of elements of tunable linear responses are utilized to approximate OTA the trained ELM weights. Our numerical investigations showcase that, in the XL regime of MS elements, the proposed XL-MIMO-ELM system achieves performance comparable to that of digital and idealized ML models across diverse datasets and wireless scenarios, thereby demonstrating the feasibility of embedding OTA learning capabilities into future communication systems.",
    "authors": [
      "Kyriakos Stylianopoulos",
      "Mattia Fabiani",
      "Giulia Torcolacci",
      "Davide Dardari",
      "George C. Alexandropoulos"
    ],
    "categories": [
      "eess.SP",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17749v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17749v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.17736v1",
    "title": "Athanor: Authoring Action Modification-based Interactions on Static Visualizations via Natural Language",
    "summary": "Interactivity is crucial for effective data visualizations. However, it is often challenging to implement interactions for existing static visualizations, since the underlying code and data for existing static visualizations are often not available, and it also takes significant time and effort to enable interactions for them even if the original code and data are available. To fill this gap, we propose Athanor, a novel approach to transform existing static visualizations into interactive ones using multimodal large language models (MLLMs) and natural language instructions. Our approach introduces three key innovations: (1) an action-modification interaction design space that maps visualization interactions into user actions and corresponding adjustments, (2) a multi-agent requirement analyzer that translates natural language instructions into an actionable operational space, and (3) a visualization abstraction transformer that converts static visualizations into flexible and interactive representations regardless of their underlying implementation. Athanor allows users to effortlessly author interactions through natural language instructions, eliminating the need for programming. We conducted two case studies and in-depth interviews with target users to evaluate our approach. The results demonstrate the effectiveness and usability of our approach in allowing users to conveniently enable flexible interactions for static visualizations.",
    "authors": [
      "Can Liu",
      "Jaeuk Lee",
      "Tianhe Chen",
      "Zhibang Jiang",
      "Xiaolin Wen",
      "Yong Wang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17736v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17736v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2601.17958v1",
    "title": "TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors",
    "summary": "Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.",
    "authors": [
      "Ido Andrew Atad",
      "Itamar Zimerman",
      "Shahar Katz",
      "Lior Wolf"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17958v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17958v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.17912v1",
    "title": "Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN",
    "summary": "Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.",
    "authors": [
      "Qinyi Liu",
      "Mohammad Khalil",
      "Naman Goel"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17912v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17912v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.17877v1",
    "title": "Comparative Algorithmic Governance of Public Health Instruments across India, EU, US and LMICs",
    "summary": "The study investigates the juridico-technological architecture of international public health instruments, focusing on their implementation across India, the European Union, the United States and low- and middle-income countries (LMICs), particularly in Sub-Saharan Africa. It addresses a research lacuna: the insufficient harmonisation between normative health law and algorithmic public health infrastructures in resource-constrained jurisdictions. The principal objective is to assess how artificial intelligence augments implementation of instruments grounded in IHR 2005 and the WHO FCTC while identifying doctrinal and infrastructural bottlenecks. Using comparative doctrinal analysis and legal-normative mapping, the study triangulates legislative instruments, WHO monitoring frameworks, AI systems including BlueDot, Aarogya Setu and EIOS, and compliance metrics. Preliminary results show that AI has improved early detection, surveillance precision and responsiveness in high-capacity jurisdictions, whereas LMICs face infrastructural deficits, data privacy gaps and fragmented legal scaffolding. The findings highlight the relevance of the EU Artificial Intelligence Act and GDPR as regulatory prototypes for health-oriented algorithmic governance and contrast them with embryonic AI integration and limited internet penetration in many LMICs. The study argues for embedding AI within a rights-compliant, supranationally coordinated regulatory framework to secure equitable health outcomes and stronger compliance. It proposes a model for algorithmic treaty-making inspired by FCTC architecture and calls for WHO-led compliance mechanisms modelled on the WTO Dispute Settlement Body to enhance pandemic preparedness, surveillance equity and transnational governance resilience.",
    "authors": [
      "Sahibpreet Singh"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17877v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17877v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.17711v1",
    "title": "CaSNet: Compress-and-Send Network Based Multi-Device Speech Enhancement Model for Distributed Microphone Arrays",
    "summary": "Distributed microphone array (DMA) is a promising next-generation platform for speech interaction, where speech enhancement (SE) is still required to improve the speech quality in noisy cases. Existing SE methods usually first gather raw waveforms at a fusion center (FC) from all devices and then design a multi-microphone model, causing high bandwidth and energy costs. In this work, we propose a \\emph{Compress-and-Send Network (CaSNet)} for resource-constrained DMAs, where one microphone serves as the FC and reference. Each of other devices encodes the measured raw data into a feature matrix, which is then compressed by singular value decomposition (SVD) to produce a more compact representation. The received features at the FC are aligned via cross window query with respect to the reference, followed by neural decoding to yield spatially coherent enhanced speech. Experiments on multiple datasets show that the proposed CaSNet can save the data amount with a negligible impact on the performance compared to the uncompressed case. The reproducible code is available at https://github.com/Jokejiangv/CaSNet.",
    "authors": [
      "Chengqian Jiang",
      "Jie Zhang",
      "Haoyin Yan"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17711v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17711v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2601.17944v1",
    "title": "Credit Fairness: Online Fairness In Shared Resource Pools",
    "summary": "We consider a setting in which a group of agents share resources that must be allocated among them in each discrete time period. Agents have time-varying demands and derive constant marginal utility from each unit of resource received up to their demand, with zero utility for any additional resources. In this setting, it is known that independently maximizing the minimum utility in each round satisfies sharing incentives (agents weakly prefer participating in the mechanism to not participating), strategyproofness (agents have no incentive to misreport their demands), and Pareto efficiency (Freeman et al. 2018). However, recent work (Vuppalapati et al. 2023) has shown that this max-min mechanism can lead to large disparities in the total resources received by agents, even when they have the same average demand. In this paper, we introduce credit fairness, a strengthening of sharing incentives that ensures agents who lend resources in early rounds are able to recoup them in later rounds. Credit fairness can be achieved in conjunction with either Pareto efficiency or strategyproofness, but not both. We propose a mechanism that is credit fair and Pareto efficient, and we evaluate its performance in a computational resource-sharing setting.",
    "authors": [
      "Seyed Majid Zahedi",
      "Rupert Freeman"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.OS"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17944v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17944v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17920v1",
    "title": "Agentic AI for Self-Driving Laboratories in Soft Matter: Taxonomy, Benchmarks,and Open Challenges",
    "summary": "Self-driving laboratories (SDLs) close the loop between experiment design, automated execution, and data-driven decision making, and they provide a demanding testbed for agentic AI under expensive actions, noisy and delayed feedback, strict feasibility and safety constraints, and non-stationarity. This survey uses soft matter as a representative setting but focuses on the AI questions that arise in real laboratories. We frame SDL autonomy as an agent environment interaction problem with explicit observations, actions, costs, and constraints, and we use this formulation to connect common SDL pipelines to established AI principles. We review the main method families that enable closed loop experimentation, including Bayesian optimization and active learning for sample efficient experiment selection, planning and reinforcement learning for long horizon protocol optimization, and tool using agents that orchestrate heterogeneous instruments and software. We emphasize verifiable and provenance aware policies that support debugging, reproducibility, and safe operation. We then propose a capability driven taxonomy that organizes systems by decision horizon, uncertainty modeling, action parameterization, constraint handling, failure recovery, and human involvement. To enable meaningful comparison, we synthesize benchmark task templates and evaluation metrics that prioritize cost aware performance, robustness to drift, constraint violation behavior, and reproducibility. Finally, we distill lessons from deployed SDLs and outline open challenges in multi-modal representation, calibrated uncertainty, safe exploration, and shared benchmark infrastructure.",
    "authors": [
      "Xuanzhou Chen",
      "Audrey Wang",
      "Stanley Yin",
      "Hanyang Jiang",
      "Dong Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17920v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17920v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17773v1",
    "title": "MarketGANs: Multivariate financial time-series data augmentation using generative adversarial networks",
    "summary": "This paper introduces MarketGAN, a factor-based generative framework for high-dimensional asset return generation under severe data scarcity. We embed an explicit asset-pricing factor structure as an economic inductive bias and generate returns as a single joint vector, thereby preserving cross-sectional dependence and tail co-movement alongside inter-temporal dynamics. MarketGAN employs generative adversarial learning with a temporal convolutional network (TCN) backbone, which models stochastic, time-varying factor loadings and volatilities and captures long-range temporal dependence. Using daily returns of large U.S. equities, we find that MarketGAN more closely matches empirical stylized facts of asset returns, including heavy-tailed marginal distributions, volatility clustering, leverage effects, and, most notably, high-dimensional cross-sectional correlation structures and tail co-movement across assets, than conventional factor-model-based bootstrap approaches. In portfolio applications, covariance estimates derived from MarketGAN-generated samples outperform those derived from other methods when factor information is at least weakly informative, demonstrating tangible economic value.",
    "authors": [
      "Jeonggyu Huh",
      "Seungwon Jeong",
      "Hyun-Gyoon Kim",
      "Hyeng Keun Koo",
      "Byung Hwa Lim"
    ],
    "categories": [
      "q-fin.ST",
      "cs.LG",
      "econ.EM"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17773v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17773v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17713v1",
    "title": "FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices",
    "summary": "With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.",
    "authors": [
      "Kaile Wang",
      "Jiannong Cao",
      "Yu Yang",
      "Xiaoyin Li",
      "Yinfeng Cao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17713v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17713v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17703v1",
    "title": "An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays",
    "summary": "Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.",
    "authors": [
      "Nikhil Kadivar",
      "Guansheng Li",
      "Jianlu Zheng",
      "John M. Higgins",
      "Ming Dao",
      "George Em Karniadakis",
      "Mengjia Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17703v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17703v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17680v1",
    "title": "$\\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts",
    "summary": "The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\\% in accuracy over conventional MoE.",
    "authors": [
      "Shota Takashiro",
      "Takeshi Kojima",
      "Shohei Taniguchi",
      "Yusuke Iwasawa",
      "Yutaka Matsuo"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17680v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17680v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17670v1",
    "title": "Grammar-Aware Literate Generative Mathematical Programming with Compiler-in-the-Loop",
    "summary": "This work investigates generative mathematical programming through the lens of Algebraic Modelling Languages (AMLs) and compiler-guided model synthesis. By leveraging PyOPL, an OPL-like AML compiler that provides detailed syntax diagnostics, we introduce SyntAGM, an end-to-end system that translates natural language problem descriptions into PyOPL models via a generate--compile--assess--revise loop. SyntAGM is grammar-aware thanks to in-context exposure to the PyOPL BNF grammar, and benefits from few-shot retrieval of literate PyOPL model exemplars. To obtain a valid PyOPL model that matches the problem description, SyntAGM mobilises compiler feedback and an LLM-based alignment judge. In a comparative study against established prompting baselines SyntAGM achieves competitive accuracy with superior token, cost, and latency profiles.",
    "authors": [
      "Roberto Rossi",
      "Steven D. Prestwich"
    ],
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17670v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17670v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2601.17933v1",
    "title": "Dissipative Learning: A Framework for Viable Adaptive Systems",
    "summary": "We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.   A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.   Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.",
    "authors": [
      "Laurent Caraffa"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17933v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17933v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17835v1",
    "title": "Geometry-Grounded Gaussian Splatting",
    "summary": "Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.",
    "authors": [
      "Baowen Zhang",
      "Chenxing Jiang",
      "Heng Li",
      "Shaojie Shen",
      "Ping Tan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17835v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17835v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17802v1",
    "title": "Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data",
    "summary": "Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.",
    "authors": [
      "A. Brawanski",
      "Th. Schaffer",
      "F. Raab",
      "K. -M. Schebesch",
      "M. Schrey",
      "Chr. Doenitz",
      "A. M. Tomé",
      "E. W. Lang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17802v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17802v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17781v1",
    "title": "Controlling Reading Ease with Gaze-Guided Text Generation",
    "summary": "The way our eyes move while reading can tell us about the cognitive effort required to process the text. In the present study, we use this fact to generate texts with controllable reading ease. Our method employs a model that predicts human gaze patterns to steer language model outputs towards eliciting certain reading behaviors. We evaluate the approach in an eye-tracking experiment with native and non-native speakers of English. The results demonstrate that the method is effective at making the generated texts easier or harder to read, measured both in terms of reading times and perceived difficulty of the texts. A statistical analysis reveals that the changes in reading behavior are mostly due to features that affect lexical processing. Possible applications of our approach include text simplification for information accessibility and generation of personalized educational material for language learning.",
    "authors": [
      "Andreas Säuberli",
      "Darja Jepifanova",
      "Diego Frassinelli",
      "Barbara Plank"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17781v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17781v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17768v1",
    "title": "LLM-42: Enabling Determinism in LLM Inference with Verified Speculation",
    "summary": "In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.   Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.",
    "authors": [
      "Raja Gond",
      "Aditya K Kamath",
      "Arkaprava Basu",
      "Ramachandran Ramjee",
      "Ashish Panwar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17768v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17768v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17733v1",
    "title": "Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles",
    "summary": "Boundary Representation (B-Rep) is the widely adopted standard   in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.   We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.",
    "authors": [
      "Junran Lu",
      "Yuanqi Li",
      "Hengji Li",
      "Jie Guo",
      "Yanwen Guo"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17733v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17733v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17644v1",
    "title": "A Systemic Evaluation of Multimodal RAG Privacy",
    "summary": "The growing adoption of multimodal Retrieval-Augmented Generation (mRAG) pipelines for vision-centric tasks (e.g. visual QA) introduces important privacy challenges. In particular, while mRAG provides a practical capability to connect private datasets to improve model performance, it risks the leakage of private information from these datasets during inference. In this paper, we perform an empirical study to analyze the privacy risks inherent in the mRAG pipeline observed through standard model prompting. Specifically, we implement a case study that attempts to infer the inclusion of a visual asset, e.g. image, in the mRAG, and if present leak the metadata, e.g. caption, related to it. Our findings highlight the need for privacy-preserving mechanisms and motivate future research on mRAG privacy.",
    "authors": [
      "Ali Al-Lawati",
      "Suhang Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17644v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17644v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2601.17865v1",
    "title": "D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models",
    "summary": "The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.",
    "authors": [
      "Jia Gu",
      "Liang Pang",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17865v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17865v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.17744v1",
    "title": "Faramesh: A Protocol-Agnostic Execution Control Plane for Autonomous Agent Systems",
    "summary": "Autonomous agent systems increasingly trigger real-world side effects: deploying infrastructure, modifying databases, moving money, and executing workflows. Yet most agent stacks provide no mandatory execution checkpoint where organizations can deterministically permit, deny, or defer an action before it changes reality. This paper introduces Faramesh, a protocol-agnostic execution control plane that enforces execution-time authorization for agent-driven actions via a non-bypassable Action Authorization Boundary (AAB). Faramesh canonicalizes agent intent into a Canonical Action Representation (CAR), evaluates actions deterministically against policy and state, and issues a decision artifact (PERMIT/DEFER/DENY) that executors must validate prior to execution. The system is designed to be framework- and model-agnostic, supports multi-agent and multi-tenant deployments, and remains independent of transport protocols (e.g., MCP). Faramesh further provides decision-centric, append-only provenance logging keyed by canonical action hashes, enabling auditability, verification, and deterministic replay without re-running agent reasoning. We show how these primitives yield enforceable, predictable governance for autonomous execution while avoiding hidden coupling to orchestration layers or observability-only approaches.",
    "authors": [
      "Amjad Fatmi"
    ],
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17744v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17744v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.17743v1",
    "title": "Video Compression with Hierarchical Temporal Neural Representation",
    "summary": "Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.",
    "authors": [
      "Jun Zhu",
      "Xinfeng Zhang",
      "Lv Tang",
      "Junhao Jiang",
      "Gai Zhang",
      "Jia Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17743v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17743v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.17716v1",
    "title": "Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games",
    "summary": "Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.",
    "authors": [
      "Daniel M. Pedrozo",
      "Telma W. de L. Soares",
      "Bryan L. M. de Oliveira"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17716v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17716v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2601.17923v1",
    "title": "Learning Transferable Skills in Action RPGs via Directed Skill Graphs and Selective Adaptation",
    "summary": "Lifelong agents should expand their competence over time without retraining from scratch or overwriting previously learned behaviors. We investigate this in a challenging real-time control setting (Dark Souls III) by representing combat as a directed skill graph and training its components in a hierarchical curriculum. The resulting agent decomposes control into five reusable skills: camera control, target lock-on, movement, dodging, and a heal-attack decision policy, each optimized for a narrow responsibility. This factorization improves sample efficiency by reducing the burden on any single policy and supports selective post-training: when the environment shifts from Phase 1 to Phase 2, only a subset of skills must be adapted, while upstream skills remain transferable. Empirically, we find that targeted fine-tuning of just two skills rapidly recovers performance under a limited interaction budget, suggesting that skill-graph curricula together with selective fine-tuning offer a practical pathway toward evolving, continually learning agents in complex real-time environments.",
    "authors": [
      "Ali Najar"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17923v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17923v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17910v1",
    "title": "Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization",
    "summary": "Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.",
    "authors": [
      "Aaron R. Flouro",
      "Shawn P. Chadwick"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17910v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17910v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17892v1",
    "title": "Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis",
    "summary": "Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.",
    "authors": [
      "Sahibpreet Singh",
      "Manjit Singh"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17892v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17892v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17879v1",
    "title": "Self-Manager: Parallel Agent Loop for Long-form Deep Research",
    "summary": "Long-form deep research requires multi-faceted investigations over extended horizons to get a comprehensive report. When handling such complex tasks, existing agents manage context at the subtask level to overcome linear context accumulation and information loss. However, they still adhere to a single context window and sequential execution paradigm, which results in mutual interference and blocking behavior, restricting scalability and adaptability. To address this issue, this paper introduces Self-Manager, a parallel agent loop that enables asynchronous and concurrent execution. The main thread can create multiple subthreads, each with its own isolated context, and manage them iteratively through Thread Control Blocks, allowing for more focused and flexible parallel agent execution. To assess its effectiveness, we benchmark Self-Manager on DeepResearch Bench, where it consistently outperforms existing single-agent loop baselines across all metrics. Furthermore, we conduct extensive analytical experiments to demonstrate the necessity of Self-Manager's design choices, as well as its advantages in contextual capacity, efficiency, and generalization.",
    "authors": [
      "Yilong Xu",
      "Zhi Zheng",
      "Xiang Long",
      "Yujun Cai",
      "Yiwei Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17879v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17879v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17807v1",
    "title": "An autonomous living database for perovskite photovoltaics",
    "summary": "Scientific discovery is severely bottlenecked by the inability of manual curation to keep pace with exponential publication rates. This creates a widening knowledge gap. This is especially stark in photovoltaics, where the leading database for perovskite solar cells has been stagnant since 2021 despite massive ongoing research output. Here, we resolve this challenge by establishing an autonomous, self-updating living database (PERLA). Our pipeline integrates large language models with physics-aware validation to extract complex device data from the continuous literature stream, achieving human-level precision (>90%) and eliminating annotator variance. By employing this system on the previously inaccessible post-2021 literature, we uncover critical evolutionary trends hidden by data lag: the field has decisively shifted toward inverted architectures employing self-assembled monolayers and formamidinium-rich compositions, driving a clear trajectory of sustained voltage loss reduction. PERLA transforms static publications into dynamic knowledge resources that enable data-driven discovery to operate at the speed of publication.",
    "authors": [
      "Sherjeel Shabih",
      "Hampus Näsström",
      "Sharat Patil",
      "Asmin Askin",
      "Keely Dodd-Clements",
      "Jessica Helisa Hautrive Rossato",
      "Hugo Gajardoni de Lemos",
      "Yuxin Liu",
      "Florian Mathies",
      "Natalia Maticiuc",
      "Rico Meitzner",
      "Edgar Nandayapa",
      "Juan José Patiño López",
      "Yaru Wang",
      "Lauri Himanen",
      "Eva Unger",
      "T. Jesper Jacobsson",
      "José A. Márquez",
      "Kevin Maik Jablonka"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17807v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17807v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17791v1",
    "title": "Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation",
    "summary": "Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\\pm$ 0.10, MAPE = 2.22 $\\pm$ 0.56 \\%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.",
    "authors": [
      "Rabin Dulal",
      "Wenfeng Jia",
      "Lihong Zheng",
      "Jane Quinn"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17791v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17791v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17678v1",
    "title": "DIML: Differentiable Inverse Mechanism Learning from Behaviors of Multi-Agent Learning Trajectories",
    "summary": "We study inverse mechanism learning: recovering an unknown incentive-generating mechanism from observed strategic interaction traces of self-interested learning agents. Unlike inverse game theory and multi-agent inverse reinforcement learning, which typically infer utility/reward parameters inside a structured mechanism, our target includes unstructured mechanism -- a (possibly neural) mapping from joint actions to per-agent payoffs. Unlike differentiable mechanism design, which optimizes mechanisms forward, we infer mechanisms from behavior in an observational setting. We propose DIML, a likelihood-based framework that differentiates through a model of multi-agent learning dynamics and uses the candidate mechanism to generate counterfactual payoffs needed to predict observed actions. We establish identifiability of payoff differences under a conditional logit response model and prove statistical consistency of maximum likelihood estimation under standard regularity conditions. We evaluate DIML with simulated interactions of learning agents across unstructured neural mechanisms, congestion tolling, public goods subsidies, and large-scale anonymous games. DIML reliably recovers identifiable incentive differences and supports counterfactual prediction, where its performance rivals tabular enumeration oracle in small environments and its convergence scales to large, hundred-participant environments. Code to reproduce our experiments is open-sourced.",
    "authors": [
      "Zhiyu An",
      "Wan Du"
    ],
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17678v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17678v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17666v1",
    "title": "Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting",
    "summary": "Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.",
    "authors": [
      "Xinyue Pan",
      "Yuhao Chen",
      "Fengqing Zhu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17666v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17666v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2601.17947v1",
    "title": "FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos",
    "summary": "Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.   Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\\sim 1.5\\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.",
    "authors": [
      "Bora Yimenicioglu",
      "Vishal Manikanden"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17947v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17947v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.17862v1",
    "title": "Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment",
    "summary": "Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.",
    "authors": [
      "Jingsong Xia",
      "Siqi Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17862v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17862v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.17723v1",
    "title": "Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study",
    "summary": "Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.",
    "authors": [
      "Tayyab Nasir",
      "Daochang Liu",
      "Ajmal Mian"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17723v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17723v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.17690v1",
    "title": "Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance",
    "summary": "Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.",
    "authors": [
      "Ziling Gong",
      "Yunyan Ouyang",
      "Iram Kamdar",
      "Melody Ma",
      "Hongjie Chen",
      "Franck Dernoncourt",
      "Ryan A. Rossi",
      "Nesreen K. Ahmed"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17690v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17690v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.17667v1",
    "title": "Entropic Risk-Aware Monte Carlo Tree Search",
    "summary": "We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \\textit{risk-aware} Markov decision processes (MDPs) with \\textit{entropic risk measure} (ERM) objectives. We provide a \\textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \\textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \\textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.",
    "authors": [
      "Pedro P. Santos",
      "Jacopo Silvestrin",
      "Alberto Sardinha",
      "Francisco S. Melo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17667v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17667v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.17654v1",
    "title": "Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training",
    "summary": "The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.   We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.",
    "authors": [
      "Ruofan Wu",
      "Jae-Won Chung",
      "Mosharaf Chowdhury"
    ],
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17654v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17654v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2601.17800v1",
    "title": "Differentiable Integer Linear Programming is not Differentiable & it's not a mere technical problem",
    "summary": "We show how the differentiability method employed in the paper ``Differentiable Integer Linear Programming'', Geng, et al., 2025 as shown in its theorem 5 is incorrect. Moreover, there already exists some downstream work that inherits the same error. The underlying reason comes from that, though being continuous in expectation, the surrogate loss is discontinuous in almost every realization of the randomness, for the stochastic gradient descent.",
    "authors": [
      "Thanawat Sornwanee"
    ],
    "categories": [
      "math.OC",
      "cs.LG"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17800v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17800v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2601.17939v1",
    "title": "DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation",
    "summary": "In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.",
    "authors": [
      "Chengkun Sun",
      "Jinqian Pan",
      "Renjie Liang",
      "Zhengkang Fan",
      "Xin Miao",
      "Jiang Bian",
      "Jie Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17939v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17939v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2601.17895v1",
    "title": "Masked Depth Modeling for Spatial Perception",
    "summary": "Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as \"masked\" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.",
    "authors": [
      "Bin Tan",
      "Changjiang Sun",
      "Xiage Qin",
      "Hanat Adai",
      "Zelin Fu",
      "Tianxiang Zhou",
      "Han Zhang",
      "Yinghao Xu",
      "Xing Zhu",
      "Yujun Shen",
      "Nan Xue"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17895v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17895v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2601.17697v1",
    "title": "StyleDecoupler: Generalizable Artistic Style Disentanglement",
    "summary": "Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.",
    "authors": [
      "Zexi Jia",
      "Jinchao Zhang",
      "Jie Zhou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17697v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17697v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2601.17866v1",
    "title": "MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance",
    "summary": "Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.",
    "authors": [
      "Yoonwoo Jeong",
      "Cheng Sun",
      "Yu-Chiang Frank Wang",
      "Minsu Cho",
      "Jaesung Choe"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17866v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17866v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2601.17646v1",
    "title": "A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization",
    "summary": "Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.",
    "authors": [
      "Karim Bounja",
      "Lahcen Laayouni",
      "Abdeljalil Sakat"
    ],
    "categories": [
      "cs.LG",
      "math.FA",
      "math.OC",
      "math.ST"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17646v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17646v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2601.17880v1",
    "title": "Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran",
    "summary": "We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset",
    "authors": [
      "Muhammad Umar Salman",
      "Mohammad Areeb Qazi",
      "Mohammed Talha Alam"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17880v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17880v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2601.17740v1",
    "title": "Learning Sewing Patterns via Latent Flow Matching of Implicit Fields",
    "summary": "Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.",
    "authors": [
      "Cong Cao",
      "Ren Li",
      "Corentin Dumery",
      "Hao Li"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17740v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17740v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2601.17900v1",
    "title": "Revisiting 3D Reconstruction Kernels as Low-Pass Filters",
    "summary": "3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.",
    "authors": [
      "Shengjun Zhang",
      "Min Chen",
      "Yibo Wei",
      "Mingyu Dong",
      "Yueqi Duan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17900v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17900v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2601.17855v1",
    "title": "A Universal Load Balancing Principle and Its Application to Large Language Model Serving",
    "summary": "Load balancing-the allocation of work across parallel resources to reduce delay, energy and cost-is a pervasive challenge in science and engineering, from large-scale simulation and data processing to cloud and manufacturing operations. Motivated by the emerging bottleneck in large language model (LLM) serving, we study a particularly stringent regime of load balancing that arises in barrier-synchronized, stateful systems: work cannot be freely migrated and progress is gated by the slowest participant at each step, so heterogeneity and temporal drift in workloads create persistent stragglers and substantial idle time. LLM serving under data-parallel decoding provides a prominent modern instance: in production traces, barrier-induced idle can exceed 40% of compute time per decode step. Here we develop a universal load-balancing principle, which admits a step-wise finite-horizon integer-optimization formulation and yields worst-case guarantees: across LLM decode models and a broader class of non-decreasing workload drift processes, it reduces long-run imbalance by a factor that grows with batch size and system scale. Extensive experiments corroborate the theory, showing substantial improvements in throughput and latency together with reductions in energy consumption. These results provide a general, theoretically grounded framework for load balancing, with immediate implications for sustainable LLM serving and broad relevance to other synchronization-gated resource-allocation problems.",
    "authors": [
      "Zixi Chen",
      "Tianci Bu",
      "Chendong Song",
      "Xin Lu",
      "Yinyu Ye",
      "Zijie Zhou"
    ],
    "categories": [
      "cs.DC",
      "stat.ML"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17855v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17855v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2601.17808v1",
    "title": "Motif Diversity in Human Liver ChIP-seq Data Using MAP-Elites",
    "summary": "Motif discovery is a core problem in computational biology, traditionally formulated as a likelihood optimization task that returns a single dominant motif from a DNA sequence dataset. However, regulatory sequence data admit multiple plausible motif explanations, reflecting underlying biological heterogeneity. In this work, we frame motif discovery as a quality-diversity problem and apply the MAP-Elites algorithm to evolve position weight matrix motifs under a likelihood-based fitness objective while explicitly preserving diversity across biologically meaningful dimensions. We evaluate MAP-Elites using three complementary behavioral characterizations that capture trade-offs between motif specificity, compositional structure, coverage, and robustness. Experiments on human CTCF liver ChIP-seq data aligned to the human reference genome compare MAP-Elites against a standard motif discovery tool, MEME, under matched evaluation criteria across stratified dataset subsets. Results show that MAP-Elites recovers multiple high-quality motif variants with fitness comparable to MEME's strongest solutions while revealing structured diversity obscured by single-solution approaches.",
    "authors": [
      "Alejandro Medina",
      "Mary Lauren Benton"
    ],
    "categories": [
      "cs.NE",
      "q-bio.GN"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17808v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17808v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.43
  },
  {
    "arxiv_id": "2601.17720v1",
    "title": "Advancing Structured Priors for Sparse-Voxel Surface Reconstruction",
    "summary": "Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.",
    "authors": [
      "Ting-Hsun Chi",
      "Chu-Rong Chen",
      "Chi-Tun Hsu",
      "Hsuan-Ting Lin",
      "Sheng-Yu Huang",
      "Cheng Sun",
      "Yu-Chiang Frank Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2026-01-25",
    "url": "https://arxiv.org/abs/2601.17720v1",
    "pdf_url": "https://arxiv.org/pdf/2601.17720v1.pdf",
    "date": "2026-01-27",
    "source": "arxiv",
    "research_score": 0.42
  }
]