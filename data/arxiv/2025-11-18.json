[
  {
    "arxiv_id": "2511.13219v1",
    "title": "FoleyBench: A Benchmark For Video-to-Audio Models",
    "summary": "Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench",
    "authors": [
      "Satvik Dixit",
      "Koichi Saito",
      "Zhi Zhong",
      "Yuki Mitsufuji",
      "Chris Donahue"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13219v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13219v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.9
  },
  {
    "arxiv_id": "2511.13391v1",
    "title": "Finding Kissing Numbers with Game-theoretic Reinforcement Learning",
    "summary": "Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.",
    "authors": [
      "Chengdong Ma",
      "Théo Tao Zhaowei",
      "Pengyu Li",
      "Minghao Liu",
      "Haojun Chen",
      "Zihao Mao",
      "Yuan Cheng",
      "Yuan Qi",
      "Yaodong Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13391v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13391v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.8
  },
  {
    "arxiv_id": "2511.13193v1",
    "title": "Cost-Effective Communication: An Auction-based Method for Language Agent Interaction",
    "summary": "Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient \"free-for-all\" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that \"free\" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.",
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Chengpei Tang",
      "Jian Wang",
      "Keze Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13193v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13193v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.79
  },
  {
    "arxiv_id": "2511.13198v1",
    "title": "ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer",
    "summary": "Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.",
    "authors": [
      "Zhixin Ou",
      "Peng Liang",
      "Jianchen Han",
      "Baihui Liu",
      "Linbo Qiao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13198v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13198v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2511.13065v1",
    "title": "RobustGait: Robustness Analysis for Appearance Based Gait Recognition",
    "summary": "Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.",
    "authors": [
      "Reeshoon Sayera",
      "Akash Kumar",
      "Sirshapan Mitra",
      "Prudvi Kamtam",
      "Yogesh S Rawat"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13065v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13065v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.78
  },
  {
    "arxiv_id": "2511.13444v1",
    "title": "Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes",
    "summary": "Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.",
    "authors": [
      "Zhipeng Ma",
      "Bo Nørregaard Jørgensen",
      "Zheng Grace Ma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13444v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13444v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2511.13043v1",
    "title": "Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training",
    "summary": "Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a \"CoT-augmented state prediction\" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.",
    "authors": [
      "Xinyuan Zhou",
      "Yi Lei",
      "Xiaoyu Zhou",
      "Jingyi Sun",
      "Yu Zhu",
      "Zhongyi Ye",
      "Weitai Zhang",
      "Quan Liu",
      "Si Wei",
      "Cong Liu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13043v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13043v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.77
  },
  {
    "arxiv_id": "2511.13335v1",
    "title": "AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects",
    "summary": "The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.",
    "authors": [
      "Maram Alharbi",
      "Salmane Chafik",
      "Saad Ezzini",
      "Ruslan Mitkov",
      "Tharindu Ranasinghe",
      "Hansi Hettiarachchi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13335v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13335v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.13319v1",
    "title": "Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs",
    "summary": "Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.   In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.",
    "authors": [
      "Chelsea McMurray",
      "Hayder Tirmazi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13319v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13319v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.13125v1",
    "title": "Region-Point Joint Representation for Effective Trajectory Similarity Learning",
    "summary": "Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \\textbf{RePo}, a novel method that jointly encodes \\textbf{Re}gion-wise and \\textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\\% over SOTA baselines across all evaluation metrics.",
    "authors": [
      "Hao Long",
      "Silin Zhou",
      "Lisi Chen",
      "Shuo Shang"
    ],
    "categories": [
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13125v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13125v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.13103v1",
    "title": "Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions",
    "summary": "Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.",
    "authors": [
      "Vidur Sinha",
      "Muhammed Ustaomeroglu",
      "Guannan Qu"
    ],
    "categories": [
      "cs.LG",
      "cs.MA",
      "eess.SY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13103v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13103v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.76
  },
  {
    "arxiv_id": "2511.13399v1",
    "title": "TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing",
    "summary": "Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the \"SCB Group\", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent \"shortcut\" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS",
    "authors": [
      "Yuchen Bao",
      "Yiting Wang",
      "Wenjian Huang",
      "Haowei Wang",
      "Shen Chen",
      "Taiping Yao",
      "Shouhong Ding",
      "Jianguo Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13399v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13399v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.13078v1",
    "title": "A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning",
    "summary": "Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.",
    "authors": [
      "Liuyi Jin",
      "Pasan Gunawardena",
      "Amran Haroon",
      "Runzhi Wang",
      "Sangwoo Lee",
      "Radu Stoleru",
      "Michael Middleton",
      "Zepeng Huo",
      "Jeeeun Kim",
      "Jason Moats"
    ],
    "categories": [
      "cs.LG",
      "eess.AS",
      "eess.IV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13078v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13078v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.75
  },
  {
    "arxiv_id": "2511.13145v1",
    "title": "Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks",
    "summary": "The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.",
    "authors": [
      "Cesar Portocarrero Rodriguez",
      "Laura Vandeweyen",
      "Yosuke Yamamoto"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13145v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13145v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.74
  },
  {
    "arxiv_id": "2511.13356v1",
    "title": "Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping",
    "summary": "Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .",
    "authors": [
      "Lei Wang",
      "Yulong Tian",
      "Hao Han",
      "Fengyuan Xu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13356v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13356v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.13116v1",
    "title": "Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning",
    "summary": "Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.",
    "authors": [
      "Qipeng Song",
      "Nan Yang",
      "Ziqi Xu",
      "Yue Li",
      "Wei Shao",
      "Feng Xia"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13116v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13116v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.13062v1",
    "title": "Self-Adaptive Graph Mixture of Models",
    "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.",
    "authors": [
      "Mohit Meena",
      "Yash Punjabi",
      "Abhishek A",
      "Vishal Sharma",
      "Mahesh Chandran"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13062v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13062v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.73
  },
  {
    "arxiv_id": "2511.13351v1",
    "title": "Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning",
    "summary": "Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.",
    "authors": [
      "Xinlan Wu",
      "Bin Zhu",
      "Feng Han",
      "Pengkun Jiao",
      "Jingjing Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13351v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13351v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.13254v1",
    "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.",
    "authors": [
      "Shalini Maiti",
      "Amar Budhiraja",
      "Bhavul Gauri",
      "Gaurav Chaurasia",
      "Anton Protopopov",
      "Alexis Audran-Reiss",
      "Michael Slater",
      "Despoina Magka",
      "Tatiana Shavrina",
      "Roberta Raileanu",
      "Yoram Bachrach"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13254v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13254v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.72
  },
  {
    "arxiv_id": "2511.13541v1",
    "title": "Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries",
    "summary": "A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.",
    "authors": [
      "Yue Hou",
      "Ruomei Liu",
      "Yingke Su",
      "Junran Wu",
      "Ke Xu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13541v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13541v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13539v1",
    "title": "BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse",
    "summary": "Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.",
    "authors": [
      "Yuanchao Wang",
      "Tian Qin",
      "Eduardo Valle",
      "Bruno Abrahao"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13539v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13539v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13478v1",
    "title": "Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling",
    "summary": "Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.",
    "authors": [
      "Adam Hazimeh",
      "Ke Wang",
      "Mark Collier",
      "Gilles Baechler",
      "Efi Kokiopoulou",
      "Pascal Frossard"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13478v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13478v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13442v1",
    "title": "Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline",
    "summary": "With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.",
    "authors": [
      "Rui Zuo",
      "Qinyue Tong",
      "Zhe-Ming Lu",
      "Ziqian Lu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13442v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13442v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13373v1",
    "title": "A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs",
    "summary": "Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.",
    "authors": [
      "Prakrit Timilsina",
      "Anuj Nepal",
      "Rajan Kadel",
      "Robin Doss"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13373v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13373v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13322v1",
    "title": "Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning",
    "summary": "Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.",
    "authors": [
      "Senne Deproost",
      "Dennis Steckelmacher",
      "Ann Nowé"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13322v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13322v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13234v1",
    "title": "MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing",
    "summary": "Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.",
    "authors": [
      "Boris Kriuk"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13234v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13234v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13150v1",
    "title": "Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification",
    "summary": "Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.",
    "authors": [
      "Rifen Lin",
      "Alex Jinpeng Wang",
      "Jiawei Mo",
      "Min Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13150v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13150v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13071v1",
    "title": "Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers",
    "summary": "Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.",
    "authors": [
      "Michal Levin",
      "Itzik Klein"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13071v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13071v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13063v1",
    "title": "FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation",
    "summary": "Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.",
    "authors": [
      "Zhenghua Li",
      "Hang Chen",
      "Zihao Sun",
      "Kai Li",
      "Xiaolin Hu"
    ],
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13063v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13063v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13044v1",
    "title": "Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data",
    "summary": "Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.",
    "authors": [
      "Rosario Napoli",
      "Giovanni Lonia",
      "Antonio Celesti",
      "Massimo Villari",
      "Maria Fazio"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13044v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13044v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.7
  },
  {
    "arxiv_id": "2511.13545v1",
    "title": "Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks",
    "summary": "The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.",
    "authors": [
      "Md. Iqbal Hossain",
      "Afia Sajeeda",
      "Neeresh Kumar Perla",
      "Ming Shao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13545v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13545v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13394v1",
    "title": "Fast and Robust Simulation-Based Inference With Optimization Monte Carlo",
    "summary": "Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.",
    "authors": [
      "Vasilis Gkolemis",
      "Christos Diou",
      "Michael Gutmann"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13394v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13394v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13223v1",
    "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
    "summary": "Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.",
    "authors": [
      "Yuxiang Zhang",
      "Zhengxu Yu",
      "Weihang Pan",
      "Zhongming Jin",
      "Qiang Fu",
      "Deng Cai",
      "Binbin Lin",
      "Jieping Ye"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13223v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13223v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13137v1",
    "title": "Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition",
    "summary": "Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\\text{D}^\\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\\text{D}^\\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\\text{D}^\\text{3}$T achieves better performance than existing baselines.",
    "authors": [
      "Yanda Zhu",
      "Yuanyang Zhu",
      "Daoyi Dong",
      "Caihua Chen",
      "Chunlin Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13137v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13137v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.69
  },
  {
    "arxiv_id": "2511.13389v1",
    "title": "Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference",
    "summary": "Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.",
    "authors": [
      "Zhipeng Ma",
      "Bo Nørregaard Jørgensen",
      "Zheng Grace Ma"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13389v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13389v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13293v1",
    "title": "Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval",
    "summary": "Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \\underline{g}enerative \\underline{h}ierarchical \\underline{a}gentic \\underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.",
    "authors": [
      "Chuang Zhao",
      "Hui Tang",
      "Hongke Zhao",
      "Xiaofang Zhou",
      "Xiaomeng Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13293v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13293v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13147v1",
    "title": "OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs",
    "summary": "Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.",
    "authors": [
      "Shaoyuan Chen",
      "Zhixuan Chen",
      "Dawei Yang",
      "Zhihang Yuan",
      "Qiang Wu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13147v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13147v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13144v1",
    "title": "Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching",
    "summary": "Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.",
    "authors": [
      "Jiacheng Cheng",
      "Xu Zhang",
      "Guanghui Qiu",
      "Yifang Zhang",
      "Yinchuan Li",
      "Kaiyuan Feng"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13144v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13144v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13108v1",
    "title": "DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection",
    "summary": "The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.",
    "authors": [
      "Jiazhen Yan",
      "Ziqiang Li",
      "Fan Wang",
      "Boyu Wang",
      "Zhangjie Fu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13108v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13108v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.68
  },
  {
    "arxiv_id": "2511.13540v1",
    "title": "Fairness-Aware Graph Representation Learning with Limited Demographic Information",
    "summary": "Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.",
    "authors": [
      "Zichong Wang",
      "Zhipeng Yin",
      "Liping Yang",
      "Jun Zhuang",
      "Rui Yu",
      "Qingzhao Kong",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13540v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13540v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13290v1",
    "title": "Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment",
    "summary": "Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via \"dropout\" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.",
    "authors": [
      "Jea Kwon",
      "Luiz Felipe Vecchietti",
      "Sungwon Park",
      "Meeyoung Cha"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13290v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13290v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13237v1",
    "title": "Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification",
    "summary": "Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\\geq10\\%$ higher confidence while improving sparsity in $\\geq40\\%$.",
    "authors": [
      "Alan G. Paredes Cetina",
      "Kaouther Benguessoum",
      "Raoni Lourenço",
      "Sylvain Kubler"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13237v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13237v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13221v1",
    "title": "Likelihood-guided Regularization in Attention Based Models",
    "summary": "The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.",
    "authors": [
      "Mohamed Salem",
      "Inyoung Kim"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13221v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13221v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13118v1",
    "title": "Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction",
    "summary": "Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.",
    "authors": [
      "Quanjiang Guo",
      "Sijie Wang",
      "Jinchuan Zhang",
      "Ben Zhang",
      "Zhao Kang",
      "Ling Tian",
      "Ke Yan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13118v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13118v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13105v1",
    "title": "PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking",
    "summary": "Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.",
    "authors": [
      "Seungjae Kim",
      "SeungJoon Lee",
      "MyeongAh Cho"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13105v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13105v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13081v1",
    "title": "Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations",
    "summary": "Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise (\"Why this prediction?\") and contrastive (\"Why this and not an alternative?\") explanations.Granularity: Ranging from fine-grained class-level (e.g., \"Why Husky?\") to coarse-grained group-level (e.g., \"Why Dog?\") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.",
    "authors": [
      "Yehonatan Elisha",
      "Seffi Cohen",
      "Oren Barkan",
      "Noam Koenigstein"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13081v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13081v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.67
  },
  {
    "arxiv_id": "2511.13465v1",
    "title": "AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate",
    "summary": "Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.",
    "authors": [
      "Meng Zhu",
      "Quan Xiao",
      "Weidong Min"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13465v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13465v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13306v1",
    "title": "DAP: A Discrete-token Autoregressive Planner for Autonomous Driving",
    "summary": "Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.",
    "authors": [
      "Bowen Ye",
      "Bin Zhang",
      "Hang Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13306v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13306v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13283v1",
    "title": "TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing",
    "summary": "Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.",
    "authors": [
      "Jongha Kim",
      "Minseong Bae",
      "Sanghyeok Lee",
      "Jinsung Yoon",
      "Hyunwoo J. Kim"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13283v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13283v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13208v1",
    "title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer",
    "summary": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \\textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet",
    "authors": [
      "Yonghui Yu",
      "Jiahang Cai",
      "Xun Wang",
      "Wenwu Yang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13208v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13208v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13159v1",
    "title": "Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis",
    "summary": "Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.",
    "authors": [
      "Zaara Zabeen Arpa",
      "Sadnam Sakib Apurbo",
      "Nazia Karim Khan Oishee",
      "Ajwad Abrar"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13159v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13159v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13124v1",
    "title": "Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges",
    "summary": "Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.",
    "authors": [
      "Changxi Chi",
      "Yufei Huang",
      "Jun Xia",
      "Jiangbin Zheng",
      "Yunfan Liu",
      "Zelin Zang",
      "Stan Z. Li"
    ],
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13124v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13124v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13095v1",
    "title": "BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models",
    "summary": "We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.",
    "authors": [
      "Chuyuan Li",
      "Giuseppe Carenini"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13095v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13095v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.66
  },
  {
    "arxiv_id": "2511.13418v1",
    "title": "Exploring Multi-Table Retrieval Through Iterative Search",
    "summary": "Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.",
    "authors": [
      "Allaa Boutaleb",
      "Bernd Amann",
      "Rafael Angarita",
      "Hubert Naacke"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13418v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13418v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13408v1",
    "title": "Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility",
    "summary": "Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus\" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.",
    "authors": [
      "Zhenyu Chen",
      "Yuguo Shao",
      "Zhengwei Liu",
      "Zhaohui Wei"
    ],
    "categories": [
      "quant-ph",
      "cs.CC",
      "cs.IT",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13408v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13408v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13353v1",
    "title": "Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images",
    "summary": "Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.",
    "authors": [
      "Lucas Gabriel Telesco",
      "Danila Nejamkin",
      "Estefanía Mata",
      "Francisco Filizzola",
      "Kevin Wignall",
      "Lucía Franco Troilo",
      "María de los Angeles Cenoz",
      "Melissa Thompson",
      "Mercedes Leguía",
      "Ignacio Larrabide",
      "José Ignacio Orlando"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13353v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13353v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13288v1",
    "title": "Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO",
    "summary": "Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",
    "authors": [
      "Haoyang Hong",
      "Jiajun Yin",
      "Yuan Wang",
      "Jingnan Liu",
      "Zhe Chen",
      "Ailing Yu",
      "Ji Li",
      "Zhiling Ye",
      "Hansong Xiao",
      "Yefei Chen",
      "Hualei Zhou",
      "Yun Yue",
      "Minghui Yang",
      "Chunxiao Guo",
      "Junwei Liu",
      "Peng Wei",
      "Jinjie Gu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13288v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13288v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13152v1",
    "title": "Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels",
    "summary": "Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.",
    "authors": [
      "Sourya Dipta Das",
      "Shubham Kumar",
      "Kuldeep Yadav"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13152v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13152v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.65
  },
  {
    "arxiv_id": "2511.13510v1",
    "title": "Naga: Vedic Encoding for Deep State Space Models",
    "summary": "This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.",
    "authors": [
      "Melanie Schaller",
      "Nick Janssen",
      "Bodo Rosenhahn"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13510v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13510v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13469v1",
    "title": "GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction",
    "summary": "Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.",
    "authors": [
      "Shiyuan Luo",
      "Chonghao Qiu",
      "Runlong Yu",
      "Yiqun Xie",
      "Xiaowei Jia"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13469v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13469v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13467v1",
    "title": "Non-Linear Scoring Model for Translation Quality Evaluation",
    "summary": "Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.   Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.   Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model   E(x) = a * ln(1 + b * x), a, b > 0,   anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.   The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.",
    "authors": [
      "Serge Gladkoff",
      "Lifeng Han",
      "Katerina Gasova"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13467v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13467v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13419v1",
    "title": "MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction",
    "summary": "Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data",
    "authors": [
      "Shaheen Mohammed Saleh Ahmed",
      "Hakan Hakan Guneyli"
    ],
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13419v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13419v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13414v1",
    "title": "PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation",
    "summary": "Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.",
    "authors": [
      "Hanwen Hu",
      "Zimo Wen",
      "Shiyou Qian",
      "Jian Co"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13414v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13414v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13315v1",
    "title": "Computer Vision based group activity detection and action spotting",
    "summary": "Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.",
    "authors": [
      "Narthana Sivalingam",
      "Santhirarajah Sivasthigan",
      "Thamayanthi Mahendranathan",
      "G. M. R. I. Godaliyadda",
      "M. P. B. Ekanayake",
      "H. M. V. R. Herath"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13315v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13315v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13282v1",
    "title": "Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space",
    "summary": "Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.",
    "authors": [
      "Kaiwen Wang",
      "Kaili Zheng",
      "Yiming Shi",
      "Chenyi Guo",
      "Ji Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13282v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13282v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13274v1",
    "title": "KForge: Program Synthesis for Diverse AI Hardware Accelerators",
    "summary": "GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.   We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.",
    "authors": [
      "Taras Sereda",
      "Tom St. John",
      "Burak Bartan",
      "Natalie Serrino",
      "Sachin Katti",
      "Zain Asgar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA",
      "cs.PF",
      "cs.SE"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13274v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13274v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13133v1",
    "title": "Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning",
    "summary": "Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.   To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.",
    "authors": [
      "Shudong Wang",
      "Xinfei Wang",
      "Chenhao Zhang",
      "Shanchen Pang",
      "Haiyuan Gui",
      "Wenhao Ji",
      "Xiaojian Liao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13133v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13133v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13061v1",
    "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity",
    "summary": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.",
    "authors": [
      "Vladimír Macko",
      "Vladimír Boža"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.DS"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13061v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13061v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13035v1",
    "title": "One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow",
    "summary": "We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.",
    "authors": [
      "Zeyuan Wang",
      "Da Li",
      "Yulin Chen",
      "Ye Shi",
      "Liang Bai",
      "Tianyuan Yu",
      "Yanwei Fu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13035v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13035v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.64
  },
  {
    "arxiv_id": "2511.13514v1",
    "title": "A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data",
    "summary": "Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box\" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.",
    "authors": [
      "Pragatheeswaran Vipulananthan",
      "Kamal Premaratne",
      "Dilip Sarkar",
      "Manohar N. Murthi"
    ],
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13514v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13514v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13505v1",
    "title": "Applying Large Language Models to Characterize Public Narratives",
    "summary": "Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.",
    "authors": [
      "Elinor Poole-Dayan",
      "Daniel T Kessler",
      "Hannah Chiou",
      "Margaret Hughes",
      "Emily S Lin",
      "Marshall Ganz",
      "Deb Roy"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13505v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13505v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13410v1",
    "title": "Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction",
    "summary": "With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.",
    "authors": [
      "Zhaopei Huang",
      "Qifeng Dai",
      "Guozheng Wu",
      "Xiaopeng Wu",
      "Kehan Chen",
      "Chuan Yu",
      "Xubin Li",
      "Tiezheng Ge",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13410v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13410v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13365v1",
    "title": "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference",
    "summary": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.",
    "authors": [
      "Ruijun Deng",
      "Zhihui Lu",
      "Qiang Duan"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13365v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13365v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13338v1",
    "title": "Tab-PET: Graph-Based Positional Encodings for Tabular Transformers",
    "summary": "Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.",
    "authors": [
      "Yunze Leng",
      "Rohan Ghosh",
      "Mehul Motani"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13338v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13338v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13309v1",
    "title": "DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving",
    "summary": "The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.",
    "authors": [
      "Kaiwen Cai",
      "Xinze Liu",
      "Xia Zhou",
      "Hengtong Hu",
      "Jie Xiang",
      "Luyao Zhang",
      "Xueyang Zhang",
      "Kun Zhan",
      "Yifei Zhan",
      "Xianpeng Lang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13309v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13309v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13273v1",
    "title": "Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs",
    "summary": "Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.",
    "authors": [
      "Zhe Sun",
      "Yujun Cai",
      "Jiayu Yao",
      "Yiwei Wang"
    ],
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13273v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13273v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13271v1",
    "title": "Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming",
    "summary": "The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.",
    "authors": [
      "Rufeng Chen",
      "Shuaishuai Jiang",
      "Jiyun Shen",
      "AJung Moon",
      "Lili Wei"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13271v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13271v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13238v1",
    "title": "Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms",
    "summary": "This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.",
    "authors": [
      "Patrick Parschan",
      "Charlott Jakob"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13238v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13238v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13195v1",
    "title": "Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection",
    "summary": "Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.",
    "authors": [
      "Soyul Lee",
      "Seungmin Baek",
      "Dongbo Min"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13195v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13195v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13170v1",
    "title": "THIR: Topological Histopathological Image Retrieval",
    "summary": "According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.   Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.",
    "authors": [
      "Zahra Tabatabaei",
      "Jon Sporring"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13170v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13170v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13126v1",
    "title": "A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition",
    "summary": "This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.",
    "authors": [
      "Nigar Alishzade",
      "Gulchin Abdullayeva"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13126v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13126v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.63
  },
  {
    "arxiv_id": "2511.13527v1",
    "title": "Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images",
    "summary": "Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.",
    "authors": [
      "Ihab Asaad",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13527v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13527v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13476v1",
    "title": "Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation",
    "summary": "Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.",
    "authors": [
      "Zhipeng Ma",
      "Ali Rida Bahja",
      "Andreas Burgdorf",
      "André Pomp",
      "Tobias Meisen",
      "Bo Nørregaard Jørgensen",
      "Zheng Grace Ma"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13476v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13476v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13361v1",
    "title": "MedDCR: Learning to Design Agentic Workflows for Medical Coding",
    "summary": "Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",
    "authors": [
      "Jiyang Zheng",
      "Islam Nassar",
      "Thanh Vu",
      "Xu Zhong",
      "Yang Lin",
      "Tongliang Liu",
      "Long Duong",
      "Yuan-Fang Li"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13361v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13361v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13295v1",
    "title": "Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection",
    "summary": "Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.",
    "authors": [
      "Chaowang Lan",
      "Jingxin Wu",
      "Yulong Yuan",
      "Chuxun Liu",
      "Huangyi Kang",
      "Caihua Liu"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13295v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13295v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13244v1",
    "title": "Seek and You Shall Fold",
    "summary": "Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.",
    "authors": [
      "Nadav Bojan Sellam",
      "Meital Bojan",
      "Paul Schanda",
      "Alex Bronstein"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13244v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13244v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13242v1",
    "title": "MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection",
    "summary": "Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.",
    "authors": [
      "Junjie Wu",
      "Guohong Fu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13242v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13242v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13222v1",
    "title": "Hybrid-Domain Adaptative Representation Learning for Gaze Estimation",
    "summary": "Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\\textbf{5.02}^{\\circ}$ and $\\textbf{3.36}^{\\circ}$, and $\\textbf{9.26}^{\\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.",
    "authors": [
      "Qida Tan",
      "Hongyu Yang",
      "Wenchao Du"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13222v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13222v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13190v1",
    "title": "Video Spatial Reasoning with Object-Centric 3D Rollout",
    "summary": "Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).",
    "authors": [
      "Haoran Tang",
      "Meng Cao",
      "Ruyang Liu",
      "Xiaoxi Liang",
      "Linglong Li",
      "Ge Li",
      "Xiaodan Liang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13190v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13190v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13189v1",
    "title": "Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework",
    "summary": "Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.",
    "authors": [
      "Diego Ortego",
      "Marlon Rodríguez",
      "Mario Almagro",
      "Kunal Dahiya",
      "David Jiménez",
      "Juan C. SanMiguel"
    ],
    "categories": [
      "cs.CV",
      "cs.IR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13189v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13189v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13091v1",
    "title": "STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization",
    "summary": "Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.",
    "authors": [
      "Yuhan Chen",
      "Yuxuan Liu",
      "Long Zhang",
      "Pengzhi Gao",
      "Jian Luan",
      "Wei Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13091v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13091v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13087v1",
    "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements",
    "summary": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.",
    "authors": [
      "SeokJoo Kwak",
      "Jihoon Kim",
      "Boyoun Kim",
      "Jung Jae Yoon",
      "Wooseok Jang",
      "Jeonghoon Hong",
      "Jaeho Yang",
      "Yeong-Dae Kwon"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13087v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13087v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13057v1",
    "title": "Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact",
    "summary": "Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the \"performance loss\" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective \"sweet spot,\" achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.",
    "authors": [
      "Satyanarayan Pati"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13057v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13057v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13055v1",
    "title": "Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries",
    "summary": "Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.",
    "authors": [
      "Ruixin Liu",
      "Zejian Yuan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13055v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13055v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13053v1",
    "title": "Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks",
    "summary": "Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.",
    "authors": [
      "Akira Tamamori"
    ],
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13053v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13053v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.62
  },
  {
    "arxiv_id": "2511.13524v1",
    "title": "FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI",
    "summary": "As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.",
    "authors": [
      "Yuhang Peng",
      "Yizhou Pan",
      "Xinning He",
      "Jihaoyu Yang",
      "Xinyu Yin",
      "Han Wang",
      "Xiaoji Zheng",
      "Chao Gao",
      "Jiangtao Gong"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13524v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13524v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13415v1",
    "title": "Attention Grounded Enhancement for Visual Document Retrieval",
    "summary": "Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \\textbf{A}ttention-\\textbf{G}rounded \\textbf{RE}triever \\textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.",
    "authors": [
      "Wanqing Cui",
      "Wei Huang",
      "Yazhi Guo",
      "Yibo Hu",
      "Meiguang Jin",
      "Junfeng Ma",
      "Keping Bi"
    ],
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13415v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13415v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13368v1",
    "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
    "summary": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.",
    "authors": [
      "Kajetan Dymkiewicz",
      "Ivan Vulic",
      "Helen Yannakoudakis",
      "Eilam Shapira",
      "Roi Reichart",
      "Anna Korhonen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13368v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13368v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13261v1",
    "title": "Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges",
    "summary": "Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant",
    "authors": [
      "Junlong Li",
      "Huaiyuan Xu",
      "Sijie Cheng",
      "Kejun Wu",
      "Kim-Hui Yap",
      "Lap-Pui Chau",
      "Yi Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13261v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13261v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13243v1",
    "title": "Uncovering and Mitigating Transient Blindness in Multimodal Model Editing",
    "summary": "Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.",
    "authors": [
      "Xiaoqi Han",
      "Ru Li",
      "Ran Yi",
      "Hongye Tan",
      "Zhuomin Liang",
      "Víctor Gutiérrez-Basulto",
      "Jeff Z. Pan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13243v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13243v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13240v1",
    "title": "Incoherent Beliefs & Inconsistent Actions in Large Language Models",
    "summary": "Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.",
    "authors": [
      "Arka Pal",
      "Teo Kitanovski",
      "Arthur Liang",
      "Akilesh Potti",
      "Micah Goldblum"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13240v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13240v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13186v1",
    "title": "DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play",
    "summary": "Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\\times$ faster convergence and 30$\\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations",
    "authors": [
      "Akash Karthikeyan",
      "Yash Vardhan Pant"
    ],
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13186v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13186v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13183v1",
    "title": "GenTract: Generative Global Tractography",
    "summary": "Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.",
    "authors": [
      "Alec Sargood",
      "Lemuel Puglisi",
      "Elinor Thompson",
      "Mirco Musolesi",
      "Daniel C. Alexander"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13183v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13183v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13052v1",
    "title": "Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting",
    "summary": "Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to \"undesirable\" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.",
    "authors": [
      "Yunhun Nam",
      "Jaehyung Kim",
      "Jongheon Jeong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13052v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13052v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13032v1",
    "title": "Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts",
    "summary": "We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.",
    "authors": [
      "Sheng Liu",
      "Yuanzhi Liang",
      "Jiepeng Wang",
      "Sidan Du",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13032v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13032v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.61
  },
  {
    "arxiv_id": "2511.13529v1",
    "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
    "summary": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\\% on spontaneous and 4.8\\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\\% and 18.26\\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
    "authors": [
      "Máté Gedeon",
      "Piroska Zsófia Barta",
      "Péter Mihajlik",
      "Tekla Etelka Gráczi",
      "Anna Kohári",
      "Katalin Mády"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13529v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13529v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13481v1",
    "title": "Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns",
    "summary": "Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.",
    "authors": [
      "Attapol T. Rutherford",
      "Sirisak Chueykamhang",
      "Thachaparn Bunditlurdruk",
      "Nanthicha Angsuwichitkul"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13481v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13481v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13457v1",
    "title": "Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure",
    "summary": "Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.",
    "authors": [
      "Bin Liu",
      "Qinghao Zhao",
      "Yuxi Zhou",
      "Zhejun Sun",
      "Kaijie Lei",
      "Deyun Zhang",
      "Shijia Geng",
      "Shenda Hong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13457v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13457v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13397v1",
    "title": "Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)",
    "summary": "The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.",
    "authors": [
      "Nikos Theodoridis",
      "Tim Brophy",
      "Reenu Mohandas",
      "Ganesh Sistu",
      "Fiachra Collins",
      "Anthony Scanlan",
      "Ciaran Eising"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13397v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13397v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13381v1",
    "title": "Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts",
    "summary": "With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.",
    "authors": [
      "Siyu Zhu",
      "Mouxiao Bian",
      "Yue Xie",
      "Yongyu Tang",
      "Zhikang Yu",
      "Tianbin Li",
      "Pengcheng Chen",
      "Bing Han",
      "Jie Xu",
      "Xiaoyan Dong"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13381v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13381v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13378v1",
    "title": "Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models",
    "summary": "Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.",
    "authors": [
      "Carlo Teo Pedretti",
      "Davide Picca",
      "Dario Rodighiero"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13378v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13378v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13312v1",
    "title": "EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation",
    "summary": "Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.",
    "authors": [
      "Jonas Bode",
      "Raphael Memmesheimer",
      "Sven Behnke"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13312v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13312v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13262v1",
    "title": "Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application",
    "summary": "This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.",
    "authors": [
      "Jack B. Coughlin",
      "Archis Joglekar",
      "Jonathan Brodrick",
      "Alexander Lavin"
    ],
    "categories": [
      "physics.comp-ph",
      "cs.CE",
      "cs.LG",
      "cs.MS",
      "physics.plasm-ph"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13262v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13262v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13168v1",
    "title": "SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration",
    "summary": "Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.",
    "authors": [
      "Haodong Wang",
      "Tao Zhuo",
      "Xiuwei Zhang",
      "Hanlin Yin",
      "Wencong Wu",
      "Yanning Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13168v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13168v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13115v1",
    "title": "A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features",
    "summary": "3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.",
    "authors": [
      "Hanzhe Liang",
      "Jie Zhou",
      "Can Gao",
      "Bingyang Guo",
      "Jinbao Wang",
      "Linlin Shen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13115v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13115v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13099v1",
    "title": "MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images",
    "summary": "Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.",
    "authors": [
      "Doanh C. Bui",
      "Ba Hung Ngo",
      "Hoai Luan Pham",
      "Khang Nguyen",
      "Maï K. Nguyen",
      "Yasuhiko Nakashima"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13099v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13099v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13040v1",
    "title": "How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm",
    "summary": "Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).",
    "authors": [
      "Kasun Wickramasinghe",
      "Nisansa de Silva"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13040v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13040v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.6
  },
  {
    "arxiv_id": "2511.13525v1",
    "title": "AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions",
    "summary": "Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.",
    "authors": [
      "Zichong Wang",
      "Zhipeng Yin",
      "Roland H. C. Yap",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13525v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13525v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13488v1",
    "title": "InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE",
    "summary": "Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.",
    "authors": [
      "Lipeng Wang",
      "Hongxing Fan",
      "Haohua Chen",
      "Zehuan Huang",
      "Lu Sheng"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13488v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13488v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13453v1",
    "title": "Hardware optimization on Android for inference of AI models",
    "summary": "The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.",
    "authors": [
      "Iulius Gherasim",
      "Carlos García Sánchez"
    ],
    "categories": [
      "cs.LG",
      "cs.PF"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13453v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13453v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13387v1",
    "title": "Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model",
    "summary": "Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.",
    "authors": [
      "Fei Kong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13387v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13387v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13185v1",
    "title": "Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction",
    "summary": "Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.",
    "authors": [
      "Aishwarya Venkataramanan",
      "Sai Karthikeya Vemuri",
      "Adithya Ashok Chalain Valapil",
      "Joachim Denzler"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13185v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13185v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13178v1",
    "title": "Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach",
    "summary": "With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.",
    "authors": [
      "Mingxuan Tian",
      "Haochen Mu",
      "Donghong Ding",
      "Mengjiao Li",
      "Yuhan Ding",
      "Jianping Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13178v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13178v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13174v1",
    "title": "Warm-starting active-set solvers using graph neural networks",
    "summary": "Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.",
    "authors": [
      "Ella J. Schmidtobreick",
      "Daniel Arnström",
      "Paul Häusner",
      "Jens Sjölund"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13174v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13174v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13160v1",
    "title": "InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions",
    "summary": "Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque \"black boxes\". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a \"what-if\" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.",
    "authors": [
      "TC Singh",
      "Sougata Mukherjea"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13160v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13160v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13132v1",
    "title": "Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack",
    "summary": "Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.",
    "authors": [
      "Chenyang Li",
      "Wenbing Tang",
      "Yihao Huang",
      "Sinong Simon Zhan",
      "Ming Hu",
      "Xiaojun Jia",
      "Yang Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13132v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13132v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13110v1",
    "title": "Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing",
    "summary": "Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.",
    "authors": [
      "Shuaibin Fan",
      "Senming Zhong",
      "Wenchao Yan",
      "Minglong Xue"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13110v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13110v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13107v1",
    "title": "Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study",
    "summary": "The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.",
    "authors": [
      "Zhichao He",
      "Mouxiao Bian",
      "Jianhong Zhu",
      "Jiayuan Chen",
      "Yunqiu Wang",
      "Wenxia Zhao",
      "Tianbin Li",
      "Bing Han",
      "Jie Xu",
      "Junyan Wu"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13107v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13107v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13039v1",
    "title": "MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization",
    "summary": "Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.",
    "authors": [
      "Zhenying Fang",
      "Richang Hong"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13039v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13039v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.59
  },
  {
    "arxiv_id": "2511.13341v1",
    "title": "An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains",
    "summary": "In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.",
    "authors": [
      "Zihe Yan",
      "Kai Luo",
      "Haoyu Yang",
      "Yang Yu",
      "Zhuosheng Zhang",
      "Guancheng Li"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13341v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13341v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13333v1",
    "title": "AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research",
    "summary": "Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.",
    "authors": [
      "Alexandru-Mihai Apostu",
      "Andrei Preda",
      "Alexandra Daniela Damir",
      "Diana Bolocan",
      "Radu Tudor Ionescu",
      "Ioana Croitoru",
      "Mihaela Gaman"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13333v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13333v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13285v1",
    "title": "SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design",
    "summary": "Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.",
    "authors": [
      "Yunjie Yu",
      "Jingchen Wu",
      "Junchen Zhu",
      "Chunze Lin",
      "Guibin Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13285v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13285v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13225v1",
    "title": "Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms",
    "summary": "With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.",
    "authors": [
      "Tyler Loakman",
      "Joseph James",
      "Chenghua Lin"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13225v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13225v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.58
  },
  {
    "arxiv_id": "2511.13497v1",
    "title": "Quantum Machine Learning via Contrastive Training",
    "summary": "Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.",
    "authors": [
      "Liudmila A. Zhukas",
      "Vivian Ni Zhang",
      "Qiang Miao",
      "Qingfeng Wang",
      "Marko Cetina",
      "Jungsang Kim",
      "Lawrence Carin",
      "Christopher Monroe"
    ],
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13497v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13497v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13421v1",
    "title": "Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression",
    "summary": "While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \\textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \\approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \\textit{i.e.}, $E(K, N) \\approx K$ for $K \\le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \\approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.",
    "authors": [
      "Tingkai Yan",
      "Haodong Wen",
      "Binghui Li",
      "Kairong Luo",
      "Wenguang Chen",
      "Kaifeng Lyu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13421v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13421v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13411v1",
    "title": "An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence",
    "summary": "We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing \"baby AGI\" becomes Superintelligence intuition.",
    "authors": [
      "Przemyslaw Chojecki"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13411v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13411v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13259v1",
    "title": "GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models",
    "summary": "Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \\textit{etc}. To bridge this gap, we introduce \\textbf{GeoX-Bench}, a comprehensive \\underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \\underline{cross}-view \\underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \\textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.",
    "authors": [
      "Yushuo Zheng",
      "Jiangyong Ying",
      "Huiyu Duan",
      "Chunyi Li",
      "Zicheng Zhang",
      "Jing Liu",
      "Xiaohong Liu",
      "Guangtao Zhai"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13259v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13259v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13079v1",
    "title": "Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving",
    "summary": "Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.",
    "authors": [
      "Jiacheng Tang",
      "Mingyue Feng",
      "Jiachao Liu",
      "Yaonong Wang",
      "Jian Pu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13079v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13079v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.57
  },
  {
    "arxiv_id": "2511.13530v1",
    "title": "Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety",
    "summary": "Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.",
    "authors": [
      "Vesna Poprcova",
      "Iulia Lefter",
      "Matthias Wieser",
      "Martijn Warnier",
      "Frances Brazier"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13530v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13530v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13526v1",
    "title": "Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models",
    "summary": "Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.",
    "authors": [
      "Zhengda Wang",
      "Daqian Shi",
      "Jingyi Zhao",
      "Xiaolei Diao",
      "Xiongfeng Tang",
      "Yanguo Qin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13526v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13526v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13507v1",
    "title": "Mapping the Vanishing and Transformation of Urban Villages in China",
    "summary": "Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the \"remained-demolished-redeveloped\" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.",
    "authors": [
      "Wenyu Zhang",
      "Yao Tong",
      "Yiqiu Liu",
      "Rui Cao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13507v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13507v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13487v1",
    "title": "Systematic evaluation of time-frequency features for binaural sound source localization",
    "summary": "This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.",
    "authors": [
      "Davoud Shariat Panah",
      "Alessandro Ragano",
      "Dan Barry",
      "Jan Skoglund",
      "Andrew Hines"
    ],
    "categories": [
      "eess.AS",
      "cs.LG",
      "cs.SD"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13487v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13487v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13431v1",
    "title": "FUSE: A Flow-based Mapping Between Shapes",
    "summary": "We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.",
    "authors": [
      "Lorenzo Olearo",
      "Giulio Viganò",
      "Daniele Baieri",
      "Filippo Maggioli",
      "Simone Melzi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13431v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13431v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13371v1",
    "title": "Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning",
    "summary": "How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.",
    "authors": [
      "Caroline Baumgartner",
      "Eleanor Spens",
      "Neil Burgess",
      "Petru Manescu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13371v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13371v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13245v1",
    "title": "Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems",
    "summary": "This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center.    In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.",
    "authors": [
      "Matt Luckcuck",
      "Maike Schwammberger",
      "Mengwei Xu"
    ],
    "categories": [
      "cs.LO",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13245v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13245v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13211v1",
    "title": "3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale",
    "summary": "Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.",
    "authors": [
      "Yijia Fan",
      "Jusheng Zhang",
      "Kaitong Cai",
      "Jing Yang",
      "Jian Wang",
      "Keze Wang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13211v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13211v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13207v1",
    "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection",
    "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.",
    "authors": [
      "Cheng Peng",
      "Zhenzhe Zhang",
      "Cheng Chi",
      "Xiaobao Wei",
      "Yanhao Zhang",
      "Heng Wang",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Jing Liu",
      "Shanghang Zhang"
    ],
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13207v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13207v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13180v1",
    "title": "Translation Entropy: A Statistical Framework for Evaluating Translation Systems",
    "summary": "The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.",
    "authors": [
      "Ronit D. Gross",
      "Yanir Harel",
      "Ido Kanter"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13180v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13180v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13166v1",
    "title": "Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users",
    "summary": "To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.",
    "authors": [
      "Zhaoxin Shen",
      "Dan Wu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13166v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13166v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13121v1",
    "title": "CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model",
    "summary": "Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.",
    "authors": [
      "Yuqi Zhang",
      "Guanying Chen",
      "Jiaxing Chen",
      "Chuanyu Fu",
      "Chuan Huang",
      "Shuguang Cui"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13121v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13121v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13102v1",
    "title": "CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation",
    "summary": "Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept \"leg\" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.",
    "authors": [
      "Yu Zhu",
      "Dan Zeng",
      "Shuiwang Li",
      "Qijun Zhao",
      "Qiaomu Shen",
      "Bo Tang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13102v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13102v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13082v1",
    "title": "Real-time prediction of breast cancer sites using deformation-aware graph neural network",
    "summary": "Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.",
    "authors": [
      "Kyunghyun Lee",
      "Yong-Min Shin",
      "Minwoo Shin",
      "Jihun Kim",
      "Sunghwan Lim",
      "Won-Yong Shin",
      "Kyungho Yoon"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13082v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13082v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13050v1",
    "title": "DS-ATGO: Dual-Stage Synergistic Learning via Forward Adaptive Threshold and Backward Gradient Optimization for Spiking Neural Networks",
    "summary": "Brain-inspired spiking neural networks (SNNs) are recognized as a promising avenue for achieving efficient, low-energy neuromorphic computing. Direct training of SNNs typically relies on surrogate gradient (SG) learning to estimate derivatives of non-differentiable spiking activity. However, during training, the distribution of neuronal membrane potentials varies across timesteps and progressively deviates toward both sides of the firing threshold. When the firing threshold and SG remain fixed, this may lead to imbalanced spike firing and diminished gradient signals, preventing SNNs from performing well. To address these issues, we propose a novel dual-stage synergistic learning algorithm that achieves forward adaptive thresholding and backward dynamic SG. In forward propagation, we adaptively adjust thresholds based on the distribution of membrane potential dynamics (MPD) at each timestep, which enriches neuronal diversity and effectively balances firing rates across timesteps and layers. In backward propagation, drawing from the underlying association between MPD, threshold, and SG, we dynamically optimize SG to enhance gradient estimation through spatio-temporal alignment, effectively mitigating gradient information loss. Experimental results demonstrate that our method achieves significant performance improvements. Moreover, it allows neurons to fire stable proportions of spikes at each timestep and increases the proportion of neurons that obtain gradients in deeper layers.",
    "authors": [
      "Jiaqiang Jiang",
      "Wenfeng Xu",
      "Jing Fan",
      "Rui Yan"
    ],
    "categories": [
      "cs.NE"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13050v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13050v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.56
  },
  {
    "arxiv_id": "2511.13466v1",
    "title": "The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology",
    "summary": "Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.",
    "authors": [
      "Jaclyn Ocumpaugh",
      "Luc Paquette",
      "Ryan S. Baker",
      "Amanda Barany",
      "Jeff Ginger",
      "Nathan Casano",
      "Andres F. Zambrano",
      "Xiner Liu",
      "Zhanlan Wei",
      "Yiqui Zhou",
      "Qianhui Liu",
      "Stephen Hutt",
      "Alexandra M. A. Andres",
      "Nidhi Nasiar",
      "Camille Giordano",
      "Martin van Velsen",
      "Micheal Mogessi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13466v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13466v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13458v1",
    "title": "Trust in Vision-Language Models: Insights from a Participatory User Workshop",
    "summary": "With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.",
    "authors": [
      "Agnese Chiatti",
      "Lara Piccolo",
      "Sara Bernardini",
      "Matteo Matteucci",
      "Viola Schiaffonati"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13458v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13458v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13359v1",
    "title": "Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms",
    "summary": "The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.",
    "authors": [
      "Yuhang Wang",
      "Yanxu Zhu",
      "Jitao Sang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13359v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13359v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13329v1",
    "title": "RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection",
    "summary": "Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \\textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.",
    "authors": [
      "Shufan Yang",
      "Zifeng Cheng",
      "Zhiwei Jiang",
      "Yafeng Yin",
      "Cong Wang",
      "Shiping Ge",
      "Yuchen Fu",
      "Qing Gu"
    ],
    "categories": [
      "cs.CL",
      "cs.CR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13329v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13329v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13229v1",
    "title": "Laplace Learning in Wasserstein Space",
    "summary": "The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning   methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical   notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to   an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator   on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through   numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.",
    "authors": [
      "Mary Chriselda Antony Oliver",
      "Michael Roberts",
      "Carola-Bibiane Schönlieb",
      "Matthew Thorpe"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13229v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13229v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13214v1",
    "title": "Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks",
    "summary": "The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.",
    "authors": [
      "Guillaume Infantes",
      "Stéphanie Roussel",
      "Antoine Jacquet",
      "Emmanuel Benazera"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13214v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13214v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13204v1",
    "title": "RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection",
    "summary": "Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both \"how\" motion evolves and \"what\" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.",
    "authors": [
      "Junhee Lee",
      "ChaeBeen Bang",
      "MyoungChul Kim",
      "MyeongAh Cho"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13204v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13204v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13111v1",
    "title": "NuBench: An Open Benchmark for Deep Learning-Based Event Reconstruction in Neutrino Telescopes",
    "summary": "Neutrino telescopes are large-scale detectors designed to observe Cherenkov radiation produced from neutrino interactions in water or ice. They exist to identify extraterrestrial neutrino sources and to probe fundamental questions pertaining to the elusive neutrino itself. A central challenge common across neutrino telescopes is to solve a series of inverse problems known as event reconstruction, which seeks to resolve properties of the incident neutrino, based on the detected Cherenkov light. In recent times, significant efforts have been made in adapting advances from deep learning research to event reconstruction, as such techniques provide several benefits over traditional methods. While a large degree of similarity in reconstruction needs and low-level data exists, cross-experimental collaboration has been hindered by a lack of diverse open-source datasets for comparing methods.   We present NuBench, an open benchmark for deep learning-based event reconstruction in neutrino telescopes. NuBench comprises seven large-scale simulated datasets containing nearly 130 million charged- and neutral-current muon-neutrino interactions spanning 10 GeV to 100 TeV, generated across six detector geometries inspired by existing and proposed experiments. These datasets provide pulse- and event-level information suitable for developing and comparing machine-learning reconstruction methods in both water and ice environments. Using NuBench, we evaluate four reconstruction algorithms - ParticleNeT and DynEdge, both actively used within the KM3NeT and IceCube collaborations, respectively, along with GRIT and DeepIce - on up to five core tasks: energy and direction reconstruction, topology classification, interaction vertex prediction, and inelasticity estimation.",
    "authors": [
      "Rasmus F. Orsoe",
      "Stephan Meighen-Berger",
      "Jeffrey Lazar",
      "Jorge Prado",
      "Ivan Mozun-Mateo",
      "Aske Rosted",
      "Philip Weigel",
      "Arturo Llorente Anaya"
    ],
    "categories": [
      "hep-ex",
      "cs.AI",
      "cs.LG",
      "physics.data-an",
      "physics.ins-det"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13111v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13111v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13036v1",
    "title": "uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data",
    "summary": "Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.",
    "authors": [
      "Dahyun Chung",
      "Donghyun Shin",
      "Yujin Sung",
      "Seunggi Moon",
      "Jinwoo Jeon",
      "Byung-Jun Lee"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13036v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13036v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.55
  },
  {
    "arxiv_id": "2511.13463v1",
    "title": "Multi-task GINN-LP for Multi-target Symbolic Regression",
    "summary": "In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.",
    "authors": [
      "Hussein Rajabu",
      "Lijun Qian",
      "Xishuang Dong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13463v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13463v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13417v1",
    "title": "Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source",
    "summary": "Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.",
    "authors": [
      "Mykola Lavreniuk",
      "Nataliia Kussul",
      "Andrii Shelestov",
      "Yevhenii Salii",
      "Volodymyr Kuzin",
      "Sergii Skakun",
      "Zoltan Szantoi"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13417v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13417v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13269v1",
    "title": "Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation",
    "summary": "Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.",
    "authors": [
      "Lingfeng Zhang",
      "Yuchen Zhang",
      "Hongsheng Li",
      "Haoxiang Fu",
      "Yingbo Tang",
      "Hangjun Ye",
      "Long Chen",
      "Xiaojun Liang",
      "Xiaoshuai Hao",
      "Wenbo Ding"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13269v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13269v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13264v1",
    "title": "SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression",
    "summary": "3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \\textbf{\\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \\times$ compression across benchmark datasets (upto $3\\times$ on large-scale scenes). On an average, SymGS enables $\\bf{108\\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \\textbf{\\color{cyan}{symgs.github.io}}",
    "authors": [
      "Keshav Gupta",
      "Akshat Sanghvi",
      "Shreyas Reddy Palley",
      "Astitva Srivastava",
      "Charu Sharma",
      "Avinash Sharma"
    ],
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13264v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13264v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13250v1",
    "title": "Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs",
    "summary": "We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.",
    "authors": [
      "Aleksandar Stanković",
      "Dejan Lisica"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13250v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13250v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13249v1",
    "title": "Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention",
    "summary": "Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.",
    "authors": [
      "Yu Wen",
      "Shuyong Gao",
      "Shuping Zhang",
      "Miao Huang",
      "Lili Tao",
      "Han Yang",
      "Haozhe Xing",
      "Lihe Zhang",
      "Boxue Hou"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13249v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13249v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13182v1",
    "title": "Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study",
    "summary": "Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.",
    "authors": [
      "Mihai Dan Nadas",
      "Laura Diosan"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13182v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13182v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13169v1",
    "title": "TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine",
    "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\\_r1 and gemini\\_2\\_5\\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the \"In-depth Challenge for Comprehensive TCM Abilities\" special track.",
    "authors": [
      "Tianai Huang",
      "Jiayuan Chen",
      "Lu Lu",
      "Pengcheng Chen",
      "Tianbin Li",
      "Bing Han",
      "Wenchao Tang",
      "Jie Xu",
      "Ming Li"
    ],
    "categories": [
      "cs.CL"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13169v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13169v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13143v1",
    "title": "SoK: The Last Line of Defense: On Backdoor Defense Evaluation",
    "summary": "Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.   Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.",
    "authors": [
      "Gorka Abad",
      "Marina Krček",
      "Stefanos Koffas",
      "Behrad Tajalli",
      "Marco Arazzi",
      "Roberto Riaño",
      "Xiaoyun Xu",
      "Zhuoran Liu",
      "Antonino Nocera",
      "Stjepan Picek"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13143v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13143v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13138v1",
    "title": "WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection",
    "summary": "3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.",
    "authors": [
      "Longhui Zheng",
      "Qiming Xia",
      "Xiaolu Chen",
      "Zhaoliang Liu",
      "Chenglu Wen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13138v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13138v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13131v1",
    "title": "MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.",
    "authors": [
      "Gagan Raj Gupta",
      "Anshul Kumar",
      "Manish Rai",
      "Apu Chakraborty",
      "Ashutosh Modi",
      "Abdelaali Chaoub",
      "Soumajit Pramanik",
      "Moyank Giri",
      "Yashwanth Holla",
      "Sunny Kumar",
      "M. V. Kiran Sooraj"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.NI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13131v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13131v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13113v1",
    "title": "Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining",
    "summary": "Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.",
    "authors": [
      "Zhaocheng Yu",
      "Kui Jiang",
      "Junjun Jiang",
      "Xianming Liu",
      "Guanglu Sun",
      "Yi Xiao"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13113v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13113v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13054v1",
    "title": "ViSS-R1: Self-Supervised Reinforcement Video Reasoning",
    "summary": "Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.",
    "authors": [
      "Bo Fang",
      "Yuxin Song",
      "Qiangqiang Wu",
      "Haoyuan Sun",
      "Wenhao Wu",
      "Antoni B. Chan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13054v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13054v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.54
  },
  {
    "arxiv_id": "2511.13542v1",
    "title": "Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop",
    "summary": "Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.",
    "authors": [
      "Amirreza Mehrabi",
      "Jason Wade Morphew",
      "Breejha Quezada",
      "N. Sanjay Rebello"
    ],
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13542v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13542v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13339v1",
    "title": "Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model",
    "summary": "Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.",
    "authors": [
      "Han Meng",
      "Gang Mei",
      "Hong Tian",
      "Nengxiong Xu",
      "Jianbing Peng"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13339v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13339v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13191v1",
    "title": "Birth of a Painting: Differentiable Brushstroke Reconstruction",
    "summary": "Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.",
    "authors": [
      "Ying Jiang",
      "Jiayin Lu",
      "Yunuo Chen",
      "Yumeng He",
      "Kui Wu",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13191v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13191v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13049v1",
    "title": "Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information",
    "summary": "We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \\textit{share a common subspace}. We assume that a large amount $M$ of \\textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\\widetilde{O}\\left(\\sqrt{\\frac{nd}{M}}\\right)$ and $\\widetilde{O}\\left(\\sqrt{\\frac{dr}{N}}\\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.",
    "authors": [
      "Antoine Ledent",
      "Mun Chong Soo",
      "Nong Minh Hieu"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13049v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13049v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.53
  },
  {
    "arxiv_id": "2511.13503v1",
    "title": "The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business",
    "summary": "Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \\textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.",
    "authors": [
      "Ioannis Diamantis"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM",
      "stat.AP"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13503v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13503v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13480v1",
    "title": "A Lexical Analysis of online Reviews on Human-AI Interactions",
    "summary": "This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.",
    "authors": [
      "Parisa Arbab",
      "Xiaowen Fang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13480v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13480v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13226v1",
    "title": "Informative Communication of Robot Plans",
    "summary": "When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.",
    "authors": [
      "Michele Persiani",
      "Thomas Hellstrom"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13226v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13226v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13135v1",
    "title": "MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation",
    "summary": "As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \\textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.",
    "authors": [
      "Junjie Yang",
      "Yuhao Yan",
      "Gang Wu",
      "Yuxuan Wang",
      "Ruoyu Liang",
      "Xinjie Jiang",
      "Xiang Wan",
      "Fenglei Fan",
      "Yongquan Zhang",
      "Feiwei Qin",
      "Changmiao Wan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13135v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13135v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13060v1",
    "title": "Latency and Ordering Effects in Online Decisions",
    "summary": "Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \\ge L_{\\mathrm{ideal}} + g_1(λ) + g_2(\\varepsilon_\\star) + g_{12}(λ,\\varepsilon_\\star) - D_{\\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\\mathrm{ncx}}\\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.",
    "authors": [
      "Duo Yi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13060v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13060v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.52
  },
  {
    "arxiv_id": "2511.13232v1",
    "title": "MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI",
    "summary": "Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.",
    "authors": [
      "Malek Al Abed",
      "Sebiha Demir",
      "Anne Groteklaes",
      "Elodie Germani",
      "Shahrooz Faghihroohi",
      "Hemmen Sabir",
      "Shadi Albarqouni"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13232v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13232v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13175v1",
    "title": "HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution",
    "summary": "Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.",
    "authors": [
      "Chao Yang",
      "Boqian Zhang",
      "Jinghao Xu",
      "Guang Jiang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13175v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13175v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13047v1",
    "title": "DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation",
    "summary": "Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.",
    "authors": [
      "Yan Gong",
      "Jianli Lu",
      "Yongsheng Gao",
      "Jie Zhao",
      "Xiaojuan Zhang",
      "Susanto Rahardja"
    ],
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13047v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13047v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.51
  },
  {
    "arxiv_id": "2511.13535v1",
    "title": "Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew",
    "summary": "As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.",
    "authors": [
      "Farhin Farhad Riya",
      "Shahinul Hoque",
      "Jinyuan Stella Sun",
      "Olivera Kotevska"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13535v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13535v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.13297v1",
    "title": "CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving",
    "summary": "End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.",
    "authors": [
      "Enhui Ma",
      "Lijun Zhou",
      "Tao Tang",
      "Jiahuan Zhang",
      "Junpeng Jiang",
      "Zhan Zhang",
      "Dong Han",
      "Kun Zhan",
      "Xueyang Zhang",
      "XianPeng Lang",
      "Haiyang Sun",
      "Xia Zhou",
      "Di Lin",
      "Kaicheng Yu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13297v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13297v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.5
  },
  {
    "arxiv_id": "2511.13420v1",
    "title": "VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task",
    "summary": "Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.",
    "authors": [
      "Xingming Long",
      "Jie Zhang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13420v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13420v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.13400v1",
    "title": "What Color Is It? A Text-Interference Multimodal Hallucination Benchmark",
    "summary": "With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the \"What Color Is It\" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.",
    "authors": [
      "Jinkun Zhao",
      "Lei Huang",
      "Wenjun Wu"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13400v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13400v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.49
  },
  {
    "arxiv_id": "2511.13344v1",
    "title": "YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection",
    "summary": "This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.",
    "authors": [
      "Ori Meiraz",
      "Sharon Shalev",
      "Avishai Weizman"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13344v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13344v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.13278v1",
    "title": "SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting",
    "summary": "Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/",
    "authors": [
      "Zihan Li",
      "Tengfei Wang",
      "Wentian Gan",
      "Hao Zhan",
      "Xin Wang",
      "Zongqian Zhan"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13278v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13278v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.48
  },
  {
    "arxiv_id": "2511.13197v1",
    "title": "Self-Supervised Ultrasound Screen Detection",
    "summary": "Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.",
    "authors": [
      "Alberto Gomez",
      "Jorge Oliveira",
      "Ramon Casero",
      "Agis Chartsias"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13197v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13197v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.13106v1",
    "title": "Low-Level Dataset Distillation for Medical Image Enhancement",
    "summary": "Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.",
    "authors": [
      "Fengzhi Xu",
      "Ziyuan Yang",
      "Mengyu Sun",
      "Joey Tianyi Zhou",
      "Yi Zhang"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13106v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13106v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.47
  },
  {
    "arxiv_id": "2511.13127v1",
    "title": "VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language",
    "summary": "Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.",
    "authors": [
      "Zonghao Ying",
      "Moyang Chen",
      "Nizhang Li",
      "Zhiqiang Wang",
      "Wenxin Zhang",
      "Quanchen Zou",
      "Zonglei Jing",
      "Aishan Liu",
      "Xianglong Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13127v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13127v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.45
  },
  {
    "arxiv_id": "2511.13533v1",
    "title": "Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems",
    "summary": "In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.",
    "authors": [
      "Jeffrey Wen",
      "Rizwan Ahmad",
      "Philip Schniter"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13533v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13533v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.13276v1",
    "title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
    "summary": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.",
    "authors": [
      "Noam Tsfaty",
      "Avishai Weizman",
      "Liav Cohen",
      "Moshe Tshuva",
      "Yehudit Aperstein"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13276v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13276v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.13199v1",
    "title": "Asymptotic confidence bands for centered purely random forests",
    "summary": "In a multivariate nonparametric regression setting we construct explicit asymptotic uniform confidence bands for centered purely random forests. Since the most popular example in this class of random forests, namely the uniformly centered purely random forests, is well known to suffer from suboptimal rates, we propose a new type of purely random forests, called the Ehrenfest centered purely random forests, which achieve minimax optimal rates. Our main confidence band theorem applies to both random forests. The proof is based on an interpretation of random forests as generalized U-Statistics together with a Gaussian approximation of the supremum of empirical processes. Our theoretical findings are illustrated in simulation examples.",
    "authors": [
      "Natalie Neumeyer",
      "Jan Rabe",
      "Mathias Trabs"
    ],
    "categories": [
      "math.ST",
      "stat.ML"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13199v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13199v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.44
  },
  {
    "arxiv_id": "2511.13494v1",
    "title": "Language-Guided Invariance Probing of Vision-Language Models",
    "summary": "Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.   Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.",
    "authors": [
      "Jae Joong Lee"
    ],
    "categories": [
      "cs.CV"
    ],
    "published": "2025-11-17",
    "url": "https://arxiv.org/abs/2511.13494v1",
    "pdf_url": "https://arxiv.org/pdf/2511.13494v1.pdf",
    "date": "2025-11-18",
    "source": "arxiv",
    "research_score": 0.43
  }
]