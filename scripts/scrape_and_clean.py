#!/usr/bin/env python3
"""
Idea Vault Daily Report Scraper

Fetches content from SOURCE_URL, strips HTML, processes with LLM,
and writes timestamped markdown files to docs/_daily/.

Based on the original AI Research Daily architecture.
"""

import os
import sys
from datetime import datetime
from pathlib import Path
import httpx
from bs4 import BeautifulSoup


def load_author_prompt():
    """Load the author prompt from file"""
    prompt_file = Path(__file__).parent / "author_prompt.txt"
    if not prompt_file.exists():
        print(f"Warning: Author prompt file not found at {prompt_file}")
        return "You are a helpful assistant that summarizes web content into markdown format."
    
    with open(prompt_file, 'r', encoding='utf-8') as f:
        return f.read().strip()


def fetch_and_clean_content(url):
    """
    Fetch content from URL and strip HTML to get clean text.
    
    Args:
        url: The source URL to fetch
        
    Returns:
        Tuple of (title, cleaned_text)
    """
    print(f"Fetching content from: {url}")
    
    try:
        response = httpx.get(url, timeout=30.0, follow_redirects=True)
        response.raise_for_status()
    except httpx.HTTPError as e:
        print(f"Error fetching URL: {e}", file=sys.stderr)
        return None, None
    
    # Parse HTML
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Remove script and style elements
    for script in soup(["script", "style", "nav", "footer", "header"]):
        script.decompose()
    
    # Extract title
    title = soup.find('title')
    title_text = title.get_text().strip() if title else "Daily Insight"
    
    # Get main content - try to find main content area
    main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')
    if main_content:
        text = main_content.get_text(separator='\n', strip=True)
    else:
        text = soup.get_text(separator='\n', strip=True)
    
    # Clean up whitespace
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    cleaned_text = '\n'.join(lines)
    
    print(f"Extracted {len(cleaned_text)} characters of content")
    return title_text, cleaned_text


def call_llm_with_prompt(system_prompt, content):
    """
    Process content with LLM using the author prompt.
    
    Note: This is a placeholder implementation. In production, you would:
    1. Use an actual LLM API (OpenAI, Anthropic, local Ollama, etc.)
    2. Pass the system_prompt and content to the LLM
    3. Return the formatted markdown response
    
    For now, we'll create a simple formatted version of the content.
    """
    # Placeholder implementation - creates a basic markdown structure
    timestamp = datetime.utcnow().strftime("%Y-%m-%d %H:%M UTC")
    
    # Simple markdown formatting
    markdown_output = f"""# Daily Insight - {timestamp}

## Summary

{content[:500]}...

## Key Points

- Content fetched from source successfully
- Processed at {timestamp}
- Ready for review

## Next Steps

This is a placeholder output. To enable full LLM processing:
1. Add your LLM API credentials
2. Update the `call_llm_with_prompt()` function
3. Re-run the scraper

---

*Generated by Idea Vault Daily Scraper*
"""
    
    return markdown_output


def write_daily_report(markdown_content):
    """
    Write markdown content to a timestamped file in docs/_daily/
    
    Args:
        markdown_content: The markdown content to write
        
    Returns:
        Path to the created file
    """
    # Ensure _daily directory exists
    daily_dir = Path("docs/_daily")
    daily_dir.mkdir(parents=True, exist_ok=True)
    
    # Create timestamped filename
    timestamp = datetime.utcnow().strftime("%Y-%m-%d-%H%M")
    filename = f"{timestamp}-daily-report.md"
    filepath = daily_dir / filename
    
    # Write content with frontmatter
    frontmatter = f"""---
layout: post
title: "Daily Report {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}"
date: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S %z')}
categories: daily
---

"""
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(frontmatter)
        f.write(markdown_content)
    
    print(f"✅ Successfully wrote daily report to: {filepath}")
    return filepath


def main():
    """Main execution function"""
    # Get source URL from environment variable
    source_url = os.environ.get('SOURCE_URL')
    
    if not source_url:
        print("Error: SOURCE_URL environment variable not set", file=sys.stderr)
        print("Please set SOURCE_URL to the web page you want to scrape", file=sys.stderr)
        sys.exit(1)
    
    # Load author prompt
    author_prompt = load_author_prompt()
    
    # Fetch and clean content
    title, content = fetch_and_clean_content(source_url)
    
    if not content:
        print("Error: Failed to fetch or parse content", file=sys.stderr)
        sys.exit(1)
    
    # Process with LLM
    print("Processing content with LLM...")
    markdown_output = call_llm_with_prompt(author_prompt, content)
    
    # Write to file
    output_path = write_daily_report(markdown_output)
    
    print(f"\n✅ Daily report generation complete!")
    print(f"   Output: {output_path}")
    print(f"   Source: {source_url}")
    

if __name__ == "__main__":
    main()
